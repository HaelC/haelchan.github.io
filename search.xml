<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Item-Based Collaborative Filtering Recommendation Algorithms reading report]]></title>
    <url>%2F2017%2F11%2F03%2FItem-Based-CFRA-reading-report%2F</url>
    <content type="text"><![CDATA[This semester I’m taking the E-commerce System Structure, and we have come to learn about personalization. In E-commerce, the form of personalization is to recommend items to the user. Recommendation Metrics Prediction accuracy Coverage Novelty Diversity Privacy Scalability Recommendation Strategies Popularity Association role Content based Collaborative filtering Hybrid Summary of Item-Based CFRAIn the corresponding experiment, our assignment is to read the paper Item-Based Collaborative Filtering Recommendation Algorithms, a famous paper in recommender system. And I’d like to record some key point. Related recommender systemsTapestryOne of the earliest implementations of collaborative filtering-based recommender systems. It relied on the explicit opinions of people from a close-knit community, such as an office workgroup.Later, several rating-based automated recommender systems were developed, such as pseudonymous CF solution provided by the GroupLens research system, Ringo and Video Recommender.Bayesian networksBayesian networks create a model based on a training set with a decision tree at each node and edges representing user information. Bayesian networks may prove practical for environments in which knowledge of user preferences changes slowly with respect to the time needed to build the model but are not suitable for environments in which user preference models must be updated rapidly or frequently since the process of building the model can be slow.ClusteringClustering techniques work by identifying groups of users who appear to have similar preferences.Clustering techniques usually produce less-personal recommendations than other methods, and in some cases, the clusters have worse accuracy than nearest neighbor algorithms.Clustering techniques can be applied as a “first step” for shrinking the candidate set in a nearest neighbor algorithm or for distributing nearest-neighbor computation across several recommender engines.Horting is a graph-based technique in which nodes are users, and edges between nodes indicate degree of similarity between two users. Predictions are produced by walking the graph to nearby nodes and combining the opinions of the nearby users. Collaborative Filtering Based Recommender SystemsFigure 1 shows the schematic diagram of the collaborative filtering process. CF algorithms represent the entire m×n user-item data as a ratings matrix, A. Each entry a_{i,j} in A represents the preference score (ratings) of the ith user on the jth item. Each individual ratings is within a numerical scale and it can as well be 0 indicating that the user has not yet rated that item.Prediction is a numerical value.Recommendation is a list of N items. Memory-based and Model-based algorithmsMemory-based CF AlgorithmsMemory-based algorithms utilize the entire user-item database to generate a prediction. These systems employ statistical techniques to find a set of users, known as neighbors, that have a history of agreeing with the target user. The techniques, also known as nearest-neighbor or user-based collaborative filtering, are more popular and widely used in practice. Model-based CF AlgorithmsModel-based collaborative filtering algorithms provide item recommendation by first developing a model of user ratings. Algorithms in this category take a probabilistic approach and envision the collaborative filtering process as computing the expected value of a user prediction, give his/her ratings on other items. Challenges of User-based CF Algorithms Sparsity. Large E-commerce systems like Amazon and Taobao have large item sets, but users can only by a small fraction of the total item set. Scalability. With millions of users and items, a typical web-based recommender system running existing algorithms will suffer serious scalability problems. Item-based Collaborative Filtering AlgorithmThe item-based approach looks into the set of items the target user has rated and computes how similar they are to the target item i and then selects k most similar items. At the same time their corresponding similarities are also computed. Once the most similar items are found, the prediction is then computed by taking a weighted average of the target user’s ratings on these similar items. Item Similarity Computation Cosine-based Similaritysim(i,j)=cos(\vec i,\vec j)=\frac{\vec i\cdot\vec j}{||\vec i||*||\vec j||}Two items are thought of as two vectors in the m dimensional user-space. The similarity betweeen them is measured by computing the cosine of the angle between these two vectors. Correlation-based Similaritysim(i,j)=\frac{\sum_{u\in U}(R_{u,i}-\overline R_i)(R_{u,j}-\overline R_j)}{\sqrt{\sum_{u\in U}(R_{u,i}-\overline R_i)^2}\sqrt{\sum_{u\in U}(R_{u,j}-\overline R_j)^2}}Similarity between two items i and j is measured by computing the Pearson-r correlation. To make the correlation computation accurate, first isolate the co-rated cases(i.e., cases where the users rated both i and j). Let the set of users who both rated i and j are denoted by U. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859/** * Implementation of Correlation-based Similarity * reference: Wang Menghan's implementation * * @author Hael Chan * @version 1.0 * @time 2017-11-05 23:51：09 */public double[][] calSimilarity(double [][]uiRating) &#123; /* uiRating is a u*i matrix, * where u is the number of users, * and i is the number of items */ int userNum = uiRating.length; // the number of rows of uiRating int itemNum = uiRating[0].length; // the number of columns of uiRating double[][] simResult = new double[itemNum][itemNum]; // a new matrix for similarity among items double[] average = new double[itemNum]; int[] nonZero = new int[itemNum]; for (int c = 0; c &lt; itemNum; c++) &#123; // for every item, iterate every user's rating, for (int r = 0; r &lt; userNum; r++) &#123; // so the nested for loop may be a bit nonintuitive if (uiRating[r][c] &gt; 0) &#123; // if uiRating[r][c] == 0, then just ignore it average[c] += uiRating[r][c]; nonZero[c]++; &#125; &#125; &#125; // Calculate every item's average rating for (int i = 0; i &lt; itemNum; i++) &#123; average[i] /= nonZero[i]; &#125; // Calculate the similarity for (int i = 0; i &lt; itemNum; i++) &#123; for (int j = i + 1; j &lt; itemNum; j++) &#123; // here j starts with i + 1 to improve efficient and avoid (i == j) case double num = 0, den = 0; double den1 = 0, den2 = 0; for (int k = 0; k &lt; userNum; k++) &#123; double diff1 = 0, diff2 = 0; if (uiRating[k][i] &gt; 0) &#123; diff1 = uiRating[k][i] - average[i]; // R_&#123;u,i&#125;-\overline R_i (LaTex syntax) &#125; if (uiRating[k][j] &gt; 0) &#123; diff2 = uiRating[k][j] - average[j]; // R_&#123;u,j&#125;-\overline R_j (LaTex syntax) &#125; num += diff1 * diff2; den1 += diff1 * diff1; den2 += diff2 * diff2; &#125; den = Math.sqrt(den1) * Math.sqrt(den2); simResult[i][j] = (den == 0) ? 0 : num / den; // remember the case that den may be zero simResult[j][i] = simResult[i][j]; &#125; &#125; return simResult; &#125; Adjusted Cosine Similaritysim(i,j)=\frac{\sum_{u\in U}(R_{u,i}-\overline R_u)(R_{u,j}-\overline R_u)}{\sqrt{\sum_{u\in U}(R_{u,i}-\overline R_u)^2}\sqrt{\sum_{u\in U}(R_{u,j}-\overline R_u)^2}}The adjusted cosine similarity offsets cosine measure case’s important drawback - the differences in rating scale between different users are not taken into account, by subtracting the corresponding user average from each co-rated pair. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758/** * Implementation of Adjusted Cosine Similarity * * @author Hael * @version 1.0 * @time 2017-11-07 09:49:51 */ public double[][] calSimilarity(double [][]uiRating) &#123; /* * Basically the method is the same as the * implementation of Correlation-based Similarity, * except the computation of average is different. */ int userNum = uiRating.length; int itemNum = uiRating[0].length; double[][] similarityResult = new double[itemNum][itemNum]; double[] average = new double[userNum]; int[] nonZero = new int[userNum]; for (int r = 0; r &lt; userNum; r++) &#123; for (int c = 0; c &lt; itemNum; c++) &#123; if (uiRating[r][c] &gt; 0) &#123; average[r] += uiRating[r][c]; nonZero[r]++; &#125; &#125; &#125; // Calculate every user's average rating instead of every item's for (int i = 0; i &lt; userNum; i++) &#123; average[i] /= nonZero[i]; &#125; for (int i = 0; i &lt; itemNum; i++) &#123; for (int j = i + 1; j &lt; itemNum; j++) &#123; double num = 0, den = 0; double den1 = 0, den2 = 0; for(int k = 0; k &lt; userNum; k++) &#123; double diff1 = 0, diff2 = 0; if (uiRating[k][i] &gt; 0) &#123; diff1 = uiRating[k][i] - average[k]; // subtract user's average rating &#125; if (uiRating[k][j] &gt; 0) &#123; diff2 = uiRating[k][j] - average[k]; &#125; num += diff1 * diff2; den1 += diff1 * diff1; den2 += diff2 * diff2; &#125; den = Math.sqrt(den1) * Math.sqrt(den2); similarityResult[i][j] = (den == 0) ? 0 : num / den; similarityResult[j][i] = similarityResult[i][j]; &#125; &#125; return similarityResult; &#125; Prediction ComputationWeighted SumThis method computes the prediction on an item i for a user u by computing the sum of the ratings given by the user on the items similar to i. Each ratings is weighted by the corresponding similarity s_{i,j} between items i and j. P_{u,i}=\frac{\sum_{all\ similar\ items,N}(s_{i,N}*R_{u,N})}{\sum_{all\ similar\ items,N}(|s_i,N|)}RegressionIn practice, the similarities computed using cosine or correlation measures may be misleading in the sense that two rating vectors may be distant(in Euclidean sense) yet may have very high similarity. The basic idea is to use the same formula as the weighted sum technique, but instead of using the similar item N’s “raw” ratings values R_{u,N}‘s, this model uses their approximated values R_{u,N}' based on a linear regression model. \overline{R_N'}=\alpha\overline R_i+\beta+\epsilonThe respective vectors of the target item i and the similar item N are denoted by R_i and R_N.]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>reading report</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Note]]></title>
    <url>%2F2017%2F11%2F01%2Fmachine-learning-note%2F</url>
    <content type="text"><![CDATA[Week OneIntroductionWhat is machine learning? Field of study that gives computers the ability to learn without being explicitly programmed. — By Arthur Samuel A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T as measured by P improves with experience E. — By Tom Mitchell Machine learning algorithmsSupervised learning Regressiontry to map input variables to some continuous function. Classificationpredict results in a discrete output. Unsupervised learningapproach problems with little or no idea what our results should look like Example: Clustering Non-clustering Linear Regression with One VariableSome notations:m: Number of training examplesx: “input” variable / featuresy: “output” variable / “target” variable (x^{(i)},y^{(i)})$$: ith training example： Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. #### Hypothesis Function and Cost Function A slightly more formal description of supervised learning problem is that given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. ![](images/ML/figure1.png) In this example of linear regression with one variable, the **hypothesis function** can be denoted as $$ h_\theta(x)=\theta_0+\theta_1x(maybe we Chinese students are more familiar with the form like h(x)=kx+b. Here \theta_0 and \theta_1 are just parameters. And our goal is to choose \theta_0 and \theta_1 so that h_\theta(x) is close to y for our training examples(x,y).The cost function takes an average difference of all the results of the hypothesis with inputs from x’s and the actual output y’s. J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^m(\hat{y}_i-y_i)^2=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x_i)-y_i)^2This function is otherwise called the “Squared error function”, or “Mean squared error”. The coefficient 1/2 is used for gradient descent so that the partial derivative result will be cleaner. Gradient Descentthe Gradient descent algorithm: repeat until convergence { \theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\ \ (simultaneously\ update\ \theta_0\ and\ \theta_1)} The value of α should not be too small or too large.If α is too small, gradient descent can be slow.If α is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge.In general, gradient descent can converge to a local minimum, even with the learning rate α fixed. After calculating partial derivation, we can get the algorithm as : repeat until convergence { \theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)} (update θ0 and θ1 simultaneously)} Linear Algebra ReviewMatrix: 2-dimensional array.Vector: An n×1 matrix Notation:Generally the uppercase letters are used for matrix and the lowercase letters are used for vector. Matrix ManipulationAddition \begin{bmatrix} a&b\\c&d \end{bmatrix}+\begin{bmatrix} w&x\\y&z \end{bmatrix}=\begin{bmatrix} a+w&b+x\\c+y&d +z\end{bmatrix}\\Scalar multiplication \begin{bmatrix} a&b\\c&d \end{bmatrix}\times x=\begin{bmatrix} a\times x&b\times x\\c\times x&d\times x \end{bmatrix}Matrix-vector multiplication \begin{bmatrix} a&b\\c&d\\e&f \end{bmatrix}\times\begin{bmatrix} x\\y \end{bmatrix}=\begin{bmatrix}ax+by\\cx+dy\\ex+fy \end{bmatrix}Let A be an m×n matrix, x be an n-dimensional vector, then the result A×x will be an m-dimensional vector.To get yi, multiply A’s ith row with elements of vector x, and add them up. Matrix-matrix multiplication \begin{bmatrix} a&b\\c&d\\e&f \end{bmatrix}\times\begin{bmatrix} w&x\\y&z \end{bmatrix}=\begin{bmatrix} aw+by&ax+bz\\cw+dy&cx+dz\\ew+fy&ex+dy \end{bmatrix}Let A be an m×n matrix, B be an n×o matrix, then the result A×B will be an m×o matrix.The ith column of the matrix C is obtained by multiplying A with the ith column of B.(for i=1,2,…,o). Then the calculation can be simplified to matrix-vector multiplication. Matrix multiplication propertiesNot commutativeLet A and B be matrices. Then in general, A×B≠B×A. AssociativeA×(B×C)=(A×B)×C Special matrixIdentity MatrixThe identity matrix, which is denoted as I(sometimes with n×n subscript), simply has 1’s on the diagonal (upper left to lower right diagonal) and 0’s elsewhere. For example: \begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}For any matrix A, A×I=I×A=A Matrix InverseIf A is an m×m matrix, and if it has an inverse(not all matrix have an inverse), A\times A^{-1}=A^{-1}\times A=IMatrices that don’t have an inverse are singular or degenerate. Matrix TransposeLet A be an m×n matrix, and let B=A^T. Then B is an n×m matrix, and B_{ij}=A_{ji}Week TwoMultivariate Linear Regressionfrom one variable to multiple variablesIn the first week’s course, we learned linear with one variable x, and the hypothesis could be h_{\theta}(x)=\theta_0+\theta_1x. As a matter of fact, there can be more than one variables. So here’re the new notations:m: the number of training examplesn: the number of features x^{(i)}$$: input (features) of ith training example `$x_j^{(i)}: value of feature j in ith training example The hypothesis should be transformed to: $$h_{\theta}(x)=\theta_0x_0+\theta_1x_1+...+\theta_nx_n=\sum_{i=0}^n\theta_nx_n(for convenience of notation, we define x0=1).Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as: h_{\theta}(x)=\begin{bmatrix}\theta_0&\theta_1&...&\theta_n\end{bmatrix}\begin{bmatrix}x_0\\x_1\\...\\x_n\end{bmatrix}=\theta^TxWe can see that linear regression with one variable is just the special case when n=1. Similarly, the new gradient descent algorithm would be represented as:repeat until convergence{ \theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}\ \ (simultaneously\ update\ \theta_j\ for\ j=0,...,n)} Feature ScalingThe idea is to make sure features are on a similar scale so that the gradient descent can be sped up.Generally, get every feature into approximately a -1]]></content>
      <tags>
        <tag>machine learning</tag>
        <tag>learning note</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[REM Reading Report]]></title>
    <url>%2F2017%2F10%2F31%2FREM-reading-report%2F</url>
    <content type="text"><![CDATA[REM Reading Report &amp; Future PlanOverviewREM &amp; PoUWREM(Resource-Efficient Mining) is a new blockchain mining framework that uses trusted hardware(Intel SGX) to achieve a fraction of the waste of PoW. It’s partially decentralized, and it achieves security guarantees similar to PoW. Its key idea, Proof-of-Useful-Work(PoUW), involves miners providing trustworthy reporting on CPU cycles they devote to inherently useful workloads. In a PoUW system, users can utilize their CPUs for any desired workload, and can simultaneously contribute their work towards securing a blockchain. Fundamental impediment of PoWAs Satoshi Nakamoto put Proof-of-Work(PoW) in BitCoin, PoW is widely used in blockchain so that the consensus can prevent an attacker from gaining majority power by cheaply masquerading as multiple machines. However, PoW in blockchains are wasteful, and it even has another name: Proof of Waste. PoWs serve no useful purpose beyond consensus and incur huge monetary and environmental costs, which is against FinTech’s demand. PoETMany attempts have been made to create a more resource useful consensus. However they have serious limitations.Intel recently introduced a new instruction set architecture extension in Intel CPUs called Software Guard Extension(SGX). SGX permits the execution of trustworthy code in an isolated, tamper free environment, and can prove remotely that outputs represent the result of such execution. And Intel proposed another innovative consensus: Proof of Elapsed Time(PoET). However, PoET presents two notable technical challenges: broken chip problem(an attacker that can corrupt a single SGX-enabled node can win every consensus round and break the system completely) and stale chip problem(miners tend to power mining rigs with cheap, outmoded SGX-enabled CPUs used solely for mining). REM addresses both the stale and broken chip problems. SGXSGX enables process execution in a Trusted Execution Environment(TEE), and specifically in SGX in a protected address space known as an enclave. An enclave protects the confidentiality and the integrity of the process from certain forms of hardware attack and other processes on the same host, including privileged processes like operating systems. SGX signs quotes in attestations using a group signature scheme called Enhanced Privacy ID or EPID, and Intel made the design choice that attestations can only be verified by accessing Intel’s Attestation Service(IAS), a public Web service maintained by Intel whose primary responsibility is to verify attestations upon request. PoUW and REMtwo key assumptionsThe basic idea of PoUW, and thus REM, is to replace the wasteful computation of PoW with arbitrary useful computation. Since it is only partially decentralized, it relies for security on two key assumptions about the hardware manufacturer’s behavior: First, Intel correctly manages identities, specifically that it assigns a signing key(used for attestations) only to a valid CPU. Second, Intel does not blacklist valid nodes in the network, rendering their attestations invalid when the IAS is queried.If Intel didn’t follow the two assumptions and was detected in any context, the company’s reputation and the perceived utility of SGX would be undermined and Intel would gain little revenue. So generally we can trust Intel SGX. the architecture of REMThere are three types of entities in the ecosystem of REM: a blockchain agent, one or more REM miners, and one or more useful work clients.The useful work clients supply useful workloads to REM miners in the form of PoUW tasks, each of which encompass a PoUW enclave and some input.The blockchain agent collects transactions and generates a block template, a block lacking the proof of useful work(PoUW). A REM miner will attach the required PoUW and return it to the agent. The agent then publishes the full block to the P2P network, making it part of the blockchain and receiving the corresponding reward. Although CPU cycles would have been a more accurate metric, they are vulnerable to manipulation. The operating system may set their values arbitrarily and even have them double-count cycles. Therefore, REM chose instruction counting for securely evaluating effort with the existing tools available in SGX.The workflow of the PoUW toolchain is as shown below: First, the useful work code (usefulwork.cpp), C / C++ source code, is assembled while reserving a register as the instruction counter. Next, the assembly code is rewritten by the toolchain such that the counter is incremented at the beginning of each basic block (a linear code sequence with no branches) by the number of instructions in that basic block. The count is performed at the beginning of a block rather than its end to prevent a cheater from jumping to the middle of a block and gaining an excessive count. PoUW attestations is formed with two-layer hierarchical attestations. Zhang et al hard-code only a single program’s fingerprint into the blockchain, a static-analysis tool called compliance checker. The compliance checker runs in a trusted environment and takes a user-supplied program as input.Every PoUW then includes two parts: The useful work program attestation on the mining success, and an attestation from the compliance checker of the program’s compliance. Note that the compliance attestation and the program’s attestation must be signed by the same CPU. Otherwise an attacker that compromises a single CPU could create fake compliance attestations for invalid tasks. REM读书报告中文版概述REM与PoUWREM（资源节约型挖矿）为一种新型的使用可信任硬件（Intel SGX）的区块链挖矿框架，大幅度减少了工作量证明(PoW)的浪费。部分去中心化的REM实现了与PoW类似的安全性保证。其核心思想，有效工作量证明(PoUW)，使得矿工提供所贡献的有效工作负载的CPU周期的可信任证明。在PoUW系统中，用户可以将CPU用于任何期望工作负载，同时贡献其工作来使得区块链更安全。 工作量证明的重大缺陷中本聪在比特币中使用的工作量证明PoW，被广泛应用于区块链技术，该共识机制可以有效防止恶意攻击者通过低成本地伪装成多台机器以获得大多数权力。然而，区块链的工作量证明是浪费的，它甚至还有一个别名：浪费量证明。除了实现共识，它别无它用，却造成了大量的资金与环境浪费，而这与金融科技的愿景是相违背的。 PoET为了实现更加节约型的共识机制，已经有许多共识机制进行尝试。不过往往这些共识机制都有非常严格的限制。近日Intel在其CPU中推行了一种新型指令集扩展结构，称为SGX(Software Guard Extension)。SGX允许可信任代码在隔绝、防干扰的环境中执行，并且远程验证程序执行的输出结果。在SGX的基础上，Intel提出了新的共识机制：运行时间证明(Proof of Elapsed Time, PoET)。然而，PoET有两个显著的技术挑战：芯片沦陷问题和芯片老旧问题。 SGXSGX使进程执行在可信任执行环境(Trusted Execution Environment, TEE)中，而且SGX中专门有一块受保护的名为enclave（暂译为保护领地）的地址。Enclave保证了进程的机密性和完整性，避免来自硬件的攻击以及其他优先进程如操作系统等影响。SGX在认证过程中使用一种名为强化隐私ID(Enhanced Privacy ID, EPID)的群签名方案。同时Intel决定认证只能通过Intel认证服务(Intel’s Attestation Service)进行，IAS是Intel维持的公共网络服务，其首要责任是验证请求的认证。 PoUW和REM两个关键假设PoUW及REM的基本思想是将PoW中浪费的计算量替代为任意有效计算量。因为它是部分去中心化的，所以其安全性依赖关于硬件制造商行为的两个关键假设：第一，Intel正确地进行身份管理，特别是对一个有效CPU只分配唯一的签名密钥（用于认证）。第二，Intel不会针对网络中的节点设立黑名单，不会当IAS请求响应时假装其认证无效。如果Intel违背上述两条假设中的任意一条，只要被检测出来，其公司的声誉以及用户对SGX的认同会大大削减，Intel得不偿失。所以总体而言我们可以信任Intel SGX。 REM框架在REM的生态中有三类实体：一位区块链代理，一位或多位REM矿工，以及一位或多位有效工作客户。有效工作客户将有效工作负载以PoUW任务的形式提供给REM矿工，每一个任务包含一个PoUW enclave以及一些输入。区块链代理收集处理，生成一个区块模板，一个没有PoUW的区块。REM矿工会附上所需的PoUW的区块并返回该区块。区块链代理随后将完整的区块发布至P2P网络，使其成为区块链的一部分，而矿工也获得对应的奖励。 尽管CPU周期也许是更准确的测量标准，它却易被操控。操作系统可以任意修改其值，甚至可以翻倍。因此，在已有SGX工具的情况下，REM选择对指令计数，以保证评价的安全性。PoUW工具链的流程如下：首先，有效工作代码，C或C++源码，以汇编形式处理，同时预留一个寄存器作为指令计数器。然后，工具链将汇编代码进行重写，使得计数器在每个基块（无分支的线性代码序列）时根据基块的指令数目进行计数。同时为了保证正确性，工具链还实现了两个强制措施：强制代码不可写入，以及强制单线程操作。 PoUW的认证过程是两级认证。Zhang Fan等人硬编码单个程序的“指纹”(fingerprint)——一种称为一致性检查的静态分析工具——至区块链中。一致性检查在可信任环境中运行，以用户提供的程序作为输入。每一份PoUW包含两部分：成功挖矿时的有效工作程序认证，以及来自一致性检查的程序一致性认证。注意到一致性认证以及程序认证的签名必须来自同一CPU。否则攻击者可以攻击单个CPU后为无效任务伪造一致性认证。 Personal perspective虽然在5.1的Why Count Instructions中解释了为什么采用指令计数的方式。 While instructions are reasonable estimates of the CPU effort, CPU cycles would have been a more accurate metric. However, although cycles are counted, and the counts can be accessed through the CPU’s performance counters, they are vulnerable to manipulation. The operating system may set their values arbitrarily, allowing a rational operator, who controls her own OS, to improve her chances of finding a block by faking a high cycle count. Moreover, counters are incremented even if an enclave is swapped out, allowing an OS scheduler to run multiple SGX instances and having them double-count cycles. Therefore, while instruction counting is not perfect, we find it is the best method for securely evaluating effort with the existing tools available in SGX. 不过个人认为还是可以采用执行时间证明(Proof of Execution Time, PoET’)的方法实现共识。CPU的晶振周期与时钟周期是固定的（本来我以为单个机器的机器周期是固定不变的，但是机器周期即为CPU周期，所以才知道是可以由操作系统改变的）。可以使用执行时间÷时钟周期（晶振周期）÷某一常数近似估计指令数目，而不需要专门设计工具链对汇编语言进行计数。另外，一篇名为Malware Guard Extension: Using SGX to Conceal Cache Attacks(Extend Version)的论文也值得引起我们对SGX安全性的重视。（这篇论文是我近日查找资料时不经意看到的搜狐报道的英特尔 SGX：是用来隐藏恶意软件，而不是保护系统；会泄漏加密密钥后去找的论文，还没有仔细看）]]></content>
      <tags>
        <tag>reading report</tag>
        <tag>blockchain</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[区块链的共识机制]]></title>
    <url>%2F2017%2F10%2F24%2Fabout-blockchain-consensus%2F</url>
    <content type="text"><![CDATA[区块链的共识机制 Blockchain Consensus区块链如何在分布式场景下达成一致，关键便是共识机制。共识协议的稳定性和防攻击性保证了区块链的安全运行。现整理常见的共识机制，包括PoW工作量证明(Proof of Work)、PoET运行时间证明(Proof of Elapsed Time)、PBFT拜占庭容错算法(Practical Byzantine Fault Tolerance)，以及两种新型的共识机制：PoL幸运值证明(Proof of Luck)以及PoUW有效工作证明(Proof of Useful Work)。 工作量证明 Proof of Work工作量证明是矿工在处理交易数据（对数据也是进行哈希）的同时不断的进行哈希计算，求得一位前23位为0的哈希值，这个值成为nonce黄金数。当全网有一位矿工哈希出nonce时，他就会把自己打包的区块公布出去，其他节点收到区块验证区块后就会一致性认为这个区块接到了区块链上，就继续进行下一个区块的打包和哈希计算。在这个过程中，中本聪通过算力的比拼牺牲了一部分最终一致性（因为会有分叉的产生）并且需要等待多个确认，但是这种简单暴力的方法却保证了整个区块链系统的合法性，而且把区块链系统的健壮性提升到极致，就算全网只剩下一个节点运行，这个区块链系统还是会继续运行下去。最后POW也充分提高了区块链系统的安全性，依靠51%攻击理论去破坏区块链系统是只有政府或者疯子才会采取的方法。 优点： 完全去中心化 节点自由进出，容易实现。 破坏系统花费的成本巨大 缺点： 对节点的性能网络环境要求高 无法达成最终一致性 浪费能源 详情可参见 Bitcoin: A Peer-to-Peer Electronic Cash System 消耗时间证明 Proof of Elapsed Time为了提高分布式共识的效率，一个优良的彩票函数(lottery function)有以下几个特征： 公平：该函数应在最广泛的可能参与者中进行领导选举的分布 投入：控制领导选举进程的花费，应当与从中的收益成正比 验证：对于所有的参与者而言，验证领导为合法选举产生的步骤应相对简单 消耗时间证明算法使用新型安全的CPU指令来实现上述目标。通过这些特点，PoET保证了领导选举过程的安全性和随机性，而不需要像大多数证明算法一样需要消耗大量资源。 详情可参见Introduction - Sawtooth latest documentation(Proof of Elapsed Time) 拜占庭容错算法 Practical Byzantine Fault Tolerance这是一种基于消息传递的一致性算法，算法经过三个阶段达成一致性，这些阶段可能因为失败而重复进行。假设节点总数为3f+1，f为拜占庭错误节点： 当节点发现leader作恶时，通过算法选举其他的replica为leader。leader通过pre-prepare （第一个协议阶段）消息把它选择的 value广播给其他replica节点，其他的replica节点如果接受则发送 prepare（第二个协议阶段），如果失败则不发送。一旦2f个节点接受prepare消息，则节点发送commit（第三个协议阶段）消息。当2f+1个节点接受commit消息后，代表该value值被确定 如下图表示了4个节点，0为leader，同时节点3为fault节点，该节点不响应和发出任何消息。最终节点状态达到commited时，表示该轮共识成功达成。 注：预准备阶段（pre-prepare）： 主节点分配一个序列号n给收到的请求，然后向所有备份节点群发预准备消息，预准备消息的格式为&lt;&lt;PRE-PREPARE,v,n,d&gt;,m&gt;，这里v是视图编号，m是客户端发送的请求消息，d是请求消息m的摘要。 准备阶段（prepare）： 如果备份节点i接受了预准备消息&lt;,m&gt;，则进入准备阶段。在准备阶段的同时，该节点向所有副本节点发送准备消息&lt;PREPARE,v,n,d,i&gt;，并且将预准备消息和准备消息写入自己的消息日志。如果看预准备消息不顺眼，就什么都不做。 确认阶段（commit）： 当(m,v,n,i)条件为真的时候，副本i将&lt;COMMIT,v,n,D(m),i&gt;向其他副本节点广播，于是就进入了确认阶段。优点：上述其他算法都脱离不了币的存在，币的存在及它的奖励机制会让区块链这一单一的世界穷者更穷，富者更富。共识效率高，可实现高频交易。缺点：当系统只剩下33%的节点运行时，系统会停止运行。 幸运值证明 Proof of Luck来自UC Berkley 的 Mitar Milutinovic等学者为了克服PoW的资源消耗、交易效率低等问题，在可信任执行环境(Trusted Execution Environments)中提出了一种新型共识算法：幸运值证明。在攻击者理性以及大多数参与者良性的情况下，使用最少量的资源与算力以实现交易验证的低延时。幸运值证明由两个函数组成：PolRound（幸运值证明循环）以及PolMine（幸运值证明挖矿）。在每次循环的开始，参与者准备通过调用PoLRound函数，传递已知的最新区块。经过一段时间（ROUND_TIME）后，参与者调用PoLMine函数挖掘新的区块。PolMine函数会生成[0,1)区间内一个满足均匀分布的随机值，根据该随机值决定这一轮所有的区块中的胜出区块。与PoET类似，Proof of Luck也是一种彩票函数，保证了随机性与安全性。 有效工作量证明 Proof of Useful Work面对工作量证明造成的大量浪费，Cornell的Fan Zhang等学者希望可以更充分地利用CPU资源，因此在改进PoET算法的基础上提出了有效工作量证明算法，并根据有效工作量证明设计了名为REM（资源节约型挖矿）的系统。REM系统中有三类实体：区块链代理，一个或多个REM矿工，以及一个或多个有效工作客户。有效工作客户将需要计算的工作以PoUW任务的形式提供给REM矿工，矿工接受区块模板及PoUW工作后便执行有效工作指令，可以执行科学实验、药学研究等。与PoW的完全去中心化不同的是，PoUW实现的是部分中心化(partially decentralized)。它依赖于Intel的SGX(Software Guard Extension)平台。SGX允许在隔离、免受干扰的环境中执行可信任代码，并且通过远程证明输出代表着执行结果。当然，这一切是基于SGX可信的基础。 Reference: 区块链共识机制浅谈 Bitcoin: A Peer-to-Peer Electronic Cash System Introduction - Sawtooth latest documentation Proof of Luck: an Efficient Blockchain Consensus Protocol REM: Resource-Efficient Mining for Blockchains]]></content>
      <tags>
        <tag>区块链</tag>
        <tag>技术</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[21岁 你好]]></title>
    <url>%2F2017%2F10%2F23%2F21-year-birthday-reflection%2F</url>
    <content type="text"><![CDATA[霜降。又到了这个平凡而又特殊的日子，转眼已经21岁了呐。不会矫情地慨叹“要奔三”等话语，只是轻道一句，时光太匆匆。从紫金港到玉泉，从freshman到sophomore再到junior，从懵懂无知到懵懂依旧。只是像一只蜗牛罢，一步一步倔强地往上爬。这个学期算是加入了计院的系统安全与虚拟化的实验室，和同级的佼佼者、研究生、老师同处一室，感受到自己的渺小，不过也作出了自己的小小贡献，亦愈发迫切地渴望提高。区块链Blockchain，似曾相识而又陌生的分布式技术，在阅读了一篇篇论文后，方有了一定的了解。而接下来要构思的专利及论文，依然路漫漫。进入大学后第一次获得了奖学金。虽然是凭借在团学联的学生工作获得的社会工作奖学金，而不是证明成绩的学业奖学金，不过知足啦。大三学年，争取在学业与创新方面都有所进步。已经浪了两年，望不再辜负自己。QQ上的生日提示肆意传播，从一大早起便收到了来自认识的不认识的好友的生日祝福。相似的祝福语，虽然只是系统自动发送的祝福语，为沉寂的手机带来一丝振动，不会一个一个去回复，可能会带来双方的尴尬，不过还是谢谢啦。以及那些记得我生日的好孩子们，感谢认识你们，比心。❤21岁，新的十年之始。愿不忘初心，砥砺前行。]]></content>
      <tags>
        <tag>随笔</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux系统下常用指令]]></title>
    <url>%2F2017%2F10%2F21%2FlinuxCommands%2F</url>
    <content type="text"><![CDATA[Foreword现整理一下Linux系统下常用的指令，如无特殊说明，这些指令也都可以在macOS系统上执行。所有指令均在Ubuntu 16.04.3 LTS版本下运行通过。参考Stanford大学的Chris Gregg在YouTube上的教程 Linux文件系统简介Linux的文件系统可以用数据结构中的树表示。使用tree指令便可以以树状结构列出目录内容（不过macOS下不能直接使用tree指令），比较直观清晰。 工作路径在Terminal下可以输入指令 pwd (print working directory)显示当前的工作路径（绝对路径）。工作路径有两种：绝对路径与相对路径。绝对路径就是从根目录(root directory)/开始的路径，相对路径是从主目录(home directory)~开始的路径。一般情况下我们都可以使用相对路径来表示我们的工作路径。另外，.表示当前路径，..表示上一级路径。 cdcd: change directorycd 路径名： 定位至目标目录cd 或cd ~： 定位至主目录cd - ： 返回至之前目录cd ..： 返回至父目录 catcat: concatenatecat 文件名： 输出文件内容。注意对文件夹无法进行cat命令。cat 文件名1 文件名2 ... 文件名n： 输出多个文件内容 lsls: listls： 列出所有非隐藏文件ls -1： 以单列的形式列出所有非隐藏文件。（即一行只显示一个文件/文件夹）ls -l： 列出所有非隐藏文件，包含更多信息：包括文件大小、是否为文件夹、ls -a： 列出所有文件，包括隐藏文件。（文件名以.开头的都是隐藏文件/隐藏文件夹）ls -rt： 按照时间修改顺序倒序显示文件ls -后面的指令可以配合使用，例如ls -alrt便是以倒序显示所有文件，并且包含更多详细信息。 pushd / popdpushd: push directorypopd: pop directory在Terminal中，经过几次cd改变路径后，有没有什么快速跳转至之前某一路径的快捷指令呢？答案就是使用pushd和popd指令，利用压栈和出栈的思想实现快速跳转。具体使用方法：pushd 新路径： 对当前路径进行压栈操作，然后跳转至新路径popd： 跳转至栈顶路径举例：假设当前我的路径为~/Documents，使用指令pushd ../Downloads后，栈中保存的路径为~/Documents，而我的路径跳转到了主目录下的Downloads文件夹。此时使用指令popd便可以跳转至栈顶路径，即~/Documents。 manman: manualman 指令名： 查看对应命令的指导手册例如：man ls，便可以查看完整的ls指令。进入指导手册后，还可以进行一些操作：可以通过上下方向键，或者J和K进行文本的浏览。使用/查找内容指令可以查找特定内容。 在查找时按下n（即键盘上的N键）可以查找下一处，按下N（即shift和N一起按）可以查找上一处。按下Q即退出指导手册。 man -k 关键词： 显示所有指导手册中带有关键词的指令。例如： man -k printf： 我们会发现，除了printf外，还有许多长得类似的指令。同时macOS与Ubuntu上显示的指令是不一样的。Linux only：在Ubuntu下有printf(1)和printf(3)两条名称相同的指令，其中printf(1)是UNIX的系统指令，而printf(3)即是我们熟悉的C语言中的输出指令。可以通过指令man 3 printf查看C语言的printf手册。所以我们在Linux下使用man也可以查看C语言的手册。 cp / mvcp: copymv: movecp 源文件 新路径： 复制源文件至新路径下，如果新路径原先不存在，则会自动生成对应路径（即文件夹）。cp -r 源文件夹 新路径： 复制文件夹至新路径下。-r 表示recursive即递归，表示将源文件夹下的所有文件都复制，即实现了整个文件夹的复制。注意新路径下如果存在与源文件同名的文件，则直接会被覆盖。 mv的用法与cp基本一样。不过对文件夹进行操作时无需使用-r命令。 rmrm: removerm 文件rm -r 文件夹注意rm操作会直接删除对应文件/文件夹，而不存在回收站之类的东西（虽然可以有backup，但是……我不会）。所以在执行rm操作时需谨慎。 grepgrep &quot;关键字&quot; 文件名： 列出文件中所有包含pattern的行 关键字中有两个特殊的字符，.和。.可以代表任一字符`表示*前面的字符可以连续出现任意次数（包括0次）。 例如：grep “dif*” demo.txt`中，会将包含di、dif、diff的所有行都列出来。 配合正则表达式，grep可以实现高效的查找。这里仅举几个例子供参考：grep &quot;[ab]e&quot; fileName： 查找所有包含ae或者be的行grep &quot;^A&quot; fileName： 列出所有以A开头的行grep &quot;^B&quot; fileName： 列出所有以B结尾的行具体的可以使用man grep指令查看手册或者网上搜寻正则表达式的相关教程。 findfind . -name 文件名： 在当前路径下（包括当前路径的子路径）寻找对应文件与grep命令稍有不同，在find命令下使用*可以表示任意字符。find . -type d： 在当前路径下寻找所有文件夹 ctrl+C中断当前正在执行的指令]]></content>
      <tags>
        <tag>linux</tag>
        <tag>tips</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[macOS常用快捷键整理]]></title>
    <url>%2F2017%2F10%2F20%2FmacOS-shortcuts%2F</url>
    <content type="text"><![CDATA[Foreword一些功能及其对应的符号：⌘：command 其功能与Windows系统下的Ctrl类似，例如command+C对应Windows下的Ctrl+C等。⌥：option 一般情况下需与其他功能键配合使用；在输入文字时option+字母可以快捷输入各种符号。⌃：control 注意不要与Windows的Ctrl混淆。⇧：shift 通常配合command一起使用。⇪：caps lock 用于切换大小写、中英文输入法，符号与shift相似，不过在后面的快捷键中不会用到该键位。 系统常用快捷键下面列举的快捷键在macOS系统下的大部分软件内都可以操作，非常实用。一些快捷键如果有记忆方式我会在括号中标注~⌘ C：复制⌘ V：粘贴⌘ X：剪切注意在Finder中无法使用⌘ X对文件进行剪切操作。如果需要对文件进行剪切操作，则应对文件使用⌘ C后，在需要粘贴的地方使用快捷键⌥ ⌘ V即可实现剪切与粘贴的操作。⌘ Z：撤销 ⌘ A(command all)：全选⌘ S(command save)：保存⌘ F(command find)：查找查找一般是查找某些文字。在Finder中进行查找时，会找出所有包含对应关键字的文件。 ⌘ H(command hide)：隐藏当前应用的所有窗口⌘ M(command minimize)：最小化当前窗口关于隐藏和最小化，看上去差不多不过还是有一些不同的。比如打开了多个Word文档，⌘ H会把所有的Word文档都隐藏，而⌘ M只会将最前面的一个Word最小化。另外，被隐藏的应用可以通过⌘ tab进行切换，而被最小化的应用无法直接通过⌘ tab进行切换（下文会提到）。 ⌘ tab：切换应用按下⌘ tab后，继续按住command键，可以看到所有当前正在运行的应用程序。然后按tab便可以在不同的应用之间切换。注意无法直接切换到被最小化的程序，需要用到下面这条比较复杂的快捷键。⌘ tab ⌥：切换至最小化的程序使用方法：按下⌘ tab，调出当前运行的程序后，一直按住⌘ command，按tab将光标定位到想要切换的程序，松开tab，按下⌥ option键，即可切换至最小化的应用程序。⌘ ~：同一程序不同窗口间切换有时可能会打开同一应用程序的多个窗口。还是以打开多个Word文档为例：需要在不同文档间切换时就可以使用该快捷键。~对应的就是数字键1左边、tab上面的那一颗按键。 ⌘ Q(command quit)：退出应用⌘ W(command window)：关闭当前窗口⌘ Q是将整个应用程序退出，同时不再占用任何进程资源；而⌘ W则只是将程序的一个窗口关闭。（可能有点难懂，还是以打开多个Word文档为例：⌘ Q是将所有窗口的关闭，同时Word本身的进程也退出了；而⌘ W则只是关闭其中的一个文档，其他文档不受印象。） ⌘ O(command open)：打开⌘ N(command new)：新建 ⌘ ,：偏好设置很多应用程序都会有偏好设置，可以在里面对该程序进行一些设置。⌘ space：切换输入法⌃ space：调用spotlight注意切换输入法不是Windows上的Ctrl+Shift，在macOS上是⌘ space（space即空格）。注意有可能部分机器上切换输入法和调用spotlight的快捷键正好相反。 ⇧ ⌘ 3：全屏幕截图⇧ ⌘ 4：选中区域截图macOS自带的截图快捷键，默认保存格式为PNG。 ⌃ ⌘ space：输入Emoji表情在需要输入Emoji的地方使用快捷键⌃ ⌘ space就可以输入各种Emoji表情啦~ 浏览器常用快捷键（以Safari为例）⌘ T(command tab)：新建标签页⌘ W(command window)：关闭标签页⇧ ⌘ ←：切换到前一个标签页⇧ ⌘ →：切换到后一个标签页⌘ 1~9：切换到第1~9个标签页上述快捷键不只是局限于浏览器，另外一些带有标签页的软件也都可以使用其中的部分或全部快捷键，例如PDF Expert等。 ⌘ R(command refresh)：刷新⌘ ←：后退⌘ →：前进在网页浏览时最常用到的几个快捷键 ⌘ L(command link)：编辑地址栏的地址⇧ ⌘ L：显示边栏⌘ D：添加书签⇧ ⌘ D：添加到阅读列表 文档处理常用快捷键（以Word为例）⌘ B(command bold)：加粗⌘ I(command italic)：斜体⌘ U(command underline)：下划线这三个快捷键在Word的工具栏中分别对应B*I*U三个选项。 ⌘ ←：光标定位至当前行的行首⌘ →：光标定位至当前行的行尾⌘ ↑：光标定位至当前段的段首⌘ ↓：光标定位至当前段的段尾⇧ ←：选中光标左边的单个字符⇧ →：选中光标右边的单个字符通过⌘和⇧ 的配合使用可以快速选择多个字符 ⌥ 1~9 A~Z：输入一些字符 CheatSheet介绍当然，macOS的快捷键远不止此，上述介绍的只是常用的快捷键。还有一些快捷键可以在Apple的官网查看。另外在此推荐一款快捷键软件：CheatSheet是一款查看快捷键的软件，可以访问其官网免费下载。在安装后只需要长按⌘ 便可查看当前软件的（几乎）全部快捷键。]]></content>
      <tags>
        <tag>tips</tag>
        <tag>macOS</tag>
        <tag>效率</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F10%2F18%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
