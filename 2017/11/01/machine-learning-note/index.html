<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="machine learning,learning note," />










<meta name="description" content="Week OneIntroductionWhat is machine learning?  Field of study that gives computers the ability to learn without being explicitly programmed. — By Arthur Samuel A computer program is said to learn from">
<meta name="keywords" content="machine learning,learning note">
<meta property="og:type" content="article">
<meta property="og:title" content="Machine Learning Note">
<meta property="og:url" content="http://haelchan.me/2017/11/01/machine-learning-note/index.html">
<meta property="og:site_name" content="Hael&#39;s Personal Website">
<meta property="og:description" content="Week OneIntroductionWhat is machine learning?  Field of study that gives computers the ability to learn without being explicitly programmed. — By Arthur Samuel A computer program is said to learn from">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://haelchan.me/images/ML/figure1.jpg">
<meta property="og:image" content="http://haelchan.me/images/ML/sigmoidFunction.jpg">
<meta property="og:image" content="http://haelchan.me/images/ML/linearOverfit.jpg">
<meta property="og:image" content="http://haelchan.me/images/ML/logisticOverfit.jpg">
<meta property="og:image" content="http://haelchan.me/images/ML/neuralEx.jpg">
<meta property="og:updated_time" content="2017-11-15T11:51:23.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Machine Learning Note">
<meta name="twitter:description" content="Week OneIntroductionWhat is machine learning?  Field of study that gives computers the ability to learn without being explicitly programmed. — By Arthur Samuel A computer program is said to learn from">
<meta name="twitter:image" content="http://haelchan.me/images/ML/figure1.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://haelchan.me/2017/11/01/machine-learning-note/"/>





  <title>Machine Learning Note | Hael's Personal Website</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hael's Personal Website</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://haelchan.me/2017/11/01/machine-learning-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hael Chan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hael's Personal Website">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Machine Learning Note</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-01T00:34:11+08:00">
                2017-11-01
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2017-11-15T19:51:23+08:00">
                2017-11-15
              </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/11/01/machine-learning-note/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/11/01/machine-learning-note/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2017/11/01/machine-learning-note/" class="leancloud_visitors" data-flag-title="Machine Learning Note">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="Week-One"><a href="#Week-One" class="headerlink" title="Week One"></a>Week One</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><strong>What is machine learning?</strong></p>
<blockquote>
<p>Field of study that gives computers the ability to learn without being explicitly programmed. — By Arthur Samuel</p>
<p>A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T as measured by P improves with experience E. — By Tom Mitchell</p>
</blockquote>
<h4 id="Machine-learning-algorithms"><a href="#Machine-learning-algorithms" class="headerlink" title="Machine learning algorithms"></a>Machine learning algorithms</h4><p><strong>Supervised learning</strong></p>
<ul>
<li><p>Regression<br>try to map input variables to some continuous function. </p>
</li>
<li><p>Classification<br>predict results in a discrete output.</p>
</li>
</ul>
<p><strong>Unsupervised learning</strong><br>approach problems with little or no idea what our results should look like</p>
<p>Example:</p>
<ul>
<li>Clustering</li>
<li>Non-clustering</li>
</ul>
<h3 id="Linear-Regression-with-One-Variable"><a href="#Linear-Regression-with-One-Variable" class="headerlink" title="Linear Regression with One Variable"></a>Linear Regression with One Variable</h3><p>Some notations:<br><strong>m</strong>: Number of training examples<br><strong>x</strong>: “input” variable / features<br><strong>y</strong>: “output” variable / “target” variable<br><strong><script type="math/tex">(x^{(i)},y^{(i)})</script></strong> : ith training example: Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. </p>
<h4 id="Hypothesis-Function-and-Cost-Function"><a href="#Hypothesis-Function-and-Cost-Function" class="headerlink" title="Hypothesis Function and Cost Function"></a>Hypothesis Function and Cost Function</h4><p>A slightly more formal description of supervised learning problem is that given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis.</p>
<p><img src="/images/ML/figure1.jpg" alt=""></p>
<p>In this example of linear regression with one variable, the <strong>hypothesis function</strong> can be denoted as </p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x</script><p>(maybe we Chinese students are more familiar with the form like h(x)=kx+b) Here <script type="math/tex">\theta_0</script> and <script type="math/tex">\theta_1</script> are just parameters. And our goal is to choose <script type="math/tex">\theta_0</script> and <script type="math/tex">\theta_1</script> so that <script type="math/tex">h_\theta(x)</script> is close to y for our training examples(x,y).<br>The <strong>cost function</strong> takes an average difference of all the results of the hypothesis with inputs from x’s and the actual output y’s.</p>
<script type="math/tex; mode=display">J(\theta_0,\theta_1)=\frac{1}{2m}\sum_{i=1}^m(\hat{y}_i-y_i)^2=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x_i)-y_i)^2</script><p>This function is otherwise called the “Squared error function”, or “Mean squared error”. The coefficient 1/2 is used for gradient descent so that the partial derivative result will be cleaner.</p>
<h4 id="Gradient-Descent"><a href="#Gradient-Descent" class="headerlink" title="Gradient Descent"></a>Gradient Descent</h4><p>the Gradient descent algorithm:</p>
<p>repeat until convergence {</p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1)\ \ (simultaneously\ update\ \theta_0\ and\ \theta_1)</script><p>} </p>
<p>The value of α should not be too small or too large.<br>If α is too small, gradient descent can be slow.<br>If α is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge.<br>In general, gradient descent can converge to a local minimum, even with the learning rate α fixed.</p>
<p>After calculating partial derivation, we can get the algorithm as :</p>
<p>repeat until convergence {</p>
<script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})</script><script type="math/tex; mode=display">\theta_1:=\theta_1-\alpha\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})\cdot x^{(i)}</script><p>  <em>(update θ0 and θ1 simultaneously)</em><br>}</p>
<h3 id="Linear-Algebra-Review"><a href="#Linear-Algebra-Review" class="headerlink" title="Linear Algebra Review"></a>Linear Algebra Review</h3><p><strong>Matrix:</strong> 2-dimensional array.<br><strong>Vector:</strong> An n×1 matrix</p>
<p><em>Notation</em>:Generally the uppercase letters are used for matrix and the lowercase letters are used for vector.</p>
<h4 id="Matrix-Manipulation"><a href="#Matrix-Manipulation" class="headerlink" title="Matrix Manipulation"></a>Matrix Manipulation</h4><p><strong>Addition</strong></p>
<script type="math/tex; mode=display">\begin{bmatrix} a&b\\c&d \end{bmatrix}+\begin{bmatrix} w&x\\y&z \end{bmatrix}=\begin{bmatrix} a+w&b+x\\c+y&d +z\end{bmatrix}\\</script><p><strong>Scalar multiplication</strong></p>
<script type="math/tex; mode=display">\begin{bmatrix} a&b\\c&d \end{bmatrix}\times x=\begin{bmatrix} a\times x&b\times x\\c\times x&d\times x \end{bmatrix}</script><p><strong>Matrix-vector multiplication</strong></p>
<script type="math/tex; mode=display">\begin{bmatrix} a&b\\c&d\\e&f \end{bmatrix}\times\begin{bmatrix} x\\y \end{bmatrix}=\begin{bmatrix}ax+by\\cx+dy\\ex+fy \end{bmatrix}</script><p>Let <em>A</em> be an m×n matrix, <em>x</em> be an n-dimensional vector, then the result <em>A×x</em> will be an m-dimensional vector.<br>To get yi, multiply A’s ith row with elements of vector x, and add them up.</p>
<p><strong>Matrix-matrix multiplication</strong></p>
<script type="math/tex; mode=display">\begin{bmatrix} a&b\\c&d\\e&f \end{bmatrix}\times\begin{bmatrix} w&x\\y&z \end{bmatrix}=\begin{bmatrix} aw+by&ax+bz\\cw+dy&cx+dz\\ew+fy&ex+dy \end{bmatrix}</script><p>Let <em>A</em> be an m×n matrix, <em>B</em> be an n×o matrix, then the result <em>A×B</em> will be an m×o matrix.<br>The ith column of the matrix C is obtained by multiplying A with the ith column of B.(for i=1,2,…,o). Then the calculation can be simplified to matrix-vector multiplication.</p>
<h4 id="Matrix-multiplication-properties"><a href="#Matrix-multiplication-properties" class="headerlink" title="Matrix multiplication properties"></a>Matrix multiplication properties</h4><p><strong>Not commutative</strong><br>Let A and B be matrices. Then <em>in general</em>, A×B≠B×A.</p>
<p><strong>Associative</strong><br>A×(B×C)=(A×B)×C</p>
<h4 id="Special-matrix"><a href="#Special-matrix" class="headerlink" title="Special matrix"></a>Special matrix</h4><p><strong>Identity Matrix</strong><br>The identity matrix, which is denoted as <em>I</em>(sometimes with n×n subscript), simply has 1’s on the diagonal (upper left to lower right diagonal) and 0’s elsewhere. For example:</p>
<script type="math/tex; mode=display">\begin{bmatrix}1&0&0\\0&1&0\\0&0&1\end{bmatrix}</script><p>For any matrix A, A×I=I×A=A</p>
<p><strong>Matrix Inverse</strong><br>If A is an m×m matrix, and if it has an inverse(not all matrix have an inverse), </p>
<script type="math/tex; mode=display">A\times A^{-1}=A^{-1}\times A=I</script><p>Matrices that don’t have an inverse are <em>singular</em> or <em>degenerate</em>.</p>
<p><strong>Matrix Transpose</strong><br>Let A be an m×n matrix, and let <script type="math/tex">B=A^T</script>. Then B is an n×m matrix, and</p>
<script type="math/tex; mode=display">B_{ij}=A_{ji}</script><h2 id="Week-Two"><a href="#Week-Two" class="headerlink" title="Week Two"></a>Week Two</h2><h3 id="Multivariate-Linear-Regression"><a href="#Multivariate-Linear-Regression" class="headerlink" title="Multivariate Linear Regression"></a>Multivariate Linear Regression</h3><h4 id="from-one-variable-to-multiple-variables"><a href="#from-one-variable-to-multiple-variables" class="headerlink" title="from one variable to multiple variables"></a>from one variable to multiple variables</h4><p>In the first week’s course, we learned linear with one variable <em>x</em>, and the hypothesis could be <script type="math/tex">h_{\theta}(x)=\theta_0+\theta_1x</script>. As a matter of fact, there can be more than one variables. So here’re the new notations:<br><strong>m</strong>: the number of training examples<br><strong>n</strong>: the number of features</p>
<p>-<script type="math/tex">x^{(i)}</script> : input (features) of ith training example<br>-<script type="math/tex">x_j^{(i)}</script> : value of feature j in ith training example</p>
<p>The hypothesis should be transformed to:</p>
<script type="math/tex; mode=display">h_{\theta}(x)=\theta_0x_0+\theta_1x_1+...+\theta_nx_n=\sum_{i=0}^n\theta_nx_n</script><p>(for convenience of notation, we define x0=1).<br>Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:</p>
<script type="math/tex; mode=display">h_{\theta}(x)=\begin{bmatrix}\theta_0&\theta_1&...&\theta_n\end{bmatrix}\begin{bmatrix}x_0\\x_1\\...\\x_n\end{bmatrix}=\theta^Tx</script><p>We can see that linear regression with one variable is just the special case when n=1.</p>
<p>Similarly, the new gradient descent algorithm would be represented as:<br>repeat until convergence{</p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha\frac{1}{m}\sum_{i=1}^m(h_{\theta}(x^{(i)}-y^{(i)})x_j^{(i)}\ \ (simultaneously\ update\ \theta_j\ for\ j=0,...,n)</script><p>}</p>
<h4 id="Feature-Scaling"><a href="#Feature-Scaling" class="headerlink" title="Feature Scaling"></a>Feature Scaling</h4><p>The idea is to make sure features are on a similar scale so that the gradient descent can be sped up.<br>Generally, get every feature into approximately a <script type="math/tex">-1<=x_i<=1</script> range. (the range is not limited to [-1,1])<br>Often used formula:</p>
<script type="math/tex; mode=display">x_i:=\frac{x_i}{max-min}</script><h4 id="Mean-normalization"><a href="#Mean-normalization" class="headerlink" title="Mean normalization"></a>Mean normalization</h4><p>Replace <script type="math/tex">x_i</script> with <script type="math/tex">x_i-\mu_i</script> to make features have approximately zero mean.<br>Formula:</p>
<script type="math/tex; mode=display">x_i:=\frac{x_i-\mu_i}{s_i}</script><p>where <script type="math/tex">\mu_i</script> is the average of all the values for feature(i) and si is the range of values(max - min) or the standard deviation.</p>
<h4 id="Something-more-about-learning-rate"><a href="#Something-more-about-learning-rate" class="headerlink" title="Something more about learning rate"></a>Something more about learning rate</h4><p>The idea of learning rate is same as in the first week. And <strong>how to make sure gradient descent is working correctly(or say, debugging)</strong>?<br>Make a plot with number of iterations on the x-axis. Now plot the cost function, J(θ) over the number of iterations of gradient descent. If J(θ) ever increases, then you probably need to decrease α.</p>
<p><strong>How to declare convergence</strong>?<br>If J(θ) decreased by less than <script type="math/tex">\epsilon(small\ value\ like 10^{-3})</script> in one iteration.</p>
<h4 id="Polynomial-regression"><a href="#Polynomial-regression" class="headerlink" title="Polynomial regression"></a>Polynomial regression</h4><p>linear:</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x</script><p>quadratic:</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2</script><p>cubic:</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3</script><p>square root:</p>
<script type="math/tex; mode=display">h_\theta(x)=\theta_0+\theta_1x+\theta_2\sqrt{x}</script><p>One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important.</p>
<h3 id="Normal-equation"><a href="#Normal-equation" class="headerlink" title="Normal equation"></a>Normal equation</h3><p>Gradient descent gives one way of minimizing J, and normal equation is another way. In the “Normal Equation” method, we will minimize J by explicitly taking its derivatives with respect to the θj ’s, and setting them to zero. This allows us to find the optimum theta without iteration.<br>Formula:</p>
<script type="math/tex; mode=display">\theta=(X^TX)^{-1}X^Ty</script><p>Here, X is m×(n+1) matrix(remember that x0 = 1), y is m-dimensional vector, and θ is (n+1) dimensional vector.</p>
<p>The advantage of normal equation:</p>
<ul>
<li>no need to choose α</li>
<li>don’t need to iterate</li>
</ul>
<p>Comparison of gradient descent and normal equation </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Gradient Descent</th>
<th>Normal Equation</th>
</tr>
</thead>
<tbody>
<tr>
<td>need to choose α</td>
<td>no need to choose α</td>
</tr>
<tr>
<td>need many iterations</td>
<td>no need to iterate</td>
</tr>
<tr>
<td>O(kn²)</td>
<td>O(n³)(need to calculate inverse)</td>
</tr>
<tr>
<td>works well when n is large</td>
<td>slow if n is very large</td>
</tr>
</tbody>
</table>
</div>
<h4 id="normal-equation-noninvertibility"><a href="#normal-equation-noninvertibility" class="headerlink" title="normal equation noninvertibility"></a>normal equation noninvertibility</h4><p>Sometimes <script type="math/tex">X^TX</script> can be invertible, the common causes might be having:</p>
<ul>
<li>Redundant features(linear dependent)</li>
<li>Too many features(e.g. m≤n)</li>
</ul>
<h3 id="Octave-MATLAB-Tutorial"><a href="#Octave-MATLAB-Tutorial" class="headerlink" title="Octave/MATLAB Tutorial"></a>Octave/MATLAB Tutorial</h3><p>Generally the commands are both used in MATLAB and Octave. (It is suggested that change Octave’s command prompt using <code>PS1(&#39;&gt;&gt; &#39;)</code>.)</p>
<p><strong>Elementary math operations</strong><br>e.g., <code>1+2</code>, <code>3-4</code>, <code>5*6</code>, <code>7/8</code>, <code>2^6</code><br>//Note: if the result is floating point, the default digits after decimal point is different between Octave (6 digits) and MATLAB (4 digits).</p>
<p><strong>Logical operations</strong><br>Equality.<br><code>1==2</code>, <code>1~=2  %~=: not equal</code></p>
<p>AND, OR, XOR.<br><code>1 &amp;&amp; 0</code>, <code>1 || 0</code>, <code>xor(1,0)</code></p>
<p>The result of logical operations is 0(false) or 1(true).</p>
<p><strong>Variable</strong><br>Simple form.<br><code>a = 3</code>, <code>a = 3;</code><br>The semicolon suppresses the print output.<br>The assignment can be constants, strings, boolean expressions, etc.</p>
<p>Display variable.<br><code>a</code>, <code>disp(a)</code>, <code>disp(sprintf(&#39;2 decimals: %0.2f&#39;, a)</code><br>Suppose a = 3, then the output of the command <code>a</code> is <code>a = 3</code>, of the command <code>disp(a)</code> is <code>3</code> and of the command <code>disp(sprintf(&#39;2 decimals: %0.2f&#39;, a)</code> is <code>2 decimals: 3.00</code>.</p>
<p>Format.<br><code>sprintf</code> is a C-like syntax that defines the output format.<br><code>format long</code>, <code>format short</code> makes the all of the following commands output in long or short format.</p>
<p><strong>Vectors and Matrices</strong><br>Matrix.<br><code>A = [1, 2; 3, 4; 5, 6]</code><br>We can memorize that the semicolon <code>;</code> means the next row of the matrix and the comma <code>,</code> (which can be replaced by space <code> </code>) means the next column.</p>
<p>Vector.<br><code>v = [1 2 3]</code>, <code>v = [1; 2; 3]</code><br>The former creates a row vector (1×3 matrix), and the latter creates a column vector (3×1 matrix).</p>
<p><strong>Some useful notation</strong><br><code>v = START(:INCREMENT):END</code><br>Create a row vector from START to END with each step incremented by INCREMENT. If <code>:INCREMENT</code> is omitted, the increment is 1.</p>
<p><code>ones(ROW, COLUMN)</code>, <code>zeros(ROW, COLUMN)</code><br>Create a ROW×COLUMN matrix of all ones/zeros.</p>
<p><code>rand(ROW, COLUMN)</code><br><code>rand</code> generates random numbers from the standard uniform distribution (0,1)<br><code>randn</code> generates random numbers from standard normal distribution.</p>
<p><code>eye(ROW)</code><br>(Eye is maybe a pun on the word identity.) Create a ROW by ROW identity matrix.</p>
<h2 id="Week-Three"><a href="#Week-Three" class="headerlink" title="Week Three"></a>Week Three</h2><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p><strong>Binary Classification</strong><br>The output y can take only two values, 0 and 1. Thus, y∈{0,1}, where the value 0 represents negative class and the value 1 represents positive class. It’s not advisable to use linear regression to represent the hypothesis. It’s logistic regression that has a range of (0,1).</p>
<p><strong>Multiclass Classification:One-vs-all</strong><br>Train a logistic regression classifier <script type="math/tex">h_\theta^{(i)}(x)</script> for each class <em>i</em> to predict the probability that <em>y = i</em>.<br>On a new input <em>x</em>, to make a prediction, pick the class <em>i</em> that maximizes <script type="math/tex">h_\theta(x)</script>.</p>
<h4 id="Logistic-Regression-Model"><a href="#Logistic-Regression-Model" class="headerlink" title="Logistic Regression Model"></a>Logistic Regression Model</h4><script type="math/tex; mode=display">h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}</script><p>The function is called <strong>sigmoid function</strong> or <strong>logistic function</strong>, and the function image is as shown below:</p>
<p><img src="/images/ML/sigmoidFunction.jpg" alt=""></p>
<p><strong>What is the interpretation of hypothesis output?</strong></p>
<p>-<script type="math/tex">h_\theta(x)</script>  estimated probability that y = 1, given x, parameterized by θ. In mathematical formula it can be denoted as </p>
<script type="math/tex; mode=display">h_\theta(x)=P(y=1|x;\theta)</script><p>Using probability theory knowledge, we can also know that</p>
<script type="math/tex; mode=display">P(y=0|x;\theta)+P(y=1|x;\theta)=1</script><p><strong>Decision Boundary</strong><br>Suppose predict “y=1” if <script type="math/tex">h_\theta(x)>=0.5</script>, and predict “y=0” if <script type="math/tex">h_\theta(x)<0.5</script>.<br>That is equal to</p>
<script type="math/tex; mode=display">\theta^TX\ge0\ \ =>\ \ y=1</script><script type="math/tex; mode=display">\theta^TX<0\ \ =>\ \ y=0</script><h4 id="Cost-function-and-Gradient-Descent"><a href="#Cost-function-and-Gradient-Descent" class="headerlink" title="Cost function and Gradient Descent"></a>Cost function and Gradient Descent</h4><p>Cost function in logistic regression is different from that in linear regression.</p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\sum_{i=1}^mCost(h_\theta(x^{(i)},y^{(i)})</script><script type="math/tex; mode=display">Cost(h_\theta(x^{(i)},y^{(i)})=-y\cdot log(h_\theta(x))-(1-y)\cdot log(1-h_\theta(x))</script><p>that means:</p>
<p>-<script type="math/tex">Cost(h_\theta(x^{(i)},y^{(i)})=-log(h_\theta(x))</script>    if y = 1;<br>-<script type="math/tex">Cost(h_\theta(x^{(i)},y^{(i)})=-log(1-h_\theta(x))</script>   if y = 0.</p>
<p>Vectorized implementation of cost function:</p>
<script type="math/tex; mode=display">h=g(X\theta)</script><script type="math/tex; mode=display">J(\theta)=\frac{1}{m}\cdot (-y^Tlog(h)-(1-y)^Tlog(1-h))</script><p><strong>Gradient Descent</strong>:</p>
<p>repeat {</p>
<script type="math/tex; mode=display">\theta_j:=\theta_j-\frac{\alpha}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><p>}</p>
<p>Vectorized implementation of gradient descent:</p>
<script type="math/tex; mode=display">\theta:=\theta-\frac{\alpha}{m}X^T(g(X\theta)-y)</script><p><em>Advanced optimization</em><br>Except gradient descent, there’re <strong>Conjugate gradient</strong>, <strong>BFGS</strong> and <strong>L-BFGS</strong> optimization algorithms. They have advantages that 1.No need to manually pick α;2.Often faster than gradient descent. But they’re more complex than gradient descent.</p>
<h3 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h3><h4 id="Overfitting"><a href="#Overfitting" class="headerlink" title="Overfitting"></a>Overfitting</h4><p>If we have too many features, the learn hypothesis may fit the training set very well, but fail to generalize to new examples. (Also known high variance)<br>There’s also another problem called <strong>Underfit</strong> problem, which has high bias.<br>Two examples:<br><img src="/images/ML/linearOverfit.jpg" alt=""></p>
<p><img src="/images/ML/logisticOverfit.jpg" alt=""></p>
<p><strong>How to address overfitting</strong></p>
<ol>
<li><p>Reduce number of features.<br>- Manually select which features to keep<br>- Model selection algorithm</p>
</li>
<li><p>Regularization.<br>- Keep all the features, but reduce magnitude/values of parameters <script type="math/tex">\theta_j</script></p>
</li>
</ol>
<p><strong>Cost function:</strong></p>
<script type="math/tex; mode=display">J(\theta)=\frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2]</script><p>The additional part is called regularization parameter. Note that λ should be set a proper value. If λ is set to an extremely large value, the algorithm may fail to eliminate overfitting or results in underfitting.</p>
<h4 id="Regularized-Linear-Regression"><a href="#Regularized-Linear-Regression" class="headerlink" title="Regularized Linear Regression"></a>Regularized Linear Regression</h4><p><strong>Gradient Descent:</strong></p>
<p>Repeat {</p>
<script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha[(\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)})+\frac{\lambda}{m}\theta_j]\ \ \ \ (j=1,2,3,...,n)</script><p>}</p>
<p>The second line can also denoted as:</p>
<script type="math/tex; mode=display">\theta_j:=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}</script><p>The coefficient <script type="math/tex">1-\alpha\frac{\lambda}{m}</script> will always be less than 1.</p>
<p><strong>Normal Equation:</strong></p>
<script type="math/tex; mode=display">\theta=(X^TX+\lambda\cdot L)^{-1}X^Ty</script><script type="math/tex; mode=display">L=\begin{bmatrix}0&\ &\ &\ &\ \\\ &1&\ &\ &\ \\\ &\ &1&\ &\ \\\ &\ &\ &...&\ \\\ &\ &\ &\ &1\end{bmatrix}</script><p>If λ&gt;0, this normal equation makes it invertible.</p>
<h4 id="Regularized-Logical-Regression"><a href="#Regularized-Logical-Regression" class="headerlink" title="Regularized Logical Regression"></a>Regularized Logical Regression</h4><p><strong>Gradient Descent:</strong><br>Repeat {</p>
<script type="math/tex; mode=display">\theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_0^{(i)}</script><script type="math/tex; mode=display">\theta_j:=\theta_j-\alpha[(\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)})+\frac{\lambda}{m}\theta_j]\ \ \ \ (j=1,2,3,...,n)</script><p>}</p>
<h2 id="Week-Four"><a href="#Week-Four" class="headerlink" title="Week Four"></a>Week Four</h2><h3 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h3><p>At a very  At a very simple level, neurons are basically computational units that take inputs (<strong>dendrites</strong>) as electrical inputs (called “spikes”) that are channeled to outputs (<strong>axons</strong>). In our model, our dendrites are like the input features x1⋯xn, and the output is the result of our hypothesis function. In this model our x0 input node is sometimes called the “bias unit.” It is always equal to 1. In neural networks, we use the same logistic function as in classification, <script type="math/tex">\frac{1}{1+e^{-\theta^TX}}</script>, yet we sometimes call it a sigmoid (logistic) activation function. In this situation, our “theta” parameters are sometimes called “weights”.<br>There’re several layers in the neural network. The first layer is called <strong>Input Layer</strong>, the last layer is called <strong>Output Layer</strong>, and the others is called <strong>Hidden Layer</strong>. </p>
<p><strong>Notations:</strong><br>-<script type="math/tex">a_i^{(j)}</script> = “activation” of unit i in layer j<br>-<script type="math/tex">\Theta^{(j)}</script> = matrix of weights controlling function mapping from layer j to layer j+1</p>
<p>In a simple example like this:<br><img src="/images/ML/neuralEx.jpg" alt=""></p>
<p>we have some equations:</p>
<script type="math/tex; mode=display">a_1^{(2)}=g(\Theta_{10}^{(1)}x_0+\Theta_{11}^{(1)}x_1+\Theta_{12}^{(1)}x_2+\Theta_{13}^{(1)}x_3)</script><script type="math/tex; mode=display">a_2^{(2)}=g(\Theta_{20}^{(1)}x_0+\Theta_{21}^{(1)}x_1+\Theta_{22}^{(1)}x_2+\Theta_{23}^{(1)}x_3)</script><script type="math/tex; mode=display">a_3^{(2)}=g(\Theta_{30}^{(1)}x_0+\Theta_{31}^{(1)}x_1+\Theta_{32}^{(1)}x_2+\Theta_{33}^{(1)}x_3)</script><script type="math/tex; mode=display">h_\Theta(x)=a_1^{(3)}=g(\Theta_{10}^{(2)}a_0^{(2)}+\Theta_{11}^{(2)}a_1^{(2)}+\Theta_{12}^{(2)}a_2^{(2)}+\Theta_{13}^{(3)}a_3^{(2)})</script><p>If network has <script type="math/tex">s_j</script> units in layer <em>j</em>, <script type="math/tex">s_{j+1}</script> units in layer <em>j+1</em>, then <script type="math/tex">\Theta^{(j)}</script> will be of dimension <script type="math/tex">s_{j+1}\times (s_j+1)</script>.</p>
<p>Vectorized Implementation:<br>-<script type="math/tex">z^{(2)}=\Theta^{(1)}a^{(1)}</script>  (regard the input layer as <script type="math/tex">a^{(1)}</script>)</p>
<script type="math/tex; mode=display">a^{(2)}=g(z^{(2)})</script><p>Add <script type="math/tex">a_0^{(2)}=1</script>    (add the bias unit)</p>
<script type="math/tex; mode=display">z^{(3)}=\Theta^{(2)}a^{(2)}</script><script type="math/tex; mode=display">h_\Theta(x)=a^{(3)}=g(z^{(3)})</script>
      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Hael Chan
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://haelchan.me/2017/11/01/machine-learning-note/" title="Machine Learning Note">http://haelchan.me/2017/11/01/machine-learning-note/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
          
            <a href="/tags/learning-note/" rel="tag"># learning note</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/10/31/REM-reading-report/" rel="next" title="REM Reading Report">
                <i class="fa fa-chevron-left"></i> REM Reading Report
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/03/Item-Based-CFRA-reading-report/" rel="prev" title="Item-Based Collaborative Filtering Recommendation Algorithms reading report">
                Item-Based Collaborative Filtering Recommendation Algorithms reading report <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>
  


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Hael Chan" />
            
              <p class="site-author-name" itemprop="name">Hael Chan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">11</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/haelchan" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:f.procumbens@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://twitter.com/Procumbens" target="_blank" title="Twitter">
                    
                      <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/hael-c/activities" target="_blank" title="知乎">
                    
                      <i class="fa fa-fw fa-zhihu"></i>知乎</a>
                </span>
              
            
          </div>

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-One"><span class="nav-number">1.</span> <span class="nav-text">Week One</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Machine-learning-algorithms"><span class="nav-number">1.1.1.</span> <span class="nav-text">Machine learning algorithms</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Regression-with-One-Variable"><span class="nav-number">1.2.</span> <span class="nav-text">Linear Regression with One Variable</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Hypothesis-Function-and-Cost-Function"><span class="nav-number">1.2.1.</span> <span class="nav-text">Hypothesis Function and Cost Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gradient-Descent"><span class="nav-number">1.2.2.</span> <span class="nav-text">Gradient Descent</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Algebra-Review"><span class="nav-number">1.3.</span> <span class="nav-text">Linear Algebra Review</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Matrix-Manipulation"><span class="nav-number">1.3.1.</span> <span class="nav-text">Matrix Manipulation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Matrix-multiplication-properties"><span class="nav-number">1.3.2.</span> <span class="nav-text">Matrix multiplication properties</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Special-matrix"><span class="nav-number">1.3.3.</span> <span class="nav-text">Special matrix</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-Two"><span class="nav-number">2.</span> <span class="nav-text">Week Two</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Multivariate-Linear-Regression"><span class="nav-number">2.1.</span> <span class="nav-text">Multivariate Linear Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#from-one-variable-to-multiple-variables"><span class="nav-number">2.1.1.</span> <span class="nav-text">from one variable to multiple variables</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Feature-Scaling"><span class="nav-number">2.1.2.</span> <span class="nav-text">Feature Scaling</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Mean-normalization"><span class="nav-number">2.1.3.</span> <span class="nav-text">Mean normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Something-more-about-learning-rate"><span class="nav-number">2.1.4.</span> <span class="nav-text">Something more about learning rate</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Polynomial-regression"><span class="nav-number">2.1.5.</span> <span class="nav-text">Polynomial regression</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Normal-equation"><span class="nav-number">2.2.</span> <span class="nav-text">Normal equation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#normal-equation-noninvertibility"><span class="nav-number">2.2.1.</span> <span class="nav-text">normal equation noninvertibility</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Octave-MATLAB-Tutorial"><span class="nav-number">2.3.</span> <span class="nav-text">Octave/MATLAB Tutorial</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-Three"><span class="nav-number">3.</span> <span class="nav-text">Week Three</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Logistic-Regression"><span class="nav-number">3.1.</span> <span class="nav-text">Logistic Regression</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Logistic-Regression-Model"><span class="nav-number">3.1.1.</span> <span class="nav-text">Logistic Regression Model</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Cost-function-and-Gradient-Descent"><span class="nav-number">3.1.2.</span> <span class="nav-text">Cost function and Gradient Descent</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regularization"><span class="nav-number">3.2.</span> <span class="nav-text">Regularization</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Overfitting"><span class="nav-number">3.2.1.</span> <span class="nav-text">Overfitting</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Regularized-Linear-Regression"><span class="nav-number">3.2.2.</span> <span class="nav-text">Regularized Linear Regression</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Regularized-Logical-Regression"><span class="nav-number">3.2.3.</span> <span class="nav-text">Regularized Logical Regression</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Week-Four"><span class="nav-number">4.</span> <span class="nav-text">Week Four</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-Networks"><span class="nav-number">4.1.</span> <span class="nav-text">Neural Networks</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hael Chan</span>

  
</div>









<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<span id="busuanzi_container_site_uv">
  欢迎~您是本站的第<span id="busuanzi_value_site_uv"></span>位访客
</span>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://hael.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://haelchan.me/2017/11/01/machine-learning-note/';
          this.page.identifier = '2017/11/01/machine-learning-note/';
          this.page.title = 'Machine Learning Note';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://hael.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'manual') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("3HhWLTewaKTSPj5DC3qp5b2m-gzGzoHsz", "zjxN2hpHQEmyMaz0XBicI3bn");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
