<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="neural machine translation,seq2seq," />




  


  <link rel="alternate" href="/atom.xml" title="Hael's Blog" type="application/atom+xml" />






<meta name="description" content="This post is mainly the reading note of the tutorial Neural Machine Translation and Sequence-to-sequence Models: A Tutorial and the solutions to its questions(maybe code, if possible).   ConceptsMachi">
<meta name="keywords" content="neural machine translation,seq2seq">
<meta property="og:type" content="article">
<meta property="og:title" content="Step into Neural Machine Translation">
<meta property="og:url" content="http://haelchan.me/2018/11/01/step-into-nmt/index.html">
<meta property="og:site_name" content="Hael&#39;s Blog">
<meta property="og:description" content="This post is mainly the reading note of the tutorial Neural Machine Translation and Sequence-to-sequence Models: A Tutorial and the solutions to its questions(maybe code, if possible).   ConceptsMachi">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://haelchan.me/images/nmt/ComputationGraph.jpg">
<meta property="og:image" content="http://haelchan.me/images/nmt/NNLM.jpg">
<meta property="og:image" content="http://haelchan.me/images/nmt/RNN.jpg">
<meta property="og:image" content="http://haelchan.me/images/nmt/encoder-decoder.jpg">
<meta property="og:updated_time" content="2018-12-02T06:57:05.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Step into Neural Machine Translation">
<meta name="twitter:description" content="This post is mainly the reading note of the tutorial Neural Machine Translation and Sequence-to-sequence Models: A Tutorial and the solutions to its questions(maybe code, if possible).   ConceptsMachi">
<meta name="twitter:image" content="http://haelchan.me/images/nmt/ComputationGraph.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://haelchan.me/2018/11/01/step-into-nmt/"/>





  <title>Step into Neural Machine Translation | Hael's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hael's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://haelchan.me/2018/11/01/step-into-nmt/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hael Chan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hael's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Step into Neural Machine Translation</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-11-01T21:50:16+08:00">
                2018-11-01
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2018-12-02T14:57:05+08:00">
                2018-12-02
              </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/11/01/step-into-nmt/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/11/01/step-into-nmt/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/11/01/step-into-nmt/" class="leancloud_visitors" data-flag-title="Step into Neural Machine Translation">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This post is mainly the reading note of the tutorial <a href="https://arxiv.org/abs/1703.01619" target="_blank" rel="external">Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</a> and the solutions to its questions(maybe code, if possible).  </p>
<h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><p><strong>Machine translation</strong> is the technology used to translate between human language.  </p>
<p><strong>Source language</strong>: the language input to the machine translation system  </p>
<p><strong>Target language</strong>: the output language  </p>
<p>Machine learning can be described as the task of converting a sequence of words in the source, and converting into a sequence of words in the target.  </p>
<p><strong>Sequence-to-sequence models</strong> refers to the broader class of models that include all models that map one sequence to another. E.g., machine translation, tagging, dialog, speech recognition, etc.  </p>
<h3 id="Statistical-Machine-Learning"><a href="#Statistical-Machine-Learning" class="headerlink" title="Statistical Machine Learning"></a>Statistical Machine Learning</h3><p>We define our task of machine learning as translating a source sentence <script type="math/tex">F=f_1,...,f_J=f_1^{|F|}</script> into a target sentence <script type="math/tex">E=e_1,...,e_I=e_1^{|E|}</script>. Here, the subscript <script type="math/tex">_1</script> at the end of the equations may be a bit misleading.  It means that the identity of the first word in the sentence is <script type="math/tex">e_1</script>, the identity of the second word in the sentence is <script type="math/tex">e_2</script>, up until the last word in the sentence being <script type="math/tex">e_{|E|}</script>.  </p>
<p>Then, translation system can be defined as a function <script type="math/tex">\hat{E}=\text{mt}(F)</script>, which returns a translation hypothesis <script type="math/tex">\hat{E}</script> given a source sentence <script type="math/tex">F</script> as input.  </p>
<p><strong>Statistical machine translation</strong> systems are systems that perform translation by creating a probabilistic model for the probability of <script type="math/tex">E</script> given <script type="math/tex">F</script>, <script type="math/tex">P(E|F;\theta)</script>, and finding the target sentence that maximizes the probability</p>
<script type="math/tex; mode=display">\hat{E}=\underset{E}{\operatorname{argmax}}P(E|F;\theta)</script><p>, where <script type="math/tex">\theta</script> are the parameters of the model specifying the probability distribution.  </p>
<p>The parameters <script type="math/tex">\theta</script> are learned from data consisting of aligned sentences in the source and target languages, which are called parallel corpora.</p>
<h2 id="n-gram-Language-Models"><a href="#n-gram-Language-Models" class="headerlink" title="n-gram Language Models"></a><em>n</em>-gram Language Models</h2><p>Instead of calculating the original joint probability <script type="math/tex">P(E)=P(|E|=T,e_1^T)</script>, it’s more manageable to calculate by multiplying together conditional probabilities for each of its elements: </p>
<script type="math/tex; mode=display">P(E)=\prod_{t=1}^{T+1}P(e_t|e_1^{t-1})</script><p>where $e_{T+1}=\langle/s\rangle$. It’s an implicit <em>sentence end</em> symbol, which we will indicate when we have terminated the sentence. By examining the position of the $\langle/s\rangle$ symbol, we can determine whether <script type="math/tex">|E|=T</script>.  </p>
<p>Then how to calculate the next word given the previous words <script type="math/tex">P(e_t|e_1^{t-1})</script>? The first way is simple: prepare a set of training data from which we can count word strings, count up the number of times we have seen a particular string of words, and divide it by the number of times we have seen the context.</p>
<script type="math/tex; mode=display">P_{ML(e_t|e_t^{t-1})}=\frac{c_{prefix}(e_1^t)}{c_{prefix}(e_1^{t-1})}</script><p>Here <script type="math/tex">c_{prefix}(\cdot)</script> is the count of the number of times this particular word string appeared at the beginning of a sentence in the training data.   </p>
<p>However, this language model will assign a probability of zero to every sentenec that it hasn’t seen before in the training corpus, which is not very useful.  </p>
<p>To solve the problem, we set a fixed window of previous words upon which we will base our probability calculations instead of calculating probabilities from the beginning of the sentence. If we limit our context to <script type="math/tex">n-1</script> previous words, this would amount to:</p>
<script type="math/tex; mode=display">P(e_t|e_1^{t-1})\approx P_{ML}(e_t|e_{t-n+1}^{t-1})</script><p>Models that make this assumption are called <em>n-**</em>gram models**. Specifically, when models where <script type="math/tex">n=1</script> are called unigram models, <script type="math/tex">n=2</script> bigram models, <script type="math/tex">n=3</script> trigram models, etc.  </p>
<p>In the simplest form, the parameters <script type="math/tex">\theta</script> of <em>n</em>-gram models consist of probabilities of the next word given <script type="math/tex">n-1</script> previous words can be calculated using maximum likelihood estimation as follows:</p>
<script type="math/tex; mode=display">\theta_{e_{t-n+1}^t}=P_{ML}(e_t|e_{t-n+1}^{t-1})=\frac{c(e_{t-n+1}^t)}{e_{t-n+1}^{t-1}}</script><h3 id="Smoothing"><a href="#Smoothing" class="headerlink" title="Smoothing"></a>Smoothing</h3><p>However, what if we encounter a two-word string that has never appeared in the training corpus? <em>n</em>-gram models fix this problem by <strong>smoothing</strong> probabilities, combining the maximum likelihood estimates for various values of <em>n</em>.  In the simple case of smoothing unigram and bigram probabilities, we can think of a model that combines together the probabilities as follows:</p>
<script type="math/tex; mode=display">P(e_t|e_{t-1})=(1-\alpha)P_{ML}(e_t|e_{t-1})+\alpha P_{ML}(e_t)</script><p>where <script type="math/tex">\alpha</script> is a variable specifying how much probability mass we hold out for the unigram distribution. As long as $\alpha&gt;0$, all the words in our vocabulary will be assigned some probability. This method is called <strong>interpolation</strong>, and is one of the standard ways to make probabilistic models more robust to low-frequency phenomena.  </p>
<p>Some more sophisticated methods for smoothing: Context-dependent smoothing coefficients, Back-off, Modified distributions, Modified Kneser-Ney smoothing.</p>
<h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p><strong>Likelihood</strong>  </p>
<script type="math/tex; mode=display">P(\mathcal{E}_{test};\theta)=\prod_{E\in\mathcal{E}_{test}}P(E;\theta)</script><p>The most straight-forward way of defining accuracy is the likelihood of the model with respect to the development or test data. The likelihood of the parameters <script type="math/tex">\theta</script> with respect to this data is equal to the probability that the model assigns to the data. </p>
<p><strong>Log likelihood</strong>  </p>
<script type="math/tex; mode=display">\log P(\mathcal{E_{test};\theta})=\sum_{E\in\mathcal{E}_{test}}\log P(E;\theta)</script><p>The log likelihood is used for a couple reasons. The first is because the probability of any particular sentence according to the language model can be a very small number, and the product of these small numbers can become a very small number that will cause numerical precision problems on standard computing hardware. The second is because sometimes</p>
<ol>
<li>product of small probability leads to very small number</li>
<li>mathematically more convenient</li>
</ol>
<p><strong>Perplexity</strong>  </p>
<script type="math/tex; mode=display">\text{ppl}(\mathcal{E}_{test};\theta)=e^{-(\log P(\mathcal{E}_{test};\theta))/\text{length}(\mathcal{E_{test}})}</script><p>An intuitive explanation of the perplexity is “how confused is the model about its decision?” More accurately, it expresses the value “if we randomly picked words from the probability distribution calculated by the language model at each time step, on average how many words would it have to pick to get the correct one?”  </p>
<h3 id="Handling-Unknown-Words"><a href="#Handling-Unknown-Words" class="headerlink" title="Handling Unknown Words"></a>Handling Unknown Words</h3><ul>
<li>Assume closed vocabulary</li>
<li>Interpolate with an unknown words distribution</li>
<li>Add an <script type="math/tex">\langle \text{unk}\rangle</script> word</li>
</ul>
<hr>
<p>Further reading includes large-scale language modeling, language model adaptation, longer-distance language count-based models, syntex-based language models.   </p>
<p><a href="http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf" target="_blank" rel="external">Course note</a> by Michael Collins from Columbia Univeristy is another good material.</p>
<h2 id="Log-linear-Langauge-Models"><a href="#Log-linear-Langauge-Models" class="headerlink" title="Log-linear Langauge Models"></a>Log-linear Langauge Models</h2><h3 id="Model-Formulation"><a href="#Model-Formulation" class="headerlink" title="Model Formulation"></a>Model Formulation</h3><p>Log-linear language models revolve around the concept of <strong>features</strong>. We define a feature function $\phi(e_{t-n+1}^{t-1})$ that takes a context as input, and outputs a real-valued <strong>feature vector</strong> <script type="math/tex">\boldsymbol{x}\in\mathbb{R}^N</script> that describe the context using <em>N</em> different features.  </p>
<p>Feature vector can be a <strong>one-hot vector</strong>. The function <script type="math/tex">\text{onehot}(i)</script> returns a vector where only the <script type="math/tex">i\text{th}</script> element is one and the rest are zero (assume the length of the vector is the appropriate length given the context).  </p>
<p><strong>Calculating scores</strong>: We calculate a score vector <script type="math/tex">\boldsymbol{s}\in\mathbb{R}^{|V|}</script> that corresponds to the likelihood of each word: words with higher scores in the vector will also have higher probabilities. The model parameters <script type="math/tex">\theta</script> specifically come in two varieties: a <strong>bias vector</strong> <script type="math/tex">\boldsymbol{b}\in\mathbb{R}^{|V|}</script>, which tells us how likely each word in the vocabulary is overall, and a <strong>weight matrix</strong> <script type="math/tex">\mathit{W}=\mathbb{R}^{|V|\times N}</script>, which describes the relationship between feature values and scores. </p>
<script type="math/tex; mode=display">\boldsymbol{s}=\mathit{W}\boldsymbol{x}+\boldsymbol{b}</script><p>To make computation more efficient (because many elements are zero in one-hot vectors or other sparse vectors), we can add together the columns of the weight matrix for all active (non-zero) features:</p>
<script type="math/tex; mode=display">\boldsymbol{s}=\sum_{\{j:x_j\ne0\}}\mathit{W}_{\cdot,j}x_j+\boldsymbol{b},</script><p>where <script type="math/tex">\mathit{W}_{\cdot,j}</script> is the <script type="math/tex">j\text{th}</script> column of <script type="math/tex">\mathit{W}</script>.   </p>
<p><strong>Calculating probabilities</strong>: </p>
<script type="math/tex; mode=display">p_j=\frac{\exp{(s_j)}}{\sum_\tilde{j}\exp(s_\tilde{j})}</script><p>By taking the exponent and dividing by the sum of the values over the entire vocabulary, these scores can be turned into probabilities that are between 0 and 1 and sum to 1. This function is called the <strong>softmax</strong> function, and often expressed in vector form as follows:</p>
<script type="math/tex; mode=display">\boldsymbol{p}=\text{softmax}(\boldsymbol{s})</script><h3 id="Learning-Model-Parameters"><a href="#Learning-Model-Parameters" class="headerlink" title="Learning Model Parameters"></a>Learning Model Parameters</h3><p>Sentence level loss function</p>
<script type="math/tex; mode=display">\ell(\mathcal{E_{train},\theta})=\log P(\mathcal{E}_{train}|\theta)=-\sum_{E\in\mathcal{E}_{train}}\log P(E|\theta)</script><p>Per-word level</p>
<script type="math/tex; mode=display">\ell(e_{t-n+1}^t,\theta)=\log P(e_t|e_{t-n+1}^{t-1})</script><p>(In the original tutorial, the equation is as below. But I guess it should be $-\log P(e<em>t|e</em>(t-n+1)^(t-1))$?)</p>
<p><strong>Methods for SGD</strong></p>
<p>Adjusting the learning rate, early stopping, shuffling training order, SGD with momentum, AdaGrad, Adam. (Common methods in machine learning)</p>
<p><strong>Derivative</strong></p>
<script type="math/tex; mode=display">\frac{d\ell(e_{t-n+1}^t,\theta)}{d\theta}</script><script type="math/tex; mode=display">\theta\larr\theta-\eta\frac{d\ell(e_{t-n+1}^t,\theta)}{d\theta}</script><p>Stepping through the full loss function in one pass:</p>
<script type="math/tex; mode=display">\boldsymbol{x}=\phi(e_{t-n+1}^{t-1})</script><script type="math/tex; mode=display">\boldsymbol{s}=\sum_{\{j:x_j\ne0\}}\mathit{W}_{\cdot,j}x_j+\boldsymbol{b}</script><script type="math/tex; mode=display">\boldsymbol{p}=\text{softmax}(\boldsymbol{s})</script><script type="math/tex; mode=display">\ell=-\log\boldsymbol{p}_{e_t}</script><p>Using the chain rule:</p>
<script type="math/tex; mode=display">\frac{d\ell(e_{t-n+1}^t,W,\boldsymbol{b})}{d\boldsymbol{b}}=\frac{d\ell}{d\boldsymbol{p}}\frac{d\boldsymbol{p}}{d\boldsymbol{s}}\frac{d\boldsymbol{s}}{d\boldsymbol{b}}=\boldsymbol{p}-\text{onehot}(e_t)</script><script type="math/tex; mode=display">\frac{d\ell(e_{t-n+1}^t,W,\boldsymbol{b})}{dW_{\cdot,j}}=\frac{d\ell}{d\boldsymbol{p}}\frac{d\boldsymbol{p}}{d\boldsymbol{s}}\frac{d\boldsymbol{s}}{dW_{\cdot,j}}=x_j(\boldsymbol{p}-\text{onehot}(e_t))</script><hr>
<p>One reason why log-linear models are nice is because they allow us to flexibly design features that we think might be useful for predicting the next word. Features include <em>Context word features, Context class, Context suffix features, Bag-of-words features,</em> etc.  </p>
<p>Further reading includes <em>Whole-sentence language models, Discriminative language models</em>.</p>
<h2 id="Neural-Networks-and-Feed-forward-Language-Models"><a href="#Neural-Networks-and-Feed-forward-Language-Models" class="headerlink" title="Neural Networks and Feed-forward Language Models"></a>Neural Networks and Feed-forward Language Models</h2><p>Multi-layer perceptrons(MLPs) consist one or more hidden layers that consist of an affine transform (a fancy name for a multiplication followed by an addition) followed by a non-linear function (step function, tanh, relu, etc), culminating in an output layer that calculates some variety of output.  </p>
<p>Neural networks can be thought of as a chain of functions that takes some input and calculates some desired output. The power of neural networks lies in the fact that chaining together a variety of simpler functions makes it possible to represent more complicated functions in an easily trainable, parameter-efficient way. </p>
<h3 id="Training-Neural-Networks"><a href="#Training-Neural-Networks" class="headerlink" title="Training Neural Networks"></a>Training Neural Networks</h3><p>Calculating loss function:</p>
<script type="math/tex; mode=display">\boldsymbol{h}'=W_{xh}\boldsymbol{x}+\boldsymbol{b}_h</script><script type="math/tex; mode=display">\boldsymbol{h}=\tanh(\boldsymbol{h}')</script><script type="math/tex; mode=display">y=\boldsymbol{w}_{hy}\boldsymbol{h}+b_y</script><script type="math/tex; mode=display">\ell=(y^*-y)^2</script><p>The derivatives:</p>
<script type="math/tex; mode=display">\frac{d\ell}{db_y}=\frac{d\ell}{dy}\frac{d_y}{db_y}</script><script type="math/tex; mode=display">\frac{d\ell}{d\boldsymbol{w}_{hy}}=\frac{d\ell}{dy}\frac{dy}{d\boldsymbol{w}_{hy}}</script><script type="math/tex; mode=display">\frac{d\ell}{d\boldsymbol{b}_h}=\frac{d\ell}{dy}\frac{dy}{d\boldsymbol{h}}\frac{d\boldsymbol{h}}{d\boldsymbol{h}'}\frac{d\boldsymbol{h}'}{d\boldsymbol{b}_h}</script><script type="math/tex; mode=display">\frac{d\ell}{dW_{xh}}=\frac{d\ell}{dy}\frac{dy}{d\boldsymbol{h}}\frac{d\boldsymbol{h}}{d\boldsymbol{h}'}\frac{d\boldsymbol{h}'}{dW_{xh}}</script><p>We could go through all of the derivations above by hand and precisely calculate the gradients of all parameters in the model. But even for a simple model like the one above, it is quite a lot of work and error prone. Fortunately, when we actually implement neural networks on a computer, there is a very useful tool that saves us a large portion of this pain: <strong>automatic differentiation</strong> (autodiff). To understand automatic differentiation, it is useful to think of our computation in a data structure called a <strong>computation graph</strong>. As shown in the following figure (figure 10 from the original paper), each node represents either an input to the network or the result of one computational operation, such as a multiplication, addition, tanh, or squared error. The first graph in the figure calculates the function of interest itself and would be used when we want to make predictions using our model, and the second graph calculates the loss function and would be used in training.</p>
<p><img src="/images/nmt/ComputationGraph.jpg" alt=""></p>
<p>Automatic differentiation is a two-step dynamic programming algorithm that operates over the second graph and performs:</p>
<ul>
<li><strong>Forward calculation</strong>, which traverses the nodes in the graph in topological order, calculating the actual result of the computation</li>
<li><strong>Back propagation</strong>, which traverses the nodes in reverse topological order, calculating the gradients</li>
</ul>
<h3 id="Neural-network-Language-Models"><a href="#Neural-network-Language-Models" class="headerlink" title="Neural-network Language Models"></a>Neural-network Language Models</h3><p><img src="/images/nmt/NNLM.jpg" alt=""></p>
<p>A tri-gram neural network model with a single layer is structured as shown in the figure above.</p>
<script type="math/tex; mode=display">\boldsymbol{m}=\text{concat}(M_{\cdot,e_{t-2}},M_{\cdot,e_{t-1}})</script><script type="math/tex; mode=display">\boldsymbol{h}=\tanh(W_{mh}\boldsymbol{m}+\boldsymbol{b}_h)</script><script type="math/tex; mode=display">\boldsymbol{s}=W_{hs}\boldsymbol{h}+\boldsymbol{b}_s</script><script type="math/tex; mode=display">\boldsymbol{p}=\text{softmax}(\boldsymbol{s})</script><p>In the first line, we obtain a vector <script type="math/tex">\boldsymbol{m}</script> representing the context $e_{i-n+1}^{i-1}$. Here, <em>M</em> is a matrix with <script type="math/tex">|V|</script> columns and <script type="math/tex">L_m</script> rows, where each column corresponds to an <script type="math/tex">L_m</script>-length vector representing a single word in the vocabulary. This vector is called a <strong>word embedding</strong> or a <strong>word representation</strong>, which is a vector of real numbers corresponding to particular words in the vocabulary.  </p>
<p>The vecror <script type="math/tex">\boldsymbol{m}</script> then results from the concatenation of the word vectors for all of the words in the context, so <script type="math/tex">|\boldsymbol{m}|=L_m*(n-1)</script>. Once we have this <script type="math/tex">\boldsymbol{m}</script>, we run the vectors through a hidden layer to obtain vector <script type="math/tex">\boldsymbol{h}</script>. By doing so, the model can learn combination features that reflect information regarding multiple words in the context.  </p>
<p>Next, we calculate the score vector for each word: <script type="math/tex">\boldsymbol{s}\in\mathbb{R}^{|V|}</script>. This is done by performing an affine transform of the hidden vector <script type="math/tex">\boldsymbol{h}</script> with a weight matrix <script type="math/tex">W_{hs}\in\mathbb{R}^{|V|\times|\boldsymbol{h}|}</script> and adding a bias vector <script type="math/tex">\boldsymbol{b}_s\in\mathbb{R}^{|V|}</script>. Finally, we get a probability estimate <script type="math/tex">\boldsymbol{p}</script> by running the calculated scores through a softmax function, like we did in the log-linear language models. For training, if we know <script type="math/tex">e_t</script> we can also calculate the loss function <script type="math/tex">\ell=-\log(p_{e_t})</script>.  </p>
<p>The advantage of neural network formulation: Better generalization of contexts, More generalizable combination of words into contexts and Ability to skip previous words.  </p>
<p>Further reading includes <em>Softmax approximations</em>, <em>Other softmax structures</em> and <em>Other models to learn word representations</em>.  </p>
<h2 id="Recurrent-Neural-Network-Language-Models"><a href="#Recurrent-Neural-Network-Language-Models" class="headerlink" title="Recurrent Neural Network Language Models"></a>Recurrent Neural Network Language Models</h2><p>Language models based on recurrent neural networks (RNNs) have the ability to capture long-distance dependencies in language.  </p>
<p>Some examples of long-distance dependencies in language: reflexive form (himself, herself) should match the gender, the conjugation of the verb based on the subject of the sentence, selectional preferences, topic and register.  </p>
<p>Recurrent neural networks are a variety of neural network that makes it possible to model these long-distance dependencies. The idea is simply to add a connection that references the previous hidden state <script type="math/tex">\boldsymbol{h}_{t-1}</script> when calculating hidden state <script type="math/tex">\boldsymbol{h}</script>.</p>
<script type="math/tex; mode=display">\boldsymbol{h}_t=\begin{cases}\tanh(W_{xh}\boldsymbol{x}_t+W_{hh}\boldsymbol{h}_{t-1}+\boldsymbol{b}_h)&t\ge1,\\0&\text{otherwise}.\end{cases}</script><p>For time steps <script type="math/tex">t\ge1</script>, the only difference from the hidden layer in a standard neural network is the addition of the connection <script type="math/tex">W_{hh}\boldsymbol{h}_{t-1}</script> form the hidden state at time step <script type="math/tex">t-1</script> connecting to that at time step <script type="math/tex">t</script>. </p>
<p><img src="/images/nmt/RNN.jpg" alt=""></p>
<p>RNNs make it possible to model long distance dependencies  because they have the ability to pass information between timesteps. For example, if some of the nodes in <script type="math/tex">\boldsymbol{h}_{t-1}</script> encode the information that “the subject of the sentence is male”, it is possible to pass on this information to <script type="math/tex">\boldsymbol{h}_t</script>, which can in turn pass it on to $\boldsymbol{h}_{t+1}$ and on to the end of the sentence. This ability to pass information across an arbitrary number of consecutive time steps is the strength of recurrent neural networks, and allows them to handle the long-distance dependencies.  </p>
<p>Feed-forward language model:</p>
<script type="math/tex; mode=display">\boldsymbol{m}_t=M_{\cdot,e_{t-1}}</script><script type="math/tex; mode=display">\boldsymbol{h}_t=\text{RNN}(\boldsymbol{m}_t,\boldsymbol{h}_{t-1})</script><script type="math/tex; mode=display">\boldsymbol{p}_t=\text{softmax}(W_{hs}\boldsymbol{h}_t+b_s)</script><h3 id="The-Vanishing-Gradient-and-Long-Short-term-Memory"><a href="#The-Vanishing-Gradient-and-Long-Short-term-Memory" class="headerlink" title="The Vanishing Gradient and Long Short-term Memory"></a>The Vanishing Gradient and Long Short-term Memory</h3><p>The <strong>vanishing gradient</strong> problem and the <strong>exploding gradient</strong> problem are the problems that simple RNNs are facing. The gradient in back propagation will gets smaller and smaller if <script type="math/tex">\frac{d\boldsymbol{h}_{t-1}}{d\boldsymbol{t}}\lt1</script> and then diminish the gradient <script type="math/tex">\frac{d\ell}{d\boldsymbol{h}_t}</script> (amplified if <script type="math/tex">\frac{d\boldsymbol{h}_{t-1}}{d\boldsymbol{t}}\gt1</script>).  </p>
<p>One method to solve this problem, in the case of diminishing gradients, is the use of a neural network architecture, the <strong>long short-term memory</strong> (LSTM), that is specifically designed to ensure that the derivate of the recurrent function is exactly one. The most fundamental idea behind the LSTM is that in addition to the standard hidden state <script type="math/tex">\boldsymbol{h}</script> used by most neural networks, it also has a <strong>memory cell <em>c</em> </strong>, for which the gradient <script type="math/tex">\frac{d\boldsymbol{c}_t}{d\boldsymbol{c}_{t-1}}</script> is exactly one. Because this gradient is exactly one, information stored in the memory cell does not suffer from vanishing gradients, and thus LSTMs can capture long-distance dependencies more effectively than standard recurrent neural networks.</p>
<script type="math/tex; mode=display">\boldsymbol{u}_t=\tanh(W_{xu}\boldsymbol{x}_t+W_{hu}h_{t-1}+\boldsymbol{b}_u)</script><script type="math/tex; mode=display">\boldsymbol{i}_t=\sigma(W_{xi}\boldsymbol{x}_t+W_{hi}h_{t-1}+\boldsymbol{b}_i)</script><script type="math/tex; mode=display">\boldsymbol{o}_t=\sigma(W_{xo}\boldsymbol{x}_t+W_{ho}h_{t-1}+\boldsymbol{b}_o)</script><script type="math/tex; mode=display">\boldsymbol{c}_t=\boldsymbol{i}_t\odot\boldsymbol{u}_t+\boldsymbol{c}_{t-1}</script><script type="math/tex; mode=display">\boldsymbol{h}_t=\boldsymbol{o}_t\odot\tanh(\boldsymbol{c}_t)</script><p>The first equation is the update, which is basically the same as the RNN update. It takes in the input and hidden state, performs an affine transform and runs it through the tanh non-linearity.  </p>
<p>The following two equations are the <strong>input gate</strong> and <strong>output gate</strong> of the LSTM respectively. The function of “gates”, as indicated by their name, is to either allow information to pass through or block it from passing. Both of these gates perform an affine transform followed by the sigmoid function. The output of the sigmoid is then used to perform a compoenntwise multiplication, <script type="math/tex">\boldsymbol{z}=\boldsymbol{x}\odot\boldsymbol{y}</script>, which means <script type="math/tex">z_i=x_i*y_i</script>, with the output of another function.  </p>
<p>The next is the most important equation in the LSTM. This equation sets <script type="math/tex">\boldsymbol{c}_t</script> to be equal to the update <script type="math/tex">\boldsymbol{u}_t</script> modulated by the input gate <script type="math/tex">\boldsymbol{i}_t</script> pllus the cell value for the previous time step <script type="math/tex">\boldsymbol{c}_{t-1}</script>. Since we are directly adding <script type="math/tex">\boldsymbol{c}_{t-1}</script> to <script type="math/tex">\boldsymbol{c}_t</script>, the gradient would be one.  </p>
<p>The final equation calculates the next hidden state of the LSTM. This is calculated by using a tanh function to scale the cell value between -1 and 1, then modulating the output using the output gate value <script type="math/tex">\boldsymbol{o}_t</script>. This will be the value actually used in any downstream calculation.</p>
<h3 id="Other-RNN-Variants"><a href="#Other-RNN-Variants" class="headerlink" title="Other RNN Variants"></a>Other RNN Variants</h3><h4 id="LSTM-with-a-forget-gate"><a href="#LSTM-with-a-forget-gate" class="headerlink" title="LSTM with a forget gate:"></a>LSTM with a forget gate:</h4><p>One modification to the standard LSTM that is used widely (in fact so widely that most people who refer to “LSTM” are now referring to this variant) is the addition of a <strong>forget gate</strong>. The equations:</p>
<script type="math/tex; mode=display">\boldsymbol{u}_t=\tanh(W_{xu}\boldsymbol{x}_t+W_{hu}h_{t-1}+\boldsymbol{b}_u)</script><script type="math/tex; mode=display">\boldsymbol{i}_t=\sigma(W_{xi}\boldsymbol{x}_t+W_{hi}h_{t-1}+\boldsymbol{b}_i)</script><script type="math/tex; mode=display">\boldsymbol{f}_t=\sigma(W_{xf}\boldsymbol{x}_t+W_{hf}h_{t-1}+\boldsymbol{b}_f)</script><script type="math/tex; mode=display">\boldsymbol{o}_t=\sigma(W_{xo}\boldsymbol{x}_t+W_{ho}h_{t-1}+\boldsymbol{b}_o)</script><script type="math/tex; mode=display">\boldsymbol{c}_t=\boldsymbol{i}_t\odot\boldsymbol{u}_t+\boldsymbol{f}_t\odot\boldsymbol{c}_{t-1}</script><script type="math/tex; mode=display">\boldsymbol{h}_t=\boldsymbol{o}_t\odot\tanh(\boldsymbol{c}_t)</script><p>The difference lies in the forget gate, which modulates the passing of the previous cell <script type="math/tex">\boldsymbol{c}_{t-1}</script> to the current cell <script type="math/tex">\boldsymbol{c}_t</script>. This forget gate is useful in that it allows the cell to easily clear its memory when justified. Forget gates have the advantage of allowing the sort of find-grained information flow control, but they also come with the risk that if <script type="math/tex">\boldsymbol{f}_t</script> is set to zero all the time, the model will forget everything and lose its ability to handle long-distance dependencies. Thus, at the beginning of neural network training, it is common to initialize the bias <script type="math/tex">\boldsymbol{b}_f</script> of the forget gate to be a somewhat large value (e.g. 1), which will make the neural net start training without using the forget gate, and only gradually start forgetting content after the net has been trained to some extent.</p>
<h4 id="Gated-Recurrent-Unit"><a href="#Gated-Recurrent-Unit" class="headerlink" title="Gated Recurrent Unit"></a>Gated Recurrent Unit</h4><p>Gated recurrent unit (GRU) is one simpler RNN variant than LSTM:</p>
<script type="math/tex; mode=display">\boldsymbol{r}_t=\sigma(W_{xr}\boldsymbol{x}_t+W_{hr}h_{t-1}+\boldsymbol{b}_r)</script><script type="math/tex; mode=display">\boldsymbol{z}_t=\sigma(W_{xz}\boldsymbol{x}_t+W_{hz}h_{t-1}+\boldsymbol{b}_z)</script><script type="math/tex; mode=display">\tilde{\boldsymbol{h}}_t=\tanh(W_{xh}\boldsymbol{x}_t+W_{hh}(\boldsymbol{r}_t\odot\boldsymbol{h}_{t-1})+\boldsymbol{b}_h)</script><script type="math/tex; mode=display">\boldsymbol{h}_t=(1-z_t)\boldsymbol{h}_{t-1}+\boldsymbol{z}_t\tilde{\boldsymbol{h}}_t</script><p>The most characteristic element of the GRU is the last equation, which interpolates between a candidate for the updated hidden state <script type="math/tex">\tilde{\boldsymbol{h}_t}</script> and the previous state <script type="math/tex">\boldsymbol{h}_{t-1}</script> (in the original tutorial, here is noted as <script type="math/tex">\tilde{\boldsymbol{h}}_{t-1}</script>, which might be a mistake). This interpolation is modulated by an <strong>update gate</strong> <script type="math/tex">\boldsymbol{z}_t</script>, where if the update gate is close to one, the GRU will use the new candidate hidden value, and if the update is close to zero, it will use the previous value. The candidate hidden state is similar to a standard RNN update but includes an additional modulation of the hidden state input by a <strong>reset gate</strong> <script type="math/tex">\boldsymbol{r}_t</script>. Compared to the LSTM, the GRU has slightly fewer parameters (it performs one less parameterized affine transform) and also does not have a separate concept of a “cell”. Thus, GRUs have been used by some to conserve memory or computation time.  </p>
<p>[The stacked RNNs , residual networks; online, batch and minibatch training will be temporarily left out (time limited). I will go over the the following chapters first and then return here.]</p>
<h2 id="Neural-Encoder-Decoder-Models"><a href="#Neural-Encoder-Decoder-Models" class="headerlink" title="Neural Encoder-Decoder Models"></a>Neural Encoder-Decoder Models</h2><p>The basic idea of the <strong>encoder-decoder</strong> model is relatively simple: we have an RNN language model, but before starting calculation of the probabilities of <em>E</em>, we first calculate the initial state of the language model using another RNN over the source sentence <em>F</em>. The name “encoder-decoder” comes from the idea that the first neural network running over <em>F</em> “encodes” its information as a vector of real-valued numbers (the hidden state), then the second neural network used to predict <em>E</em> “decodes” this information into the target sentence.</p>
<p><img src="/images/nmt/encoder-decoder.jpg" alt=""></p>
<script type="math/tex; mode=display">\boldsymbol{m}_t^{(f)}=M_{\cdot,f_t}^{(f)}</script><script type="math/tex; mode=display">\boldsymbol{h}_t^{(f)}=\begin{cases}\text{RNN}^{(f)}(\boldsymbol{m}_t^{(f)},\boldsymbol{h}_{t-1}^{(f)})&t\ge1,\\0&\text{otherwise.}\end{cases}</script><script type="math/tex; mode=display">\boldsymbol{m}_t^{(e)}=M_{\cdot,e_{t-1}}^{(e)}</script><script type="math/tex; mode=display">\boldsymbol{h}_t^{(e)}=\begin{cases}\text{RNN}^{(e)}(\boldsymbol{m}_t^{(e)},\boldsymbol{h}_{t-1}^{(e)})&t\ge1,\\\boldsymbol{h}_{|F|}^{(f)}&\text{otherwise}.\end{cases}</script><script type="math/tex; mode=display">\boldsymbol{p}_t^{(e)}=\text{softmax}(W_{hs}\boldsymbol{h}_t^{(e)}+b_s)</script><p>In the first two lines, we look up the embedding <script type="math/tex">\boldsymbol{m}_t^{(f)}</script> and calculate the encoder hidden state <script type="math/tex">\boldsymbol{h}_t^{(f)}</script> for the <em>t</em>th word in the source sequence <em>F</em>. We start with am empty vector <script type="math/tex">\boldsymbol{h}_0^{(f)}=\boldsymbol{0}</script>, and by <script type="math/tex">\boldsymbol{h}_{|F|}^{(f)}</script>, the encoder has seen all the words in the source sentence. Thus, this hidden state should theoretically be able to encode all of the information in the source sentence.  </p>
<p>In the decoder phase, we predict the probability of word <script type="math/tex">e_t</script> at each time step. First, we similarly look up <script type="math/tex">\boldsymbol{m}_t^{(e)}</script>, but this time use the previous word <script type="math/tex">e_{t-1}</script>, as we must condition the probability of <script type="math/tex">e_t</script> on the previous word, not on itself. Then, we run the decoder to calculate <script type="math/tex">\boldsymbol{h}_t^{(e)}</script>, whose initial state <script type="math/tex">\boldsymbol{h}_0^{(e)}=\boldsymbol{h}_{|F|}^{(f)}</script>. Finally, we calculate the probability <script type="math/tex">\boldsymbol{p}_t^{(e)}</script> by using a softmax on the hidden state <script type="math/tex">\boldsymbol{h}_t^{(e)}</script>.</p>
<h2 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h2><ul>
<li><p><em>If V is the size of the target vocabulary, how many are there for a sentence of length T?</em> (on page 4)</p>
<p>There are <script type="math/tex">V^T</script>.</p>
</li>
<li><p><em>How many parameters does an n-gram model with a particular n have?</em> (on page 6)</p>
<script type="math/tex; mode=display">V^n</script></li>
<li><p><em>What is this probability?</em> (on page 7)</p>
<script type="math/tex; mode=display">P=P(e_2=\text{am}|e_1=\text{i})\cdot P(e_3=\text{from}|e_2=\text{am})\cdot P(e_4=\text{utah}|e_3=\text{from}\cdot P(e_5=.|e_4=\text{utah}))\\=\frac{1}{2}\cdot1\cdot\frac{1}{2}\cdot1\\=\frac{1}{4}</script></li>
</ul>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    Hael Chan
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="http://haelchan.me/2018/11/01/step-into-nmt/" title="Step into Neural Machine Translation">http://haelchan.me/2018/11/01/step-into-nmt/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/neural-machine-translation/" rel="tag"># neural machine translation</a>
          
            <a href="/tags/seq2seq/" rel="tag"># seq2seq</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/10/17/bert/" rel="next" title="BERT - A new era of NLP">
                <i class="fa fa-chevron-left"></i> BERT - A new era of NLP
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>
  


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Hael Chan" />
            
              <p class="site-author-name" itemprop="name">Hael Chan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">21</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">14</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/haelchan" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:f.procumbens@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://twitter.com/Procumbens" target="_blank" title="Twitter">
                    
                      <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/hael-c/activities" target="_blank" title="知乎">
                    
                      <i class="fa fa-fw fa-compass"></i>知乎</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-globe"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://ruder.io" title="Sebastian Ruder" target="_blank">Sebastian Ruder</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://smerity.com" title="Semerity" target="_blank">Semerity</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://einstein.ai/research/blog" title="Salesforce Research" target="_blank">Salesforce Research</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.reddit.com/r/LanguageTechnology/" title="Reddit" target="_blank">Reddit</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://learnerwn.github.io" title="WN_Blog" target="_blank">WN_Blog</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://hey-yahei.cn" title="YaHei" target="_blank">YaHei</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://godweiyang.com" title="WeiYang Blog" target="_blank">WeiYang Blog</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.linzehui.me" title="Weekly Review" target="_blank">Weekly Review</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://jalammar.github.io" title="Jay Alammar" target="_blank">Jay Alammar</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Concepts"><span class="nav-number">1.</span> <span class="nav-text">Concepts</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Statistical-Machine-Learning"><span class="nav-number">1.1.</span> <span class="nav-text">Statistical Machine Learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#n-gram-Language-Models"><span class="nav-number">2.</span> <span class="nav-text">n-gram Language Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Smoothing"><span class="nav-number">2.1.</span> <span class="nav-text">Smoothing</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Evaluation"><span class="nav-number">2.2.</span> <span class="nav-text">Evaluation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Handling-Unknown-Words"><span class="nav-number">2.3.</span> <span class="nav-text">Handling Unknown Words</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Log-linear-Langauge-Models"><span class="nav-number">3.</span> <span class="nav-text">Log-linear Langauge Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Model-Formulation"><span class="nav-number">3.1.</span> <span class="nav-text">Model Formulation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Learning-Model-Parameters"><span class="nav-number">3.2.</span> <span class="nav-text">Learning Model Parameters</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Networks-and-Feed-forward-Language-Models"><span class="nav-number">4.</span> <span class="nav-text">Neural Networks and Feed-forward Language Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Training-Neural-Networks"><span class="nav-number">4.1.</span> <span class="nav-text">Training Neural Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-network-Language-Models"><span class="nav-number">4.2.</span> <span class="nav-text">Neural-network Language Models</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recurrent-Neural-Network-Language-Models"><span class="nav-number">5.</span> <span class="nav-text">Recurrent Neural Network Language Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Vanishing-Gradient-and-Long-Short-term-Memory"><span class="nav-number">5.1.</span> <span class="nav-text">The Vanishing Gradient and Long Short-term Memory</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Other-RNN-Variants"><span class="nav-number">5.2.</span> <span class="nav-text">Other RNN Variants</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#LSTM-with-a-forget-gate"><span class="nav-number">5.2.1.</span> <span class="nav-text">LSTM with a forget gate:</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Gated-Recurrent-Unit"><span class="nav-number">5.2.2.</span> <span class="nav-text">Gated Recurrent Unit</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Neural-Encoder-Decoder-Models"><span class="nav-number">6.</span> <span class="nav-text">Neural Encoder-Decoder Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Questions"><span class="nav-number">7.</span> <span class="nav-text">Questions</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hael Chan</span>

  
</div>









<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<span id="busuanzi_container_site_uv">
  Welcome~ Thanks for visiting my blog. You are the №.<span id="busuanzi_value_site_uv"></span> visitor.
</span>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://hael.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://haelchan.me/2018/11/01/step-into-nmt/';
          this.page.identifier = '2018/11/01/step-into-nmt/';
          this.page.title = 'Step into Neural Machine Translation';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://hael.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'manual') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("3HhWLTewaKTSPj5DC3qp5b2m-gzGzoHsz", "zjxN2hpHQEmyMaz0XBicI3bn");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
