<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="reading report,natural language language," />










<meta name="description" content="Composition in Distributional Models of SemanticsSemantic representationSemantic NetworksSemantic networks represent concepts as nodes in a graph. Edges in the graph denote semantic relationships betw">
<meta name="keywords" content="reading report,natural language language">
<meta property="og:type" content="article">
<meta property="og:title" content="Learning note of two papers about composition model">
<meta property="og:url" content="http://haelchan.me/2018/04/08/composition-model/index.html">
<meta property="og:site_name" content="Hael&#39;s Blog">
<meta property="og:description" content="Composition in Distributional Models of SemanticsSemantic representationSemantic NetworksSemantic networks represent concepts as nodes in a graph. Edges in the graph denote semantic relationships betw">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://haelchan.me/images/composition/tensorProduct.jpg">
<meta property="og:image" content="http://haelchan.me/images/composition/decomposition.jpg">
<meta property="og:updated_time" content="2018-06-02T09:47:47.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Learning note of two papers about composition model">
<meta name="twitter:description" content="Composition in Distributional Models of SemanticsSemantic representationSemantic NetworksSemantic networks represent concepts as nodes in a graph. Edges in the graph denote semantic relationships betw">
<meta name="twitter:image" content="http://haelchan.me/images/composition/tensorProduct.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://haelchan.me/2018/04/08/composition-model/"/>





  <title>Learning note of two papers about composition model | Hael's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hael's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://haelchan.me/2018/04/08/composition-model/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hael Chan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hael's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Learning note of two papers about composition model</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-04-08T23:46:59+08:00">
                2018-04-08
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-06-02T17:47:47+08:00">
                2018-06-02
              </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/04/08/composition-model/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/04/08/composition-model/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/04/08/composition-model/" class="leancloud_visitors" data-flag-title="Learning note of two papers about composition model">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Composition-in-Distributional-Models-of-Semantics"><a href="#Composition-in-Distributional-Models-of-Semantics" class="headerlink" title="Composition in Distributional Models of Semantics"></a>Composition in Distributional Models of Semantics</h1><h2 id="Semantic-representation"><a href="#Semantic-representation" class="headerlink" title="Semantic representation"></a>Semantic representation</h2><h3 id="Semantic-Networks"><a href="#Semantic-Networks" class="headerlink" title="Semantic Networks"></a>Semantic Networks</h3><p>Semantic networks represent concepts as nodes in a graph. Edges in the graph denote semantic relationships between concepts(e.g., <em>DOG</em> IS-A <em>MAMMAL</em>, <em>DOG</em> HAS <em>TAIL</em>) and word meaning is expressed by the number of type of connections to other words. In this framework, word similarity is a function of path length-semantically related words are expected to have shorter paths between them. Semantic networks constitute a somewhat idealized representation that abstracts away from real-word usage-they are traditionally hand coded by modelers who a priori decide which relationships are most relevant in representing meaning.<br>More recent work creates a semantic network from word association norms (Nelson, McEvoy, &amp; Schreiber, 1999); however, these can only represent a small fraction of the vocabulary of an adult speaker.</p>
<h3 id="Feature-based-Models"><a href="#Feature-based-Models" class="headerlink" title="Feature-based Models"></a>Feature-based Models</h3><p>Feature-based model has the idea that word meaning can be described in terms of feature lists. Theories tend to differ with respect to their definition of features. In many cases, the features are obtained by asking native speakers to generate attributes they consider important in describing the meaning of a word. This allows the representation of each word by a distribution of numerical values over the feature set.<br>Admittedly, norming studies have the potential of revealing which dimensions of meaning are psychologically salient. However, a number of difficulties arise when working with such data. For example, the number and types of attributes generated can vary substantially as a function of the amount of time devoted to each word. There are many degrees of freedom in the way that responses are coded and analyzed. And multiple subjects are required to create a representation for each word, which in practice limits elicitation studies to a small-size lexicon.</p>
<h3 id="Semantic-Spaces"><a href="#Semantic-Spaces" class="headerlink" title="Semantic Spaces"></a>Semantic Spaces</h3><p>It has been driven by the assumption that word meaning can be learned from the linguistic environment. Words that are similar in meaning tend to occur in contexts of similar words. Semantic space models capture meaning <em>quantitatively</em> in terms of simple co-occurrence statistics. Words are represented as vectors in a high-dimensional space, where each component corresponds to some co-occurring contextual element. The latter can be words themselves, larger linguistic units such as paragraphs or documents, or even more complex linguistic representations such as n-grams and the argument slots of predicates.<br>The advantage of taking such a geometric approach is that the similarity of word meanings can be easily quantifies by measuring their distance in the vector space, or the cosine of the angle between them.</p>
<h4 id="Some-semantic-space-models"><a href="#Some-semantic-space-models" class="headerlink" title="Some semantic space models"></a>Some semantic space models</h4><p><strong>Hyperspace Analog to Language model(HAL)</strong><br>Represents each word by a vector where each element of the vector corresponds to a weighted co-occurrence value of that word with some other word.</p>
<p><strong>Latent Semantic Analysis(LSA)</strong><br>Derives a high-dimensional semantic space for words while using co-occurrence information between words and the passages they occur in. LSA constructs a word-document co-occurrence matrix from a large document collection.</p>
<p><strong>Probabilistic topic models</strong><br>Offers an alternative to semantic spaces based on the assumption that words observed in a corpus manifest some latent structure linked to topics. Words are represented as a probability distribution over a set of topics(corresponding to coarse-grained senses). Each topic is a probability distribution over words, and the content of the topic is reflected in the words to which it assigns high probability. Topic models are <em>generative</em>, they specify a probabilistic procedure by which documents can be generated. Thus, to make a new document, one first chooses a distribution over topics. Then for each word in that document, one chooses a topic at random according to this distribution and selects a word from that topic. Under this framework, the problem of meaning representation is expressed as one of statistical inference: Given some data-words in a corpus-infer the latent structure from which it was generated.</p>
<h4 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h4><ul>
<li>semantic priming</li>
<li>discourse comprehension</li>
<li>word categorization</li>
<li>judgments of essay quality</li>
<li>synonymy tests</li>
<li>association</li>
</ul>
<h2 id="Composition"><a href="#Composition" class="headerlink" title="Composition"></a>Composition</h2><p>It is well known that linguistic structures are <em>compositional</em>(simpler elements are combined to form more complex ones).<br>Morphemes are combined into words, words into phrases, and phrases into sentences. It is also reasonable to assume that the meaning of sentences is composed of the meanings of individual words or phrases.<br>Compositionality allows languages to construct complex meanings from combinations of simpler elements. This property is often captured in the following principle: The meaning of a whole is a function of the meaning of the parts. Therefore, whatever approach we take to modeling semantics, representing the meanings of complex structures will involve modeling the way in which meanings combine.</p>
<h3 id="Preset"><a href="#Preset" class="headerlink" title="Preset"></a>Preset</h3><p>In this article, the authors attempt to bridge the gap in the literature by developing models of semantic composition that can represent the meaning of word combinations as opposed to individual words. Our models are narrower in scope compared with those developed in earlier connectionist work. Our vectors represent words; they are high-dimensional but relatively structured, and every component corresponds to a predefined context in which the words are found. The author take it as a defining property of the vectors they consider that the values of their components are derived from event frequencies such as the number of times a given word appears in a given context. Having this in mind, they present a general framework for vector-based composition that allows us to consider different classes of models. Specifically, they formulate composition as a function of two vectors and introduce models based on addition and multiplication. They also investigate how the choice of the underlying semantic representation interacts with the choice of composition function by comparing a spatial model that represents words as vectors in a high-dimensional space against a probabilistic model that represents words as topic distributions. They assess the performance of these models directly on a similarity task. They elicit similarity ratings for pairs of adjective–noun, noun–noun, and verb–object constructions and examine the strength of the relationship between similarity ratings and the predictions of their models.</p>
<h3 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h3><script type="math/tex; mode=display">\begin{equation}\mathbf{p}=f(\mathbf{u,v})\end{equation}</script><p>Express the composition of two constituents, <script type="math/tex">\mathbf{u}</script> and <script type="math/tex">\mathbf{v}</script>, in terms of a function acting on those constituents.</p>
<script type="math/tex; mode=display">\begin{equation}\mathbf{p}=f(\mathbf{u,v},R)\end{equation}</script><p>A further refinement of the above principle <strong>taking the role of syntax into account</strong>: The meaning of a whole is a function of the meaning of the parts and of the way they are syntactically combined. They thus modify the composition function in Eq. 1 to account for the fact that there is a syntactic relation <em>R</em> between constituents <script type="math/tex">\mathbf{u}</script> and <script type="math/tex">\mathbf{v}</script>.<br>Even this formulation may not be fully adequate. The meaning of the whole is greater than the meaning of the parts. The implication here is that language users are bringing more to the problem of constructing complex meanings than simply the meaning of the parts and their syntactic relations. This additional information includes both knowledge about the language itself and also knowledge about the real world. </p>
<script type="math/tex; mode=display">\begin{equation}\mathbf{p}=f(\mathbf{u,v},R,K)\end{equation}</script><p>A full understanding of the compositional process involves an account of how novel interpretations are integrated with existing knowledge. The composition function needs to be augmented to include an additional argument, <em>K</em>, representing any knowledge utilized by the compositional process.</p>
<p>Compositionality is a matter of degree rather than a binary notion. Linguistic structures range from fully compositional(e.g., <em>black hair</em>), to partly compositional syntactically fixed expressions,(e.g., <em>take advantage</em>), in which the constituents can still be assigned separate meanings, and noncompositional idioms(e.g., <em>kick the bucket</em>) or multiword expressions(e.g., <em>by and large</em>), whose meaning cannot be distributed across their constituents.</p>
<h4 id="Logic-based-view"><a href="#Logic-based-view" class="headerlink" title="Logic-based view"></a>Logic-based view</h4><p>Within symbolic logic, compositionality is accounted for elegantly by assuming a tight correspondence between syntactic expressions and semantic form. In this tradition, the meaning of a phrase or sentence is its truth conditions which are expressed in terms of truth relative to a model. In classical Montague grammar, for each syntactic category there is a uniform semantic type(e.g., sentences express propositions; nouns and adjectives express properties of entities; verbs express properties of events). Most lexical meanings are left unanalyzed and treated as <em>primitive</em>.<br>Noun is represented by logical symbol. Verb is represented by a function from entities to propositions, expressed in <a href="https://en.wikipedia.org/wiki/Lambda_calculus" target="_blank" rel="external">lambda calculus</a>.(Well I’m not familiar with lambda calculus yet, but the idea is similar to predicate logic in discrete mathematics.)<br>For example, the proper noun <em>John</em> is represented by the logical symbol <em>JOHN</em> denoting a specific entity, and the verb <em>wrote</em> is represented as <em>λx.WROTE(x)</em>. Applying this function to the entity <em>JOHN</em> yields the logical formula <em>WROTE(JOHN)</em> as a representation of the sentence <em>John wrote</em>. It is worth noting that the entity and predicate within this formula are represented symbolically, and that the connection between a symbol and its meaning is an arbitrary matter of convention.</p>
<p><strong>Advantage</strong><br>Allows composition to be carried out syntactically.<br>The laws of deductive logic in particular can be defined as syntactic processes which act irrespective of the meanings of the symbols involved.</p>
<p><strong>Disadvantage</strong><br>Abstracting away from the actual meanings may not be fully adequate for modeling semantic composition.<br>For example, <script type="math/tex">GOOD(JOHN)\wedge LAWYER(JOHN)</script> doesn’t mean that <em>John is a good lawyer</em>.<br>Modeling semantic composition means modeling the way in which meanings combine, and this requires that words have representations which are richer than single, arbitrary symbols.</p>
<h4 id="Connectionism"><a href="#Connectionism" class="headerlink" title="Connectionism"></a>Connectionism</h4><p>The key premise here is that knowledge is represented not as discrete symbols that enter into symbolic expressions, but as patterns of activation distributed over many processing elements. These representations are distributed in the sense that any single concept is represented as a pattern, that is, vector, of activation over many elements(nodes or units) that are typically assumed to correspond to neurons or small collections of neurons.</p>
<p><strong>Tensor product</strong><br>The tensor product <script type="math/tex">\mathbf{u}\otimes\mathbf{v}</script> is a matrix whose components are all the possible products <script type="math/tex">u_iv_j</script> of the components of vectors <strong>u</strong> and <strong>v</strong>.<br><img src="/images/composition/tensorProduct.jpg" alt=""><br>The tensor product has dimensionality m×n, which grows exponentially in size as more constituents are composed.</p>
<p><strong>Holographic reduced representation</strong><br>The tensor product is projected onto the space of the original vectors, thus avoiding any dimensionality increase.<br>The projection is defined in terms of <em>circular convolution</em>, which compresses the tensor product of two vectors. The compression is achieved by summing along the transdiagonal(?) elements of the tensor product. Noisy versions of the original vectors can be recovered by means of <em>circular correlation</em>, which is the approximate inverse of circular convolution. The success of circular correlation crucially depends on the components of the <em>n</em>-dimensional vectors <strong>u</strong> and <strong>v</strong> being real numbers and randomly distributed with mean 0 and variance 1/n.</p>
<p><strong>Binary spatter codes</strong><br>Binary spatter codes are a particularly simple form of holographic reduced representation. Typically, these vectors are random bit strings or binary <em>N</em> vectors (e.g., <em>N</em>=10000). Compositional representations are synthesized from parts or chunks. Chunks are combined by binding, which is the same as taking the exclusive or(XOR) of two vectors. Here, only the transdiagonal elements of the tensor product of two vectors are kept and the rest are discarded.</p>
<p>Both spatter codes and holographic reduced representations can be implemented efficiently and the dimensionality of the resulting vector does not change.<br>The downside is that operations like circular convolution are a form of lossy compression that introduces noise into the representation. To retrieve the original vectors from their bindings, a <em>clean-up memory</em> process is usually employed where the noisy vector is compared with all component vectors in order to find the closest one.</p>
<h4 id="Semantic-Space"><a href="#Semantic-Space" class="headerlink" title="Semantic Space"></a>Semantic Space</h4><p><strong>Premise</strong>: Words occurring within similar contexts are semantically similar.<br>Semantic space models extract from a corpus a set of counts representing the occurrences of a target word <em>t</em> in the specific context <em>c</em> of choice and then map these counts into the components of a vector in some space.<br>Semantic space models resemble the representations used in the connectionist literature. Words are represented as vectors and their meaning is distributed across many dimensions. Crucially, the vector components are neither binary nor randomly distributed(compared with holographic reduced representation and binary spatter code mentioned above). They correspond to co-occurrence counts, and it is assumed that differences in meaning arise from differences in the distribution of these counts across contexts.</p>
<h2 id="Composition-models"><a href="#Composition-models" class="headerlink" title="Composition models"></a>Composition models</h2><p><strong>Aim</strong>: construct vector representations for phrases and sentences.<br><strong>Note</strong>: the problem of combining semantic vectors to make a representation of a multiword phrase is different to the problem of how to incorporate information <em>about</em> multiword contexts into a distributional representation for a single target word.</p>
<p>Define <strong>p</strong>, the composition of vectors <strong>u</strong> and <strong>v</strong>, representing a pair of words which stand in some syntactic relation <em>R</em>, given some background knowledge <em>K</em> as: <script type="math/tex">\mathbf{p}=f(\mathbf{u},\mathbf{v},R,K)</script>.<br>To begin with, just ignore <em>K</em> so as to explore what can be achieved in the absence of any background or world knowledge.</p>
<p><strong>Assumption</strong><br>Constituents are represented by vectors which subsequently combine in some way to produce a new vector.<br><strong>p</strong> lies in the same space as <strong>u</strong> and (~?)<strong>v</strong>. This essentially means that all syntactic types have the same dimensionality.<br>The restriction renders the composition problem computationally feasible. </p>
<p>A hypothetical semantic space for <em>practical</em> and <em>difficulty</em></p>
<div class="table-container">
<table>
<thead>
<tr>
<th></th>
<th>music</th>
<th>solution</th>
<th>economy</th>
<th>craft</th>
<th>reasonable</th>
</tr>
</thead>
<tbody>
<tr>
<td>practical</td>
<td>0</td>
<td>6</td>
<td>2</td>
<td>10</td>
<td>4</td>
</tr>
<tr>
<td>difficulty</td>
<td>1</td>
<td>8</td>
<td>4</td>
<td>4</td>
<td>0</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Additive-composition-models"><a href="#Additive-composition-models" class="headerlink" title="Additive composition models"></a>Additive composition models</h3><h4 id="Simplest-addition"><a href="#Simplest-addition" class="headerlink" title="Simplest addition"></a>Simplest addition</h4><script type="math/tex; mode=display">\mathbf{p}=\mathbf{u}+\mathbf{v}</script><script type="math/tex; mode=display">\mathbf{practical}+\mathbf{difficulty}=\begin{bmatrix}1&14&6&14&4\end{bmatrix}</script><p>This model assumes that composition is a symmetric function of the constituents; in other words, the order of constituents essentially makes no difference.<br>Might be reasonable for certain structures, a list perhaps.</p>
<h4 id="Addition-with-neighbors"><a href="#Addition-with-neighbors" class="headerlink" title="Addition with neighbors"></a>Addition with neighbors</h4><p>A sum of predicate, argument, and a number of neighbors of the predicate:</p>
<script type="math/tex; mode=display">\mathbf{p}=\mathbf{u}+\mathbf{v}+\sum_in_i</script><p>Model the composition of a predicate with its argument in a manner that distinguishes the role of these constituents, making use of the lexicon of semantic representations to identify the features of each constituent relevant to their combination.<br>Considerable latitude is allowed in selecting the appropriate neighbors. Kintsch(2001) considers only the <em>m</em> most similar neighbors to the predicate, from which he subsequently selects <em>k</em>, those most similar to its argument.</p>
<p>E.g., in the composition of <em>practical</em> with <em>difficulty</em>, the chosen neighbor is <em>problem</em>, with <script type="math/tex">\mathbf{problem}=\begin{bmatrix}2&15&7&9&1\end{bmatrix}</script>,</p>
<script type="math/tex; mode=display">\mathbf{practical}+\mathbf{difficulty}+\mathbf{problem}=\begin{bmatrix}3&29&13&23&5\end{bmatrix}</script><p>In this process, the selection of relevant neighbors for the predicate plays a role similar to the integration of a representation with existing background knowledge in the original construction–integration model. Here, background knowledge takes the form of the lexicon from which the neighbors are drawn.</p>
<h4 id="Weighted-summation"><a href="#Weighted-summation" class="headerlink" title="Weighted summation"></a>Weighted summation</h4><p>Weight the constituents differentially in the summation.</p>
<script type="math/tex; mode=display">\mathbf{p}=\alpha\mathbf{v}+\beta\mathbf{u}</script><p>This makes the composition function asymmetric in <strong>u</strong> and <strong>v</strong> allowing their distinct syntactic roles to be recognized.<br>E.g., set α to 0.4 and β to 0.6.</p>
<script type="math/tex; mode=display">0.4\times\mathbf{practical}+0.6\times\mathbf{difficulty}=\begin{bmatrix}0.6&7.2&3.2&6.4&1.6\end{bmatrix}</script><p>(there’s some calculation mistake in the paper)</p>
<p><strong>Extrem form</strong></p>
<script type="math/tex; mode=display">\mathbf{p}=\mathbf{v}</script><p>One of the vectors(<strong>u</strong>) contributes nothing at all to the combination. It can serve a simple baseline against which to compare more sophisticated models.</p>
<h3 id="Multiplicative-function"><a href="#Multiplicative-function" class="headerlink" title="Multiplicative function"></a>Multiplicative function</h3><h4 id="Simple"><a href="#Simple" class="headerlink" title="Simple"></a>Simple</h4><script type="math/tex; mode=display">\mathbf{p}=\mathbf{u}\odot\mathbf{v}</script><p>where the symbol <script type="math/tex">\odot</script> represents multiplication of the corresponding components: <script type="math/tex">p_i=u_i\cdot v_i</script></p>
<script type="math/tex; mode=display">\mathbf{practical}\odot\mathbf{difficulty}=\begin{bmatrix}0&48&8&40&0\end{bmatrix}</script><p>It is still a symmetric function and thus does not take word order or syntax into account.</p>
<h4 id="Tensor-product"><a href="#Tensor-product" class="headerlink" title="Tensor product"></a>Tensor product</h4><script type="math/tex; mode=display">\mathbf{p}=\mathbf{u}\otimes\mathbf{v}</script><p>where the symbol <script type="math/tex">\otimes</script> stands for the operation of taking all pairwise products of the components of <strong>u</strong> and <strong>v</strong>: <script type="math/tex">p_{i,j}=u_i\cdot v_j</script></p>
<script type="math/tex; mode=display">\mathbf{practical}\otimes\mathbf{difficulty}=\begin{bmatrix}0&0&0&0&0\\6&48&24&24&0\\2&16&8&8&0\\10&80&40&40&0\\4&32&16&16&0\end{bmatrix}</script><h4 id="Circular-Convolution"><a href="#Circular-Convolution" class="headerlink" title="Circular Convolution"></a>Circular Convolution</h4><script type="math/tex; mode=display">\mathbf{p}=\mathbf{u}\circledast\mathbf{v}</script><p>where the symbol <script type="math/tex">\circledast</script> stands for a compression of the tensor product based on summing along its transdiagonal elements: <script type="math/tex">p_i=\sum_ju_j\cdot v_{(i-j)}</script><br>Subscripts are interpreted modulo <em>n</em> which gives the operation its circular nature.</p>
<script type="math/tex; mode=display">\mathbf{practical}\circledast\mathbf{difficulty}=\begin{bmatrix}80&62&66&50&116\end{bmatrix}</script><p>(the result in the paper is reversed, which seems wrong)</p>
<p>Temporarily not understand why multiplicative functions only affect magnitude but not direction, while additive models can have a considerable effect on both the magnitude and direction. And cosine similarity is itself insensitive to the magnitudes of vectors.</p>
<h3 id="Matrix-idea"><a href="#Matrix-idea" class="headerlink" title="Matrix idea"></a>Matrix idea</h3><h4 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h4><p>To see how the vector <strong>u</strong> can be thought of as something that modifies <strong>v</strong>, consider the partial product of <strong>C</strong> with <strong>u</strong>, producing a matrix which is called <strong>U</strong>.</p>
<script type="math/tex; mode=display">\mathbf{p=Cuv=Uv}</script><p>Here, the composition function can be thought of as the action of a matrix, <strong>U</strong>, representing one constituent, on a vector <strong>v</strong>, representing the other constituent. Since the authors’ decision to use vectors, they just make use of the  insight. Map a constituent vector, <strong>u</strong>, onto a matrix, <strong>U</strong>, while representing all words with vectors.</p>
<script type="math/tex; mode=display">U_{ij}=0,U_{ii}=u_i</script><p><strong>U</strong>‘s off-diagonal elements are zero and <strong>U</strong>‘s diagonal elements are equal to the components of <strong>u</strong>.<br>The action of this matrix on <strong>v</strong> is a type of dilation, in that it stretches and squeezes <strong>v</strong> in various directions. Specifically, <strong>v</strong> is scaled by a factor of <script type="math/tex">u_i</script> along the <em>i</em>th basis.<br>The drawback of this process is that its results are independent on the basis used.</p>
<h4 id="Parallel-and-Orthogonal-Decomposition"><a href="#Parallel-and-Orthogonal-Decomposition" class="headerlink" title="Parallel and Orthogonal Decomposition"></a>Parallel and Orthogonal Decomposition</h4><p>Ideally, we would like to have a basis-independent composition, that is, one which is based solely on the geometry of <strong>u</strong> and <strong>v</strong>. One way to achieve basis independence is by dilating <strong>v</strong> along the direction of <strong>u</strong>, rather than along the basis directions. Just decompose <strong>v</strong> into a component parallel to <strong>u</strong> and a component orthogonal to <strong>u</strong>, and then stretch the parallel component to modulate <strong>v</strong> to be more like <strong>u</strong>.<br><img src="/images/composition/decomposition.jpg" alt=""></p>
<script type="math/tex; mode=display">\mathbf{x=\frac{u\cdot v}{u\cdot u}u}</script><script type="math/tex; mode=display">\mathbf{y=v-x=v-\frac{u\cdot v}{u\cdot u}u}</script><p>By dilating <strong>x</strong> by a factor λ, while leaving <strong>y</strong> unchanged, we get a modified vector <strong>v</strong>‘, which has been stretched to emphasize the contribution of <strong>u</strong>:</p>
<script type="math/tex; mode=display">\mathbf{v'=\lambda x+y=\lambda\frac{u\cdot v}{u\cdot u}u+v-\frac{u\cdot v}{u\cdot u}u=(\lambda-1)\frac{u\cdot v}{u\cdot u}u+v}</script><p>Multiplying through by <script type="math/tex">\mathbf{u\cdot u}</script> makes the expression easier to work with(since the cosine similarity function is insensitive to the magnitudes of vectors)</p>
<script type="math/tex; mode=display">\mathbf{p=(u\cdot u)v+(\lambda-1)(u\cdot v)u}</script><p>From the given example, <script type="math/tex">\mathbf{practical\times practical}=156</script> and <script type="math/tex">\mathbf{practical\times difficulty}=96</script>. Assuming λ=2 and we can get</p>
<script type="math/tex; mode=display">156\times\mathbf{difficulty}+96\times\mathbf{practical}=\begin{bmatrix}156&1824&816&1584&384\end{bmatrix}</script><p>(Again there’s some mistakes in the paper, it confused the coefficient uu and uv.)</p>
<h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Code-implementation"><a href="#Code-implementation" class="headerlink" title="Code implementation"></a>Code implementation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># The simplest addition</span></div><div class="line"><span class="comment"># p = u + v</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">additive</span><span class="params">(u, v)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">        u: row vector</span></div><div class="line"><span class="string">        v: row vector</span></div><div class="line"><span class="string">        the size of u is the same as that of v</span></div><div class="line"><span class="string">    '''</span></div><div class="line">    <span class="keyword">return</span> np.add(u, v)</div><div class="line"></div><div class="line"><span class="comment"># Addition with neighbors</span></div><div class="line"><span class="comment"># p = u + v + n</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kintsch</span><span class="params">(u, v, n)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">        u: row vector</span></div><div class="line"><span class="string">        v: row vector</span></div><div class="line"><span class="string">        n: list of row vectors</span></div><div class="line"><span class="string">        the size of u is the same as that of v, as well as the elements of n</span></div><div class="line"><span class="string">    '''</span></div><div class="line">    result = u + v</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(n)):</div><div class="line">        result += n[i]</div><div class="line">    <span class="keyword">return</span> result</div><div class="line"></div><div class="line"><span class="comment"># Multiplication of the corresponding components(element-wise)</span></div><div class="line"><span class="comment"># p = u * v</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiplicative</span><span class="params">(u, v)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">        u: row vector</span></div><div class="line"><span class="string">        v: row vector</span></div><div class="line"><span class="string">        the size of u is the same as that of v</span></div><div class="line"><span class="string">    '''</span></div><div class="line">    <span class="keyword">return</span> u * v</div><div class="line"></div><div class="line"><span class="comment"># Tensor product</span></div><div class="line"><span class="comment"># p = u \otimes v</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensor</span><span class="params">(u, v)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">        u: row vector</span></div><div class="line"><span class="string">        v: row vector</span></div><div class="line"><span class="string">        the size of u is the same as that of v</span></div><div class="line"><span class="string">    '''</span></div><div class="line">    <span class="keyword">return</span> np.dot(u.T, v)</div><div class="line"></div><div class="line"><span class="comment"># Circular convolution</span></div><div class="line"><span class="comment"># p = u \circledast v</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">circular</span><span class="params">(u, v)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">        u: row vector</span></div><div class="line"><span class="string">        v: row vector</span></div><div class="line"><span class="string">        the size of u is the same as that of v</span></div><div class="line"><span class="string">    '''</span></div><div class="line"></div><div class="line">    <span class="comment"># --------------------------------------</span></div><div class="line">    <span class="comment">#   Reference:</span></div><div class="line">    <span class="comment">#   Kh40tiK at StackOverflow</span></div><div class="line">    <span class="comment">#   https://stackoverflow.com/questions/35474078/python-1d-array-circular-convolution</span></div><div class="line"></div><div class="line">    <span class="keyword">return</span> np.real(np.fft.ifft(np.fft.fft(u) * np.fft.fft(v)))</div><div class="line"></div><div class="line">    <span class="comment"># --------------------------------------</span></div><div class="line">    <span class="comment"># implementation with the definition of circular convolution</span></div><div class="line"></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">    result = np.zeros(u.shape)</span></div><div class="line"><span class="string">    for i in range(result.shape[1]):</span></div><div class="line"><span class="string">        for j in range(u.shape[1]):</span></div><div class="line"><span class="string">            if i - j &lt; 0:</span></div><div class="line"><span class="string">                result[0][i] += u[0][j] * v[0][i - j + u.shape[1]]</span></div><div class="line"><span class="string">            else:</span></div><div class="line"><span class="string">                result[0][i] += u[0][j] * v[0][i - j]</span></div><div class="line"><span class="string">    return result</span></div><div class="line"><span class="string">    '''</span></div><div class="line"></div><div class="line"><span class="comment"># Weighted additive</span></div><div class="line"><span class="comment"># p = alpha * u + beta * v</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">weighted</span><span class="params">(u, v, alpha, beta)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">        u: row vector</span></div><div class="line"><span class="string">        v: row vector</span></div><div class="line"><span class="string">        alpha: weighted parameter of u</span></div><div class="line"><span class="string">        beta: weighted parameter of v</span></div><div class="line"><span class="string">        the size of u is the same as that of v</span></div><div class="line"><span class="string">    '''</span></div><div class="line">    <span class="keyword">return</span> alpha * u + beta * v</div><div class="line"></div><div class="line"><span class="comment"># Dilation</span></div><div class="line"><span class="comment"># p=(u \cdot u) v + (lambda - 1) (u \cdot v) u</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">dilation</span><span class="params">(u, v, lamda)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">        u: row vector</span></div><div class="line"><span class="string">        v: row vector</span></div><div class="line"><span class="string">        lamda: dilation factor</span></div><div class="line"><span class="string">        the size of u as the same as that of v</span></div><div class="line"><span class="string">    '''</span></div><div class="line">    uu = np.sum(u * u)</div><div class="line">    print(uu)</div><div class="line">    uv = np.sum(u * v)</div><div class="line">    print(uv)</div><div class="line">    <span class="keyword">return</span> uu * v + (lamda - <span class="number">1</span>) * uv * u</div><div class="line"></div><div class="line"><span class="comment"># Head only, ignoring the effect of u</span></div><div class="line"><span class="comment"># p = v</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">headOnly</span><span class="params">(u, v)</span>:</span></div><div class="line">    <span class="string">'''</span></div><div class="line"><span class="string">        u: row vector</span></div><div class="line"><span class="string">        v: row vector</span></div><div class="line"><span class="string">    '''</span></div><div class="line">    <span class="keyword">return</span> v</div><div class="line"></div><div class="line"><span class="comment"># Semantic space for PRACTICAL and DIFFICULTY according to the paper</span></div><div class="line">practical = np.array([<span class="number">0</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">4</span>])</div><div class="line">difficulty = np.array([<span class="number">1</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">0</span>])</div><div class="line"></div><div class="line"><span class="comment"># Reshape the "rank-1 array" to vector(1-dimension matrix)</span></div><div class="line">practical = practical.reshape([<span class="number">1</span>, <span class="number">5</span>])</div><div class="line">difficulty = difficulty.reshape([<span class="number">1</span>, <span class="number">5</span>])</div><div class="line"></div><div class="line">problem = np.array([<span class="number">2</span>, <span class="number">15</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">1</span>])</div><div class="line"></div><div class="line">print(additive(practical, difficulty))</div><div class="line">print(kintsch(practical, difficulty, [problem]))</div><div class="line">print(multiplicative(practical, difficulty))</div><div class="line">print(tensor(practical, difficulty))</div><div class="line">print(weighted(practical, difficulty, <span class="number">0.4</span>, <span class="number">0.6</span>))</div><div class="line">print(circular(practical, difficulty))</div><div class="line">print(dilation(practical, difficulty, <span class="number">2</span>))</div></pre></td></tr></table></figure>
<h1 id="Comparison-Study-on-Critical-Components-in-Compositional-Model-for-Phrase-Representation"><a href="#Comparison-Study-on-Critical-Components-in-Compositional-Model-for-Phrase-Representation" class="headerlink" title="Comparison Study on Critical Components in Compositional Model for Phrase Representation"></a>Comparison Study on Critical Components in Compositional Model for Phrase Representation</h1><h2 id="Word-Representation"><a href="#Word-Representation" class="headerlink" title="Word Representation"></a>Word Representation</h2><p><strong>Count model</strong><br>The count model learns word vectors by calculating the co-occurrence frequency of each word with features. </p>
<p><strong>Predict model</strong><br>The predict model learns word vectors by maximizing the probability of the contexts in which the word is observed in the corpus.<br>The Skip-Gram model and CBOW model included in word2vec tool are most widely used for generating word vectors.</p>
<h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>A method described by many works. I’m just planning to write another post about Word2Vec in details.</p>
<h3 id="Retrofitting-Method"><a href="#Retrofitting-Method" class="headerlink" title="Retrofitting Method"></a>Retrofitting Method</h3><p>There’re some mistakes in introducing the retrofitting method in this paper(e.g., the neighbor word vector should be <script type="math/tex">q_j</script> instead of <script type="math/tex">q_i</script> which is confusing.) , so I just find the original paper describing retrofitting method: <a href="https://www.cs.cmu.edu/~hovy/papers/15HLT-retrofitting-word-vectors.pdf" target="_blank" rel="external">Retrofitting Word Vectors to Semantic Lexicons</a>.</p>
<p>Let <script type="math/tex">V=\{w_1,...,w_n\}</script> be a <strong>vocabulary</strong>, i.e., the set of word types, (<em>well, I don’t know why it’s the set of word types…</em>) and <script type="math/tex">\Omega</script> be an <strong>ontology</strong> that encodes semantic relations between words in <script type="math/tex">V</script>. <script type="math/tex">\Omega</script> is an undirected graph <script type="math/tex">(V,E)</script> with one vertex for each word type and edges <script type="math/tex">(w_i,w_j)\in E\subseteq V\times V</script> indicating a semantic relationship of interest.<br>The matrix <script type="math/tex">\hat{Q}</script> will be the collection of vector representation <script type="math/tex">\hat{q_i}\in\mathbb{R}^d</script>, for each <script type="math/tex">w_i\in V</script>, learned using a standard data-driven technique, where <script type="math/tex">d</script> is the length of the word vectors. The objective is to learn the matrix <script type="math/tex">Q=(q_1,...,q_n)</script> such that the columns are both close (under a distance metric) to their counterparts in <script type="math/tex">\hat{Q}</script> <em>and</em> to adjacent vertices in <script type="math/tex">\Omega</script>.<br>The distance between a pair of vectors is defined to be the Euclidean distance. Since we want the inferred word vector to be close to the observed value <script type="math/tex">\hat{q_i}</script> and close to its neighbors <script type="math/tex">q_j,\forall j</script> such that <script type="math/tex">(i,j)\in E</script>, the objective to be minimized becomes:</p>
<script type="math/tex; mode=display">\Psi(Q)=\sum_{i=1}^n[\alpha_i||q_i-\hat{q_i}||^2+\sum_{(i,j)\in E}\beta_{i,j}||q_i-q_j||^2]</script><p>where <script type="math/tex">\alpha</script> and <script type="math/tex">\beta</script> values control the relative strengths of associations.</p>
<h3 id="Word-Paraphrasing"><a href="#Word-Paraphrasing" class="headerlink" title="Word Paraphrasing"></a>Word Paraphrasing</h3><p>Related paper: <a href="https://arxiv.org/pdf/1506.03487.pdf" target="_blank" rel="external">From Paraphrase Database to Compositional Paraphrase Model and Back</a><br>Train word vectors with a contrastive max-margin objective function. Specifically, the training data consisting of a set X of word paraphrase pairs <script type="math/tex">(x_1,x_2)</script>, while <script type="math/tex">(t_1,t_2)</script> are negative examples that are the most similar word pairs to <script type="math/tex">(x_1,x_2)</script> in a mini-batch during optimization. The objective function is given as follows:</p>
<script type="math/tex; mode=display">\min{W_w}\frac{1}{|X|}(\sum_{(x_1,x_2)\in X}\max{(0,1-W_w^{x_1}\cdot W_w^{x_2}+W_w^{x_1}\cdot W_w^{t_1})+\max{(0,1-W_w^{x_1}\cdot W_w^{x_2}+W_w^{x_2}\cdot W_w^{t_2})}+\lambda||W_{w_{initial}}-W_w||^2}</script><p>where <script type="math/tex">\lambda</script> is the regularization parameter, <script type="math/tex">|X|</script> is the length of training paraphrase pairs, <script type="math/tex">W_w</script> is the target word vector matrix, and <script type="math/tex">W_{w_{initial}}</script> is the initial word vector matrix.</p>
<hr>
<h2 id="Training-data"><a href="#Training-data" class="headerlink" title="Training data"></a>Training data</h2><p>For bigram phrase similarity task, there are two general types of training data in existing work.</p>
<p><strong>Pseudo-word training data</strong><br>Consists of tuples in the form {adjective1 noun1, adjective1-noun1}. E.g., {older man, older-man}.<br>Pseudo-word training data have been widely used in composition models.</p>
<p><strong>Pair training data</strong><br>Consists of tuples in the form {adjective1 noun1, adjective2 noun2}. E.g., {older man, elderly woman}.</p>
<p>For multi-word phrase, pair training data is the first choice because it is hard to learn the representation of pseudo-words with multiple words. So this paper use pair training data for multi-word phrase experiments.</p>
<h2 id="Composition-Function"><a href="#Composition-Function" class="headerlink" title="Composition Function"></a>Composition Function</h2><p>With a phrase <em>p</em> consisting of two words <script type="math/tex">w^{(1)}</script> and <script type="math/tex">w^{(2)}</script>, we can establish equation as follows:</p>
<script type="math/tex; mode=display">f(\vec w^{(1)},\vec w^{(2)}=\vec{p})</script><p>where <script type="math/tex">\vec w^{(1)},\vec w^{(2)}</script> are the word representations, <script type="math/tex">\vec{p}</script> is the phrase representation, and <script type="math/tex">f</script> is the composition function.</p>
<p>The Additive model assumes that the meaning of the composition is a linear combination of the constituent words.<br>The Additive model described the phenomenon: people concatenate the meanings of two words when understanding phrases.</p>
<p>The Multiplicative model assumes that the meaning of composition is the element-wise product of the two vectors.<br>The multiplicative model has the highest correlation with the neural activity observed in human brain when reading adjective-noun phrase data.</p>
<p>Composition functions such as Matrix, RecNN(Recursive Neural Network) and RNN transform word vectors into another vector space through matrix transformations and nonlinear functions. These three functions differ primarily in the order of transformation.<br>The Matrix model first transforms component words into another vector space and then composes them using addition. The RecNN model takes word order into consideration, concatenates vector component words and then transforms the vector using a matrix and nonlinearity. The RNN model composes words in a sentence from left to right by forming new representations from previous representations and the representation of the current word.</p>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Hael Chan
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://haelchan.me/2018/04/08/composition-model/" title="Learning note of two papers about composition model">http://haelchan.me/2018/04/08/composition-model/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/reading-report/" rel="tag"># reading report</a>
          
            <a href="/tags/natural-language-language/" rel="tag"># natural language language</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/31/NLP-note/" rel="next" title="COMS W4705 Natural Language Processing Note">
                <i class="fa fa-chevron-left"></i> COMS W4705 Natural Language Processing Note
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/27/tensorflow-learning/" rel="prev" title="TensorFlow Learning">
                TensorFlow Learning <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>
  


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Hael Chan" />
            
              <p class="site-author-name" itemprop="name">Hael Chan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/haelchan" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:f.procumbens@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://twitter.com/Procumbens" target="_blank" title="Twitter">
                    
                      <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/hael-c/activities" target="_blank" title="知乎">
                    
                      <i class="fa fa-fw fa-compass"></i>知乎</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-globe"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://learnerwn.github.io" title="WN_Blog" target="_blank">WN_Blog</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://ruder.io" title="Sebastian Ruder" target="_blank">Sebastian Ruder</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://godweiyang.com" title="WeiYang Blog" target="_blank">WeiYang Blog</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Composition-in-Distributional-Models-of-Semantics"><span class="nav-number">1.</span> <span class="nav-text">Composition in Distributional Models of Semantics</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Semantic-representation"><span class="nav-number">1.1.</span> <span class="nav-text">Semantic representation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Semantic-Networks"><span class="nav-number">1.1.1.</span> <span class="nav-text">Semantic Networks</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Feature-based-Models"><span class="nav-number">1.1.2.</span> <span class="nav-text">Feature-based Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Semantic-Spaces"><span class="nav-number">1.1.3.</span> <span class="nav-text">Semantic Spaces</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Some-semantic-space-models"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">Some semantic space models</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Application"><span class="nav-number">1.1.3.2.</span> <span class="nav-text">Application</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Composition"><span class="nav-number">1.2.</span> <span class="nav-text">Composition</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Preset"><span class="nav-number">1.2.1.</span> <span class="nav-text">Preset</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Functions"><span class="nav-number">1.2.2.</span> <span class="nav-text">Functions</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Logic-based-view"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Logic-based view</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Connectionism"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">Connectionism</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Semantic-Space"><span class="nav-number">1.2.2.3.</span> <span class="nav-text">Semantic Space</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Composition-models"><span class="nav-number">1.3.</span> <span class="nav-text">Composition models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Additive-composition-models"><span class="nav-number">1.3.1.</span> <span class="nav-text">Additive composition models</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Simplest-addition"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">Simplest addition</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Addition-with-neighbors"><span class="nav-number">1.3.1.2.</span> <span class="nav-text">Addition with neighbors</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Weighted-summation"><span class="nav-number">1.3.1.3.</span> <span class="nav-text">Weighted summation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Multiplicative-function"><span class="nav-number">1.3.2.</span> <span class="nav-text">Multiplicative function</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Simple"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">Simple</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Tensor-product"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">Tensor product</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Circular-Convolution"><span class="nav-number">1.3.2.3.</span> <span class="nav-text">Circular Convolution</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Matrix-idea"><span class="nav-number">1.3.3.</span> <span class="nav-text">Matrix idea</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Basic"><span class="nav-number">1.3.3.1.</span> <span class="nav-text">Basic</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Parallel-and-Orthogonal-Decomposition"><span class="nav-number">1.3.3.2.</span> <span class="nav-text">Parallel and Orthogonal Decomposition</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Experiment"><span class="nav-number">1.4.</span> <span class="nav-text">Experiment</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Code-implementation"><span class="nav-number">1.4.1.</span> <span class="nav-text">Code implementation</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Comparison-Study-on-Critical-Components-in-Compositional-Model-for-Phrase-Representation"><span class="nav-number">2.</span> <span class="nav-text">Comparison Study on Critical Components in Compositional Model for Phrase Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Word-Representation"><span class="nav-number">2.1.</span> <span class="nav-text">Word Representation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Word2Vec"><span class="nav-number">2.1.1.</span> <span class="nav-text">Word2Vec</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Retrofitting-Method"><span class="nav-number">2.1.2.</span> <span class="nav-text">Retrofitting Method</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Word-Paraphrasing"><span class="nav-number">2.1.3.</span> <span class="nav-text">Word Paraphrasing</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-data"><span class="nav-number">2.2.</span> <span class="nav-text">Training data</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Composition-Function"><span class="nav-number">2.3.</span> <span class="nav-text">Composition Function</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hael Chan</span>

  
</div>









<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<span id="busuanzi_container_site_uv">
  欢迎~您是本站的第<span id="busuanzi_value_site_uv"></span>位访客
</span>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://hael.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://haelchan.me/2018/04/08/composition-model/';
          this.page.identifier = '2018/04/08/composition-model/';
          this.page.title = 'Learning note of two papers about composition model';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://hael.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'manual') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("3HhWLTewaKTSPj5DC3qp5b2m-gzGzoHsz", "zjxN2hpHQEmyMaz0XBicI3bn");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
