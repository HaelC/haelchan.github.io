<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="learning note,natural language processing,Python," />





  <link rel="alternate" href="/atom.xml" title="Hael's Blog" type="application/atom+xml" />






<meta name="description" content="This note is based on Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit. I’ve uploaded the exercises solution to GitHub. Texts and wordsTexts are represented i">
<meta name="keywords" content="learning note,natural language processing,Python">
<meta property="og:type" content="article">
<meta property="og:title" content="Natural Language Processing with Python and NLTK">
<meta property="og:url" content="http://haelchan.me/2018/07/23/nlp-with-python-and-nltk/index.html">
<meta property="og:site_name" content="Hael&#39;s Blog">
<meta property="og:description" content="This note is based on Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit. I’ve uploaded the exercises solution to GitHub. Texts and wordsTexts are represented i">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://haelchan.me/images/linguistic/text-corpus-structure.png">
<meta property="og:image" content="http://haelchan.me/images/linguistic/lexicon.png">
<meta property="og:image" content="http://haelchan.me/images/linguistic/pipeline.png">
<meta property="og:updated_time" content="2018-09-10T15:49:59.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Natural Language Processing with Python and NLTK">
<meta name="twitter:description" content="This note is based on Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit. I’ve uploaded the exercises solution to GitHub. Texts and wordsTexts are represented i">
<meta name="twitter:image" content="http://haelchan.me/images/linguistic/text-corpus-structure.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://haelchan.me/2018/07/23/nlp-with-python-and-nltk/"/>





  <title>Natural Language Processing with Python and NLTK | Hael's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hael's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://haelchan.me/2018/07/23/nlp-with-python-and-nltk/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hael Chan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hael's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Natural Language Processing with Python and NLTK</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-07-23T16:20:56+08:00">
                2018-07-23
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-09-10T23:49:59+08:00">
                2018-09-10
              </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/07/23/nlp-with-python-and-nltk/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/07/23/nlp-with-python-and-nltk/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/07/23/nlp-with-python-and-nltk/" class="leancloud_visitors" data-flag-title="Natural Language Processing with Python and NLTK">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>This note is based on <a href="http://www.nltk.org/book/" target="_blank" rel="external">Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit</a>. I’ve uploaded the exercises solution to <a href="https://github.com/HaelChan/NLP-with-Python-Solutions" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Texts-and-words"><a href="#Texts-and-words" class="headerlink" title="Texts and words"></a>Texts and words</h2><p>Texts are represented in Python using lists. We can use indexing, slicing, and the <code>len()</code> function.</p>
<p>Some word comparison operators: <code>s.startswith(t)</code>, <code>s.endswith(t)</code>, <code>t in s</code>, <code>s.islower()</code>, <code>s.isupper()</code>, <code>s.isalpha()</code>, <code>s.isalnum()</code>, <code>s.isdigit()</code>, <code>s.istitle()</code>.</p>
<h3 id="Searching"><a href="#Searching" class="headerlink" title="Searching"></a>Searching</h3><p><code>text1.concordance(&quot;monstrous&quot;)</code><br>A concordance view shows us every occurrence of a given word, together with some context.<br>A concordance permits us to see words in context.</p>
<p><code>text1.similar(&quot;monstrous&quot;)</code><br>We can find out other words appear in a similar range of contexts, by appending the term <code>similar</code> to the name of the text, then inserting the relevant word in parentheses. (a bit like synonyms)</p>
<p><code>text2.common_contexts([&quot;monstrous&quot;, &quot;very&quot;])</code><br>The term <code>common_contexts</code> allows us to examine just the contexts that are shared by two or more words.</p>
<p><code>text4.dispersion_plot([&quot;citizens&quot;, &quot;democracy&quot;, &quot;freedom&quot;, &quot;duties&quot;, &quot;America&quot;])</code><br>We can determine the location of a word in the text: how many words from the beginning it appears. This positional information can be displayed using a dispersion plot.</p>
<p><strong>Fine-grained Selection of Words</strong><br>The mathematical set notation and corresponding Python expression.</p>
<script type="math/tex; mode=display">\{w|w\in V&P(w)\}</script><p><code>[w for w in V if p(w)]</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">V = set(text1)</div><div class="line">long_words = [w <span class="keyword">for</span> w <span class="keyword">in</span> V <span class="keyword">if</span> len(w) &gt; <span class="number">15</span>]</div></pre></td></tr></table></figure>
<p><strong>Collocations and Bigrams</strong><br>The bigram is written as <code>(&#39;than&#39;, &#39;said&#39;)</code> in Python.<br>A collocation is a sequence of words that occur together unusually often. Collocations are essentially just frequent bigrams, except that we want to pay more attention to the cases that involve rare words. In particular, we want to find bigrams that occur more often then we would expect based on the frequency of the individual words.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">list(bigrams([<span class="string">'more'</span>, <span class="string">'is'</span>, <span class="string">'said'</span>, <span class="string">'than'</span>, <span class="string">'done'</span>]))</div><div class="line">text4.collocations()</div><div class="line">``` </div><div class="line"></div><div class="line"><span class="comment">### Counting</span></div><div class="line">A **token** <span class="keyword">is</span> the technical name <span class="keyword">for</span> a sequence of characters that we want to trat <span class="keyword">as</span> a group.</div><div class="line">We will think of a text <span class="keyword">as</span> nothing more than a sequence of words <span class="keyword">and</span> punctuation. Thus we can use `len()` to compute the number of tokens.</div><div class="line">A **word type** <span class="keyword">is</span> the form <span class="keyword">or</span> spelling of the word independently of its specific occurrences <span class="keyword">in</span> a text - that <span class="keyword">is</span>, the word considered <span class="keyword">as</span> a unique item of vocabulary.</div><div class="line">Since our tokens include punctuation symbols <span class="keyword">as</span> well, we will generally call these unique items **types** instead of word types.</div><div class="line"></div><div class="line">```Python</div><div class="line">len(text1)                                                      <span class="comment"># the tokens of the text</span></div><div class="line">len(set(text1))                                                 <span class="comment"># the types of the text considering case</span></div><div class="line">len(set(word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> text1))                        <span class="comment"># without considering case</span></div><div class="line">len(set(word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> text1 <span class="keyword">if</span> word.isalpha()))      <span class="comment"># eliminate numbers and punctuation</span></div></pre></td></tr></table></figure>
<p>A <strong>frequency distribution</strong> tells us the frequency of each vocabulary item in the text.<br>FreqDist can be treated as <code>dictionary</code> in Python, where the word(or word length, etc) is the key, and the occurrence is the corresponding value.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">fdist1 = FreqDist(text1)                        <span class="comment"># invoke FreqDist(), pass the name of the text as an argument</span></div><div class="line">fdist1.most_common(<span class="number">50</span>)                          <span class="comment"># gives a list of the 50 most frequently occurring types in the text</span></div><div class="line">fdist1[<span class="string">'whale'</span>]                                 <span class="comment"># the frequency of the given word</span></div><div class="line">fdist1.hapaxes()                                <span class="comment"># find the words that occur only once</span></div><div class="line"></div><div class="line">fdist = FreqDist(len(w) <span class="keyword">for</span> w <span class="keyword">in</span> text1)         <span class="comment"># the length of words' frequency distribution</span></div><div class="line">fdist.most_common()                             <span class="comment"># if no argument is passed, just print all word lengths</span></div><div class="line">fdist.max()                                     <span class="comment"># print the most frequent word length</span></div><div class="line">fdist[<span class="number">3</span>]                                        <span class="comment"># print the word length 3's occurrence</span></div><div class="line">fdist.freq(<span class="number">3</span>)                                   <span class="comment"># print the frequency of word length 3</span></div></pre></td></tr></table></figure>
<p>functions defined for NLTK’s Frequency Distributions can be found <a href="http://www.nltk.org/book/ch01.html#tab-word-tests" target="_blank" rel="external">here</a></p>
<h2 id="Text-Corpora-and-Lexical-Resources"><a href="#Text-Corpora-and-Lexical-Resources" class="headerlink" title="Text Corpora and Lexical Resources"></a>Text Corpora and Lexical Resources</h2><h3 id="Accessing-Text-Corpora"><a href="#Accessing-Text-Corpora" class="headerlink" title="Accessing Text Corpora"></a>Accessing Text Corpora</h3><p>A text corpus is a large, structured collection of texts. Some text corpora are categorized, e.g., by genre or topic; sometimes the categories of a corpus overlap each other.<br><img src="/images/linguistic/text-corpus-structure.png" alt=""><br>The NLTK has many corpus in the package <code>nltk.corpus</code>. To perform the functions introduced before, we have to employ the following pair of statements:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">emma = nltk.Text(nltk.corpus.guterberg.words(<span class="string">'austen-emma.txt'</span>))        <span class="comment"># type cast, from StreamBackedCorpusView to Text</span></div><div class="line">emma.concordance(<span class="string">'surprise'</span>)</div></pre></td></tr></table></figure>
<p>A short program to display information about each text, by looping over all the values of <code>fileid</code> corresponding to the <code>gutenberg</code> file identifiers and then computing statistics for each text.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> fileid <span class="keyword">in</span> gutenberg.fileids():</div><div class="line">    num_chars = len(gutenberg.raw(fileid))                              <span class="comment"># raw() tells us how many letters occur in the text</span></div><div class="line">    num_words = len(gutenberg.words(fileid))                            <span class="comment"># words() divides the text into words</span></div><div class="line">    num_sents = len(gutenberg.sents(fileid))                            <span class="comment"># sents() divides the text up into its sentences, where each sentence is a list of words</span></div><div class="line">    num_vocab = len(set(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> gutenberg.words(fileid)))    <span class="comment"># count unique words</span></div><div class="line">    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)   <span class="comment"># use round() to round each number to the nearest integer</span></div><div class="line">    <span class="comment"># average word lenth, average sentence length, the number of times each vocabulary item appears in the text on average</span></div></pre></td></tr></table></figure>
<p><strong>Brown Corpus</strong><br>The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre.(see <a href="http://www.nltk.org/book/ch02.html#tab-brown-sources" target="_blank" rel="external">here</a> for detail)<br>We can access the corpus as a list of words, or a list of sentences. We can optionally specify particular categories or files to read.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">brown.categories()</div><div class="line">brown.fileids()</div><div class="line">brown.words(categories=<span class="string">'news'</span>)</div><div class="line">brown.words(fileids=<span class="string">'cg22'</span>)</div><div class="line">brown.sents(categories=[<span class="string">'news'</span>, <span class="string">'editorial'</span>, <span class="string">'reviews'</span>])</div></pre></td></tr></table></figure>
<p>Use Brown Corpus to study <strong>stylistics</strong>: systematic differences between genres.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">news_text = brown.words(categories=<span class="string">'news'</span>)</div><div class="line">fdist = nltk.FreqDist(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> news_text)</div><div class="line">modals = [<span class="string">'can'</span>, <span class="string">'could'</span>, <span class="string">'may'</span>, <span class="string">'might'</span>, <span class="string">'must'</span>, <span class="string">'will'</span>]</div><div class="line"><span class="keyword">for</span> m <span class="keyword">in</span> modals:</div><div class="line">    print(m + <span class="string">':'</span>, fdist[m], end=<span class="string">' '</span>)</div><div class="line">print()                                                                         <span class="comment"># I add this line to modify the output</span></div><div class="line"></div><div class="line">cfd = nltk.ConditionalFreqDist(</div><div class="line">            (genre, word.lower())                                               <span class="comment"># to count the Uppercased words as well, or the statistics would be inconsistent</span></div><div class="line">            <span class="keyword">for</span> genre <span class="keyword">in</span> brown.categories()</div><div class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> brown.words(categories=genre))</div><div class="line">genres = [<span class="string">'news'</span>, <span class="string">'religion'</span>, <span class="string">'hobbies'</span>, <span class="string">'science_fiction'</span>, <span class="string">'romance'</span>, <span class="string">'humor'</span>]         <span class="comment"># genres = brown.categories()</span></div><div class="line">cfd.tabulate(conditions=genres, samples=modals)</div></pre></td></tr></table></figure>
<p>A conditional frequency distribution is a collection of frequency distributions, each one for a different “condition”. The condition will often be the category of the text.<br>A frequency distribution counts observable events, such as the appearance of words in a text. A conditional frequency distribution needs to pair each event with a condition. </p>
<p>NLTK’s Conditional Frequency Distributions: commonly-used methods and idioms for defining, accessing, and visualizing a conditional frequency distribution of counters.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Example</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>cfdist = ConditionalFreqDist(pairs)</td>
<td>create a conditional frequency distribution from a list of pairs</td>
</tr>
<tr>
<td>cfdist.conditions()</td>
<td>the conditions</td>
</tr>
<tr>
<td>cfdist[condition]</td>
<td>the frequency distribution for this condition</td>
</tr>
<tr>
<td>cfdist[condition][sample]</td>
<td>frequency for the given sample for this condition</td>
</tr>
<tr>
<td>cfdist.tabulate()</td>
<td>tabulate the conditional frequency distribution</td>
</tr>
<tr>
<td>cfdist.tabulate(samples, conditions)</td>
<td>tabulation limited to the specified samples and conditions</td>
</tr>
<tr>
<td>cfdist.plot()</td>
<td>graphical plot of the conditional frequency distribution</td>
</tr>
<tr>
<td>cfdist.plot(samples, conditions)</td>
<td>graphical plot limited to the specified samples and conditions</td>
</tr>
<tr>
<td>cfdist1 &lt; cfdist2</td>
<td>test if samples in cfdist1 occur less frequently than in cfdist2</td>
</tr>
</tbody>
</table>
</div>
<p><strong>Reuters Corpus</strong><br>The Reuters Corpus contains 10788 news documents totaling 1.3 million words. The documents have been classified into 90 topics and grouped into training and test sets.<br>Unlike the Brown Corpus, categories in the Reuters corpus overlap with each other, simply because a news story often covers multiple topics. We can ask for the topics covered by one or more documents, or for the documents included in one or more categories.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">reuters.categories(<span class="string">'training/9865'</span>)</div><div class="line">reuters.fileids(<span class="string">'barley'</span>)</div></pre></td></tr></table></figure>
<p><a href="http://www.nltk.org/book/ch02.html#tab-corpora" target="_blank" rel="external">Here</a> lists some of the Corpora and Corpus Samples Distributed with NLTK. For more information consult <a href="http://www.nltk.org/howto/" target="_blank" rel="external">NLTK HOWTOs</a>.</p>
<p>Basic Corpus Functionality defined in NLTK:</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Example</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>fileids()</td>
<td>the files of the corpus</td>
</tr>
<tr>
<td>fileids([categories])</td>
<td>the files of the corpus corresponding to these categories</td>
</tr>
<tr>
<td>categories()</td>
<td>the categories of the corpus</td>
</tr>
<tr>
<td>categories([fileids])</td>
<td>the categories of the corpus corresponding to these files</td>
</tr>
<tr>
<td>raw()</td>
<td>the raw content of the corpus</td>
</tr>
<tr>
<td>raw(fileids=[f1,f2,f3])</td>
<td>the raw content of the specified files</td>
</tr>
<tr>
<td>raw(categories=[c1,c2])</td>
<td>the raw content of the specified categories</td>
</tr>
<tr>
<td>words()</td>
<td>the words of the whole corpus</td>
</tr>
<tr>
<td>words(fileids=[f1,f2,f3])</td>
<td>the words of the specified fileids</td>
</tr>
<tr>
<td>words(categories=[c1,c2])</td>
<td>the words of the specified categories</td>
</tr>
<tr>
<td>sents()</td>
<td>the sentences of the whole corpus</td>
</tr>
<tr>
<td>sents(fileids=[f1,f2,f3])</td>
<td>the sentences of the specified fileids</td>
</tr>
<tr>
<td>sents(categories=[c1,c2])</td>
<td>the sentences of the specified categories</td>
</tr>
<tr>
<td>abspath(fileid)</td>
<td>the location of the given file on disk</td>
</tr>
<tr>
<td>encoding(fileid)</td>
<td>the encoding of the file (if known)</td>
</tr>
<tr>
<td>open(fileid)</td>
<td>open a stream for reading the given corpus file</td>
</tr>
<tr>
<td>root</td>
<td>if the path to the root of locally installed corpus</td>
</tr>
<tr>
<td>readme()</td>
<td>the contents of the README file of the corpus</td>
</tr>
</tbody>
</table>
</div>
<h3 id="Lexical-Resources"><a href="#Lexical-Resources" class="headerlink" title="Lexical Resources"></a>Lexical Resources</h3><p>A lexicon, or lexical resource, is a collection of words and/or phrases along with associated such as part of speech and sense definition. Lexical resources are secondary to texts, and are usually created and enriched with the help of texts. For example, <code>vocab = sorted(set(my_text))</code> and <code>word_freq = FreqDist(my_text)</code> are both simple lexical resources.<br><img src="/images/linguistic/lexicon.png" alt=""><br><em>Lexicon Terminology: lexical entries for two lemmas having the same spelling (homonyms), providing part of speech and gloss information.</em></p>
<p><strong>Wordlist Corpora</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># find unusual or mis-spelt words in a text corpus</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">unusual_words</span><span class="params">(text)</span>:</span></div><div class="line">    text_vocab = set(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> text <span class="keyword">if</span> w.isalpha())</div><div class="line">    english_vocab = set(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> nltk.corpus.words.words())</div><div class="line">    unusual = text_vocab - english_vocab</div><div class="line">    <span class="keyword">return</span> sorted(unusual)</div><div class="line"></div><div class="line">unusual_words(nltk.corpus.gutenberg.words(<span class="string">'austen-sense.txt'</span>))</div><div class="line">unusual_words(nltk.corpus.nps_chat.words())</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># word list for solving word puzzles</span></div><div class="line">puzzle_letters = nltk.FreqDist(<span class="string">'egivrvonl'</span>)             <span class="comment"># calculate frequency distribution for given word puzzle letters</span></div><div class="line">obligatory = <span class="string">'r'</span>                                        <span class="comment"># the center letter which must be included</span></div><div class="line">wordlist = nltk.corpus.words.words()</div><div class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> wordlist <span class="keyword">if</span> len(w) &gt;= <span class="number">6</span></div><div class="line">                     <span class="keyword">and</span> obligatory <span class="keyword">in</span> w</div><div class="line">                     <span class="keyword">and</span> nltk.FreqDist(w) &lt;= puzzle_letters]    <span class="comment"># the comparison of frequency distribution of each letter</span></div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># find names which appear in both male.txt and female.txt</span></div><div class="line">names = nltk.corpus.names                                       <span class="comment"># which contains 'male.txt' and 'female.txt'</span></div><div class="line">male_names = names.words(<span class="string">'male.txt'</span>)</div><div class="line">female_names = names.words(<span class="string">'female.txt'</span>)</div><div class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> male_names <span class="keyword">if</span> w <span class="keyword">in</span> female_names]</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># comparative wordlist - Swadesh wordlists</span></div><div class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> swadesh</div><div class="line">swedesh.fileids()</div><div class="line">fr2en = swadesh.entries([<span class="string">'fr'</span>, <span class="string">'en'</span>])</div><div class="line">de2en = swadesh.entries([<span class="string">'de'</span>, <span class="string">'en'</span>])</div><div class="line">es2en = swadesh.entries([<span class="string">'es'</span>, <span class="string">'en'</span>])</div><div class="line">translate = dict(fr2en)                         <span class="comment"># convert into a simple dictionary</span></div><div class="line">translate.update(dict(de2en))</div><div class="line">translate.update(dict(es2en))</div><div class="line">translate[<span class="string">'chien'</span>]                              <span class="comment"># 'dog'</span></div></pre></td></tr></table></figure>
<p><em>The CMU Pronouncing Dictionary, Toolbox are introduced in the book, I’ll just omit them in the note.</em></p>
<h3 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h3><p>WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155287 words and 117659 synonym sets.</p>
<p><strong>Synsets</strong><br>With the WordNet, we can find the word’s synonyms in <em>synsets</em> - “synonym set”, definitions and examples as well.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># synsets</span></div><div class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> wordnet <span class="keyword">as</span> wn</div><div class="line">wn.synsets(<span class="string">'motorcar'</span>)                          <span class="comment"># [Synset('car.n.01')]</span></div><div class="line">wn.synset(<span class="string">'car.n.01'</span>).lemma_names()             <span class="comment"># ['car', 'auto', 'automobile', 'machine', 'motorcar']</span></div><div class="line">wn.synset(<span class="string">'car.n.01'</span>).definition()</div><div class="line">wn.synset(<span class="string">'car.n.01'</span>).examples()</div></pre></td></tr></table></figure>
<p><strong>Hyponyms and hypernyms</strong><br>WordNet synsets correspond to abstract concepts, and they don’t always have corresponding words in English. These concepts are linked together in a hierarchy. (See hyponyms and in lexical relations.)<br>The corresponding methods are <code>hyponyms()</code> and <code>hypernyms()</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># hyponyms and hypernyms</span></div><div class="line">motorcar = wn.synset(<span class="string">'car.n.01'</span>)</div><div class="line">types_of_motorcar = motorcar.hyponyms()</div><div class="line">parent_of_motorcar = motorcar.hypernyms()</div><div class="line">paths = motorcar.hypernym_paths()</div></pre></td></tr></table></figure>
<p><strong>Some other lexical relations</strong><br>Another important way to navigate the WordNet network is from items to their components (meronyms), or to the things they are contained in (holonyms). There are three kinds of holonym-meronym relation: <code>member_meronyms()</code>, <code>part_meronyms()</code>, <code>substance_meronyms()</code>, <code>member_holonyms()</code>, <code>part_holonyms()</code>, <code>substance_holonyms()</code>.<br>There are also relationships between verbs. For example, the act of <em>walking</em> involves the act of <em>stepping</em>, so walking entails stepping. Some verbs have multiple entailments.(NLTK also includes VerbNet, a hierarhical verb lexicon linked to WordNet. It can be accessed with <code>nltk.corpus.verbnet</code>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># meronyms and holonyms</span></div><div class="line">tree = wn.synset(<span class="string">'tree.n.01'</span>)</div><div class="line">tree.part_meronyms()</div><div class="line">tree.substance_meronyms()</div><div class="line">tree.member_holonyms()</div><div class="line"></div><div class="line"><span class="comment"># entailments</span></div><div class="line">wn.synset(<span class="string">'walk.v.01'</span>).entailments()</div><div class="line">wn.synset(<span class="string">'eat.v.01'</span>).entailments()</div><div class="line">wn.synset(<span class="string">'tease.v.03'</span>).entailments()</div><div class="line"></div><div class="line"><span class="comment"># antonymy</span></div><div class="line">wn.lemma(<span class="string">'supply.n.02.supply'</span>).antonymys()</div><div class="line">wn.lemma(<span class="string">'rush.v.01.rush'</span>).antonymys()</div><div class="line">wn.lemma(<span class="string">'horizontal.a.01.horizontal'</span>).antonymys()</div><div class="line">wn.lemma(<span class="string">'staccato.r.01.staccato'</span>).antonymys()</div></pre></td></tr></table></figure>
<p><strong>Semantic Similarity</strong><br>Given a particular synset, we can traverse the WordNet network to find synsets with related meanings. Knowing which words are semantically related is useful for indexing a collection of texts, so that a search for a general term will match documents containing specific terms.<br>We can qualify the concept of generality(specific or general) by looking up the depth of the synset.<br><code>path_similarity</code> assigns a score in the range 0–1 based on the shortest path that connects the concepts in the hypernym hierarchy (-1 is returned in those cases where a path cannot be found). Comparing a synset with itself will return 1. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">right_whale = wn.synset(<span class="string">'right_whale.n.01'</span>)</div><div class="line">orca = wn.synset(<span class="string">'orca.n.01'</span>)</div><div class="line">minke = wn.synset(<span class="string">'minke_whale.n.01'</span>)</div><div class="line">tortoise = wn.synset(<span class="string">'tortoise.n.01'</span>)</div><div class="line">novel = wn.synset(<span class="string">'novel.n.01'</span>)</div><div class="line"></div><div class="line">right_whale.lowest_common_hypernyms(minke)</div><div class="line">right_whale.min_depth()</div><div class="line">right_whale.path_similarity(minke)</div></pre></td></tr></table></figure>
<h2 id="Processing-Raw-Text"><a href="#Processing-Raw-Text" class="headerlink" title="Processing Raw Text"></a>Processing Raw Text</h2><h3 id="Accessing-Text"><a href="#Accessing-Text" class="headerlink" title="Accessing Text"></a>Accessing Text</h3><p><strong>From Web</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># from electronic books</span></div><div class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</div><div class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> word_tokenize</div><div class="line">url = <span class="string">"http://www.gutenberg.org/files/2554/2554-0.txt"</span></div><div class="line">response = request.urlopen(url)</div><div class="line">raw = response.read().decode(<span class="string">'utf-8'</span>)</div><div class="line">raw = raw[raw.find(<span class="string">'PART I'</span>):raw.rfind(<span class="string">"End of Project Gutenberg’s Crime"</span>)]   <span class="comment"># extract useful information</span></div><div class="line">tokens = word_tokenize(raw)</div><div class="line">text = nltk.Text(tokens)</div><div class="line"></div><div class="line"><span class="comment"># from HTML is almost the same, except that to get text out of HTML</span></div><div class="line"><span class="comment"># we have to use a Python library called BeautifulSoup, </span></div><div class="line"><span class="comment"># available from http://www.crummy.com/software/BeautifulSoup/</span></div><div class="line"><span class="comment"># since there're many markup symbols, e.g., &lt;&lt; &gt;&gt; |</span></div><div class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</div><div class="line">raw = BeautifulSoup(html).get_text()</div><div class="line">tokens = word_tokenize(raw)</div><div class="line"></div><div class="line"><span class="comment"># With the help of a Python library called the Universal Feed Parser available from </span></div><div class="line"><span class="comment"># https://pypi.python.org/pypi/feedparser, we can access the content of a blog</span></div><div class="line"><span class="keyword">import</span> feedparser</div><div class="line">llog = feedparser.parse(<span class="string">"http://languagelog.ldc.upenn.edu/nll/?feed=atom"</span>)</div></pre></td></tr></table></figure>
<p><strong>From local files</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">f = open(<span class="string">'document.txt'</span>)</div><div class="line">f.read()                                    <span class="comment"># read the contents of the entire file</span></div><div class="line"></div><div class="line">f = open(<span class="string">'document.txt'</span>, <span class="string">'rU'</span>)              <span class="comment"># after read() we have to reopen the file</span></div><div class="line">                                            <span class="comment"># this time 'r' means to open the file for reading(the default)</span></div><div class="line">                                            <span class="comment"># and 'U' stands for 'Universal', which lets us ignore the </span></div><div class="line">                                            <span class="comment"># different conventions used for marking newlines</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:                              <span class="comment"># read line by line</span></div><div class="line">    print(line.strip())</div></pre></td></tr></table></figure>
<p>ASCII text and HTML text are human readable formats. Text often comes in binary formats — like PDF and MSWord — that can only be opened using specialized software. Third-party libraries such as <code>pypdf</code> and <code>pywin32</code> provide access to these formats. Extracting text from multi-column documents is particularly challenging. For once-off conversion of a few documents, it is simpler to open the document with a suitable application, then save it as text to your local drive, and access it as described below. If the document is already on the web, you can enter its URL in Google’s search box. The search result often includes a link to an HTML version of the document, which you can save as text.</p>
<p><img src="/images/linguistic/pipeline.png" alt=""></p>
<h3 id="Regular-Expression"><a href="#Regular-Expression" class="headerlink" title="Regular Expression"></a>Regular Expression</h3><p>I’ve uploaded my summary of regular expression in the post <a href="http://haelchan.me/2018/05/23/regular-expression/">Regular Expression</a>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> re</div><div class="line">wordlist = [w <span class="keyword">for</span> w <span class="keyword">in</span> nltk.corpus.words.words(<span class="string">'en'</span>) <span class="keyword">if</span> w.islower()]</div><div class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> wordlist <span class="keyword">if</span> re.search(<span class="string">'ed$'</span>, w)]</div></pre></td></tr></table></figure>
<p>NLTK provides a regular expression tokenizer: <code>nltk.regexp_tokenize()</code>.</p>
<h2 id="Structured-Program"><a href="#Structured-Program" class="headerlink" title="Structured Program"></a>Structured Program</h2><p>The basic part is discussed in my <a href="http://haelchan.me/2018/02/07/python-learning/">Python learning</a> note. And in this section, I just record some unfamiliar knowledge.</p>
<p>Assignment always copies the value of an expression, but a value is not always what you might expect it to be. In particular, the “value” of a structured object such as a list is actually just a <em>reference</em> to the object.<br>Python provides two ways to check that a pair of items are the same. The <code>is</code> operator tests for object identity.</p>
<p>A list is typically a sequence of objects all having the same type, of arbitrary length. We often use lists to hold sequences of words. In contrast, a tuple is typically a collection of objects of <em>different types</em>, of <em>fixed length</em>. We often use a tuple to hold a <strong>record</strong>, a collection of different <strong>fields</strong> relating to some entity.</p>
<p><strong>Generator Expression:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>max([w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(text)]) [<span class="number">1</span>]</div><div class="line"><span class="string">'word'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>max(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(text)) [<span class="number">2</span>]</div><div class="line"><span class="string">'word'</span></div></pre></td></tr></table></figure>
<p>The second line uses a <strong>generator expression</strong>. This is more than a notational convenience: in many language processing situations, generator expressions will be more efficient. In 1, storage for the list object must be allocated before the value of <code>max()</code> is computed. If the text is very large, this could be slow. In 2, the data is streamed to the calling function. Since the calling function simply has to find the maximum value - the word which comes latest in lexicographic sort order - it can process the stream of data without having to store anything more than the maximum value seen so far.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">search1</span><span class="params">(substring, words)</span>:</span></div><div class="line">    result = []</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</div><div class="line">        <span class="keyword">if</span> substring <span class="keyword">in</span> word:</div><div class="line">            result.append(word)</div><div class="line">    <span class="keyword">return</span> result</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">search2</span><span class="params">(substring, words)</span>:</span></div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</div><div class="line">        <span class="keyword">if</span> substring <span class="keyword">in</span> word:</div><div class="line">            <span class="keyword">yield</span> word</div></pre></td></tr></table></figure>
<p><strong>Function</strong><br>It is not necessary to have any parameters.<br>A function usually communicates its results back to the calling program via the <code>return</code> statement.<br>A Python function is not required to have a return statement. Some functions do their work as a side effect, printing a result, modifying a file, or updating the contents of a parameter to the function (such functions are called “procedures” in some other programming languages).</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">my_sort1</span><span class="params">(mylist)</span>:</span>      <span class="comment"># good: modifies its argument, no return value</span></div><div class="line"><span class="meta">... </span>    mylist.sort()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">my_sort2</span><span class="params">(mylist)</span>:</span>      <span class="comment"># good: doesn't touch its argument, returns value</span></div><div class="line"><span class="meta">... </span>    <span class="keyword">return</span> sorted(mylist)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">my_sort3</span><span class="params">(mylist)</span>:</span>      <span class="comment"># bad: modifies its argument and also returns it</span></div><div class="line"><span class="meta">... </span>    mylist.sort()</div><div class="line"><span class="meta">... </span>    <span class="keyword">return</span> mylist</div></pre></td></tr></table></figure>
<p>When you refer to an existing name from within the body of a function, the Python interpreter first tries to resolve the name with respect to the names that are local to the function. If nothing is found, the interpreter checks if it is a global name within the module. Finally, if that does not succeed, the interpreter checks if the name is a Python built-in. This is the so-called <strong>LGB rule</strong> of name resolution: local, then global, then built-in.</p>
<p>Python also lets us pass a function as an argument to another function. Now we can abstract out the operation, and apply a <em>different operation</em> on the <em>same data</em>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>sent = [<span class="string">'Take'</span>, <span class="string">'care'</span>, <span class="string">'of'</span>, <span class="string">'the'</span>, <span class="string">'sense'</span>, <span class="string">','</span>, <span class="string">'and'</span>, <span class="string">'the'</span>,</div><div class="line"><span class="meta">... </span>        <span class="string">'sounds'</span>, <span class="string">'will'</span>, <span class="string">'take'</span>, <span class="string">'care'</span>, <span class="string">'of'</span>, <span class="string">'themselves'</span>, <span class="string">'.'</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">extract_property</span><span class="params">(prop)</span>:</span></div><div class="line"><span class="meta">... </span>    <span class="keyword">return</span> [prop(word) <span class="keyword">for</span> word <span class="keyword">in</span> sent]</div><div class="line">...</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>extract_property(len)</div><div class="line">[<span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">1</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">last_letter</span><span class="params">(word)</span>:</span></div><div class="line"><span class="meta">... </span>    <span class="keyword">return</span> word[<span class="number">-1</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>extract_property(last_letter)</div><div class="line">[<span class="string">'e'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'e'</span>, <span class="string">'e'</span>, <span class="string">','</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'s'</span>, <span class="string">'l'</span>, <span class="string">'e'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'s'</span>, <span class="string">'.'</span>]</div></pre></td></tr></table></figure>
<p>Python’s default arguments are evaluated once when the function is defined, not each time the function is called (like it is in say, Ruby). This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well.</p>
<p><strong>Space-Time Tradeoffs</strong><br>We can test the effeciency using the <code>timeit</code> module. The <code>Timer</code> class has two parameters, a statement which is executed multiple times, and setup code that is executed once at the beginning.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> timeit <span class="keyword">import</span> Timer</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>vocab_size = <span class="number">100000</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>setup_list = <span class="string">"import random; vocab = range(%d)"</span> % vocab_size [<span class="number">1</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>setup_set = <span class="string">"import random; vocab = set(range(%d))"</span> % vocab_size [<span class="number">2</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>statement = <span class="string">"random.randint(0, %d) in vocab"</span> % (vocab_size * <span class="number">2</span>) [<span class="number">3</span>]</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(Timer(statement, setup_list).timeit(<span class="number">1000</span>))</div><div class="line"><span class="number">2.78092288971</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(Timer(statement, setup_set).timeit(<span class="number">1000</span>))</div><div class="line"><span class="number">0.0037260055542</span></div></pre></td></tr></table></figure>
<p><strong>Dynamic Programming</strong><br>Dynamic programming is a general technique for designing algorithms which is widely used in natural language processing. Dynamic programming is used when a problem contains overlapping sub-problems. Instead of computing solutions to these sub-problems repeatedly, we simply store them in a lookup table.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka1</span><span class="params">(n)</span>:</span></div><div class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</div><div class="line">        <span class="keyword">return</span> [<span class="string">""</span>]</div><div class="line">    <span class="keyword">elif</span> n == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> [<span class="string">"S"</span>]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka1(n<span class="number">-1</span>)]</div><div class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka1(n<span class="number">-2</span>)]</div><div class="line">        <span class="keyword">return</span> s + l</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka2</span><span class="params">(n)</span>:</span></div><div class="line">    lookup = [[<span class="string">""</span>], [<span class="string">"S"</span>]]</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</div><div class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> lookup[i+<span class="number">1</span>]]</div><div class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> lookup[i]]</div><div class="line">        lookup.append(s + l)</div><div class="line">    <span class="keyword">return</span> lookup[n]</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka3</span><span class="params">(n, lookup=&#123;<span class="number">0</span>:[<span class="string">""</span>], <span class="number">1</span>:[<span class="string">"S"</span>]&#125;)</span>:</span></div><div class="line">    <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> lookup:</div><div class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka3(n<span class="number">-1</span>)]</div><div class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka3(n<span class="number">-2</span>)]</div><div class="line">        lookup[n] = s + l</div><div class="line">    <span class="keyword">return</span> lookup[n]</div><div class="line"></div><div class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> memoize</div><div class="line"><span class="meta">@memoize</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka4</span><span class="params">(n)</span>:</span></div><div class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</div><div class="line">        <span class="keyword">return</span> [<span class="string">""</span>]</div><div class="line">    <span class="keyword">elif</span> n == <span class="number">1</span>:</div><div class="line">        <span class="keyword">return</span> [<span class="string">"S"</span>]</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka4(n<span class="number">-1</span>)]</div><div class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka4(n<span class="number">-2</span>)]</div><div class="line">        <span class="keyword">return</span> s + l</div></pre></td></tr></table></figure>
<p>Four Ways to Compute Sanskrit Meter: (i) recursive; (ii) bottom-up dynamic programming; (iii) top-down dynamic programming; and (iv) built-in memoization.</p>
<h2 id="Categorizing-and-Tagging-Words"><a href="#Categorizing-and-Tagging-Words" class="headerlink" title="Categorizing and Tagging Words"></a>Categorizing and Tagging Words</h2><p>Universal Part-of-Speech Tagset<br>| Tag | Meaning | English Examples |<br>| —- | —- | —- |<br>| ADJ | adjective | new, good |<br>| ADP | adposition | on, of |<br>| ADV | adverb | really, already |<br>| CONJ | conjunction | and, or |<br>| DET | determiner, article | the, some |<br>| NOUN | noun | year, home |<br>| NUM | numeral | twenty-four, fourth |<br>| PRT | particle | at, on |<br>| PRON | pronoun | he, their |<br>| VERB | verb | is, say |<br>| . | punctuation marks | .,;! |<br>| X | other | gr8, univeristy |</p>
<p>Some related functions:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># nltk.pos_tag()</span></div><div class="line">text = word_tokenize(<span class="string">"They refuse to permit us to obtain the refuse permit"</span>)</div><div class="line">nltk.pos_tag(text)</div><div class="line"></div><div class="line"><span class="comment"># nltk.tag.str2tuple()</span></div><div class="line">tagged_token = nltk.tag.str2tuple(<span class="string">'fly/NN'</span>)</div><div class="line"></div><div class="line"><span class="comment"># nltk.corpus.tagged_words()</span></div><div class="line">nltk.corpus.brown.tagged_words()</div><div class="line">nltk.corpus.brown.tagged_words(tagset=<span class="string">'universal'</span>)</div></pre></td></tr></table></figure>
<h3 id="Automatic-Tagging"><a href="#Automatic-Tagging" class="headerlink" title="Automatic Tagging"></a>Automatic Tagging</h3><p>The Default Tagger<br>Default taggers assign their tag to every single word, even words that have never been encountered before.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">default_tagger = nltk.DefaultTagger(<span class="string">'NN'</span>)</div><div class="line">default_tagger.tag(tokens)</div></pre></td></tr></table></figure>
<p>The Regular Expression Tagger<br>The regular expression tagger assigns tags to tokens on the basis of matching patterns.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">patterns = [</div><div class="line">    (<span class="string">r'.*s$'</span>, <span class="string">'NNS'</span>)</div><div class="line">    (<span class="string">r'.*ing$'</span>, <span class="string">'VBG'</span>)</div><div class="line">]</div></pre></td></tr></table></figure>
<p>The Lookup Tagger<br>Find the most frequent words and store their most likely tag. Then use this information as the model for a “lookup tagger”(an NLTK <code>UnigramTagger</code>).<br>For those words not among the most frequent words, it’s okay to assign the default tag of <code>NN</code>. In other words, we want to use the lookup table first, and if it is unable to assign a tag, then use the default tagger, a process known as backoff (5). </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">fd = nltk.FreqDist(brown.words(categories=<span class="string">'news'</span>))</div><div class="line">cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories=<span class="string">'news'</span>))</div><div class="line">most_freq_words = fd.most_common(<span class="number">100</span>)</div><div class="line">likely_tags = dict((word, cfd[word].max()) <span class="keyword">for</span> (word, _) <span class="keyword">in</span> most_freq_words)</div><div class="line">baseline_tagger = nltk.UnigramTagger(model=likely_tags,</div><div class="line">                                     backoff=nltk.DefaultTagger(<span class="string">'NN'</span>))</div></pre></td></tr></table></figure>
<h3 id="N-Gram-Tagging"><a href="#N-Gram-Tagging" class="headerlink" title="N-Gram Tagging"></a>N-Gram Tagging</h3><p>Unigram Tagging<br>Unigram taggers are based on a simple statistical algorithm: for each token, assign the tag that is most likely for that particular token.<br>An n-gram tagger is a generalization of a unigram tagger whose context is the current word together with the part-of-speech tags of the <em>n-1</em> preceding tokens.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">unigram_tagger = nltk.UnigramTagger(train_sents)</div><div class="line">bigram_tagger = nltk.BigramTagger(train_sents)</div></pre></td></tr></table></figure>
<p>Note:<br>n-gram taggers should not consider context that crosses a sentence boundary. Accordingly, NLTK taggers are designed to work with lists of sentences, where each sentence is a list of words. At the start of a sentence, t_{n-1} and preceding tags are set to <code>None</code>.</p>
<p>As <em>n</em> gets larger, the specificity of the contexts increases, as does the chance that the data we wish to tag contains contexts that were not present in the training data. This is known as the <em>sparse data</em> problem, and is quite pervasive in NLP. As a consequence, there is a trade-off between the accuracy and the coverage of our results (and this is related to the precision/recall trade-off in information retrieval).<br>One way to address the trade-off between accuracy and coverage is to use the more accurate algorithms when we can, but to fall back on algorithms with wider coverage when necessary. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 1. Try tagging the token with the bigram tagger.</span></div><div class="line"><span class="comment"># 2. If the bigram tagger is unable to find a tag for the token, try the unigram tagger.</span></div><div class="line"><span class="comment"># 3. If the unigram tagger is also unable to find a tag, use a default tagger.</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>t0 = nltk.DefaultTagger(<span class="string">'NN'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>t1 = nltk.UnigramTagger(train_sents, backoff=t0)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>t2 = nltk.BigramTagger(train_sents, backoff=t1)</div><div class="line"></div><div class="line"><span class="comment"># We can further specify that a tagger needs to see more than one instance of a context in order to retain it.</span></div><div class="line">nltk.BigramTagger(sents, cutoff=<span class="number">2</span>, backoff=t1)</div><div class="line"><span class="comment"># it will discard contexts that have only been seen once or twice.</span></div></pre></td></tr></table></figure>
<h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>Classification is the task of choosing the correct class label for a given input. In basic classification tasks, each input is considered in isolation from all other inputs, and the set of labels is defined in advance.<br>E.g., deciding whether an email is spam or not; deciding what the topic of a news article is, from a fixed list of topic areas such as “sports,” “technology,” and “politics”; deciding whether a given occurrence of the word bank is used to refer to a river bank, a financial institution, the act of tilting to the side, or the act of depositing something in a financial institution.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># The example of gender identification</span></div><div class="line"></div><div class="line"><span class="comment"># the function of extracting possible features</span></div><div class="line"><span class="comment"># e.g., the name's last/first letter, the length of the name,</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gender_features</span><span class="params">(word)</span>:</span></div><div class="line">    <span class="keyword">return</span> &#123;<span class="string">'last_letter'</span>: word[<span class="number">-1</span>]&#125;</div><div class="line">    <span class="comment"># return &#123;'first_letter': word[0]&#125;</span></div><div class="line">    <span class="comment"># return &#123;'word_length': len(word)&#125;</span></div><div class="line"></div><div class="line"><span class="comment"># get the name data</span></div><div class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> names</div><div class="line">labeled_names = ([(name, <span class="string">'male'</span>) <span class="keyword">for</span> name <span class="keyword">in</span> names.words(<span class="string">'male.txt'</span>)] + </div><div class="line">                 [(name, <span class="string">'female'</span>) <span class="keyword">for</span> name <span class="keyword">in</span> names.words(<span class="string">'female.txt'</span>)])</div><div class="line"><span class="keyword">import</span> random</div><div class="line">random.shuffle(labeled_names)</div><div class="line"></div><div class="line"><span class="comment"># seperate the training set and test set</span></div><div class="line">featuresets = [(gender_features(n), gender) <span class="keyword">for</span> (n, gender) <span class="keyword">in</span> labeled_names]</div><div class="line"><span class="comment"># train_set, test_set = featuresets[500:], featuresets[:500]</span></div><div class="line"><span class="comment"># nltk.classify.apply_features returns an object that acts like a list but does not store all the feature sets in memory</span></div><div class="line"><span class="keyword">from</span> nltk.classify <span class="keyword">import</span> apply_features</div><div class="line">train_set = apply_features(gender_features, labeled_names[<span class="number">500</span>:])</div><div class="line">test_set = apply_features(gender_features, labeled_names[:<span class="number">500</span>])</div><div class="line"></div><div class="line"><span class="comment"># use Naive Bayes classifier and see the accuracy</span></div><div class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</div><div class="line">nltk.classify.accuracy(classifier, test_set)</div><div class="line"></div><div class="line"><span class="comment"># Accuracy:</span></div><div class="line"><span class="comment"># last letter: 0.774</span></div><div class="line"><span class="comment"># first letter: 0.622</span></div><div class="line"><span class="comment"># name length: 0.618</span></div></pre></td></tr></table></figure>
<p>The training set is used to train the model, and the dev-test set is used to perform error analysis. The test set serves in our final evaluation of the system. Just as common machine learning does.</p>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Hael Chan
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://haelchan.me/2018/07/23/nlp-with-python-and-nltk/" title="Natural Language Processing with Python and NLTK">http://haelchan.me/2018/07/23/nlp-with-python-and-nltk/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/learning-note/" rel="tag"># learning note</a>
          
            <a href="/tags/natural-language-processing/" rel="tag"># natural language processing</a>
          
            <a href="/tags/Python/" rel="tag"># Python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/07/17/reading-note-of-the-study-of-language-2e/" rel="next" title="Overview of Linguistics - Reading Notes of The Study of Language(Second edition)">
                <i class="fa fa-chevron-left"></i> Overview of Linguistics - Reading Notes of The Study of Language(Second edition)
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>
  


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Hael Chan" />
            
              <p class="site-author-name" itemprop="name">Hael Chan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">22</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">19</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/haelchan" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:f.procumbens@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://twitter.com/Procumbens" target="_blank" title="Twitter">
                    
                      <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/hael-c/activities" target="_blank" title="知乎">
                    
                      <i class="fa fa-fw fa-compass"></i>知乎</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-globe"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://learnerwn.github.io" title="WN_Blog" target="_blank">WN_Blog</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://hey-yahei.cn" title="YaHei" target="_blank">YaHei</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://ruder.io" title="Sebastian Ruder" target="_blank">Sebastian Ruder</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://godweiyang.com" title="WeiYang Blog" target="_blank">WeiYang Blog</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.linzehui.me" title="Weekly Review" target="_blank">Weekly Review</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Texts-and-words"><span class="nav-number">1.</span> <span class="nav-text">Texts and words</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Searching"><span class="nav-number">1.1.</span> <span class="nav-text">Searching</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Text-Corpora-and-Lexical-Resources"><span class="nav-number">2.</span> <span class="nav-text">Text Corpora and Lexical Resources</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Accessing-Text-Corpora"><span class="nav-number">2.1.</span> <span class="nav-text">Accessing Text Corpora</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Lexical-Resources"><span class="nav-number">2.2.</span> <span class="nav-text">Lexical Resources</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#WordNet"><span class="nav-number">2.3.</span> <span class="nav-text">WordNet</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Processing-Raw-Text"><span class="nav-number">3.</span> <span class="nav-text">Processing Raw Text</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Accessing-Text"><span class="nav-number">3.1.</span> <span class="nav-text">Accessing Text</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regular-Expression"><span class="nav-number">3.2.</span> <span class="nav-text">Regular Expression</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Structured-Program"><span class="nav-number">4.</span> <span class="nav-text">Structured Program</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Categorizing-and-Tagging-Words"><span class="nav-number">5.</span> <span class="nav-text">Categorizing and Tagging Words</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Automatic-Tagging"><span class="nav-number">5.1.</span> <span class="nav-text">Automatic Tagging</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#N-Gram-Tagging"><span class="nav-number">5.2.</span> <span class="nav-text">N-Gram Tagging</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Classification"><span class="nav-number">6.</span> <span class="nav-text">Classification</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hael Chan</span>

  
</div>









<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<span id="busuanzi_container_site_uv">
  欢迎~您是本站的第<span id="busuanzi_value_site_uv"></span>位访客
</span>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://hael.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://haelchan.me/2018/07/23/nlp-with-python-and-nltk/';
          this.page.identifier = '2018/07/23/nlp-with-python-and-nltk/';
          this.page.title = 'Natural Language Processing with Python and NLTK';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://hael.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'manual') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("3HhWLTewaKTSPj5DC3qp5b2m-gzGzoHsz", "zjxN2hpHQEmyMaz0XBicI3bn");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  


  

  

</body>
</html>
