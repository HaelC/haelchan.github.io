<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="learning note," />





  <link rel="alternate" href="/atom.xml" title="Hael's Blog" type="application/atom+xml" />






<meta name="description" content="Week OneIntroductionWhat is natural language processing?computers using natural language as input and/or outputNLP contains two types: understanding(NLU) and generation(NLG). ApplicationsMachine Trans">
<meta name="keywords" content="learning note">
<meta property="og:type" content="article">
<meta property="og:title" content="COMS W4705 Natural Language Processing Note">
<meta property="og:url" content="http://haelchan.me/2018/03/31/NLP-note/index.html">
<meta property="og:site_name" content="Hael&#39;s Blog">
<meta property="og:description" content="Week OneIntroductionWhat is natural language processing?computers using natural language as input and/or outputNLP contains two types: understanding(NLU) and generation(NLG). ApplicationsMachine Trans">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://haelchan.me/images/NLP/overview.jpg">
<meta property="og:image" content="http://haelchan.me/images/NLP/parsing.jpg">
<meta property="og:image" content="http://haelchan.me/images/NLP/syntactic.jpg">
<meta property="og:image" content="http://haelchan.me/images/NLP/trigramDiscount.jpg">
<meta property="og:image" content="http://haelchan.me/images/NLP/lowFrequency.jpg">
<meta property="og:image" content="http://haelchan.me/images/NLP/parsingSyntactic.jpg">
<meta property="og:image" content="http://haelchan.me/images/NLP/parseInfo1.jpg">
<meta property="og:image" content="http://haelchan.me/images/NLP/parseInfo2.jpg">
<meta property="og:image" content="http://haelchan.me/images/NLP/parseInfo3.jpg">
<meta property="og:updated_time" content="2018-10-18T15:06:35.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="COMS W4705 Natural Language Processing Note">
<meta name="twitter:description" content="Week OneIntroductionWhat is natural language processing?computers using natural language as input and/or outputNLP contains two types: understanding(NLU) and generation(NLG). ApplicationsMachine Trans">
<meta name="twitter:image" content="http://haelchan.me/images/NLP/overview.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://haelchan.me/2018/03/31/NLP-note/"/>





  <title>COMS W4705 Natural Language Processing Note | Hael's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hael's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            Search
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="Searching..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://haelchan.me/2018/03/31/NLP-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hael Chan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hael's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">COMS W4705 Natural Language Processing Note</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2018-03-31T23:45:27+08:00">
                2018-03-31
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">Post modified&#58;</span>
              
              <time title="Post modified" itemprop="dateModified" datetime="2018-10-18T23:06:35+08:00">
                2018-10-18
              </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/03/31/NLP-note/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/03/31/NLP-note/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/03/31/NLP-note/" class="leancloud_visitors" data-flag-title="COMS W4705 Natural Language Processing Note">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">Visitors&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Week-One"><a href="#Week-One" class="headerlink" title="Week One"></a>Week One</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>What is natural language processing?<br>computers using natural language as input and/or output<br>NLP contains two types: understanding(NLU) and generation(NLG).<br><img src="/images/NLP/overview.jpg" alt=""></p>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><p><strong>Machine Translation</strong><br>Translate from one language to another.<br><strong>Information Extraction</strong><br>Take some text as input, and produce structured(database) representation of some key content in the text.<br>Goal: Map a document collection to structured database<br>Motivation: Complex searches, Statistical queries.</p>
<p><strong>Text Summarization</strong><br>Take a single document, or potentially a group of several documents, and try to condense them down to summarize main information in those documents.</p>
<p><strong>Dialogue Systems</strong><br>Human can interact with computer.</p>
<h3 id="Basic-NLP-Problems"><a href="#Basic-NLP-Problems" class="headerlink" title="Basic NLP Problems"></a>Basic NLP Problems</h3><h4 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h4><p>Strings to Tagged Sequences<br>Examples:<br><strong>Part-of-speech tagging</strong><br>Profits(/N) soared(/V) at(/P) Boeing(/N) Co.(/N) .(/.) easily(/ADV) topping(/V) forecasts(/N) on (/P) Wall(/N) Street(/N) .(/.)</p>
<p><strong>Name Entity Recognition</strong><br>Profits(/NA) soared(/NA) at(/NA) Boeing(/SC) Co.(/CC) .(/NA) easily(/NA) topping(/NA) forecasts(/NA) on (/NA) Wall(/SL) Street(/CL) .(/.)<br>/NA: not any entity<br>/SC: start of company<br>/CC: continuation of company<br>/SL: start of location<br>/CL: continuation of location</p>
<h4 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h4><p><img src="/images/NLP/parsing.jpg" alt=""></p>
<h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><blockquote>
<p>At last, a computer that understands you like your mother.</p>
</blockquote>
<h4 id="Ambiguity"><a href="#Ambiguity" class="headerlink" title="Ambiguity"></a>Ambiguity</h4><p>This sentence can be interpreted in different ways:<br>1.It understands you as well as your mother understands you.<br>2.It understands (that) you like your mother.<br>3.It understands you as well as it understands your mother.</p>
<p>Ambiguity at Many Levels:<br><strong>Acoustic</strong> level<br>In speech recognition:<br>1.”… a computer that understands <em>you like</em> your mother”<br>2.”… a computer that understands <em>lie cured</em> mother”</p>
<p><strong>Syntactic</strong> level<br><img src="/images/NLP/syntactic.jpg" alt=""></p>
<p><strong>Semantic</strong> level<br>A word may have a variety of meanings and it may cause word sense ambiguity.</p>
<p><strong>Discourse</strong> (multi-clause) level<br>Alice says they’ve built a computer that understands you like your mother<br>But she…<br>  … doesn’t know any details<br>  … doesn’t understand me at all<br>This is an instance of <strong>anaphora</strong>, where <em>she</em> referees to some other discourse entity.</p>
<h2 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h2><p>· We have some (finite) vocabulary, say <script type="math/tex">\mathcal{V}={\text{the, a, man, telescope, Beckham, two, ...}}</script></p>
<p>· We have an (infinite) set of strings, <script type="math/tex">\mathcal{V}^\dagger</script>:</p>
<blockquote>
<p>the STOP<br>a STOP<br>the fan STOP<br>the fan saw Beckham STOP<br>the fan saw saw STOP<br>the fan saw Beckham play for Real Madrid STOP</p>
</blockquote>
<p>· We have a <em>training sample</em> of example sentences in English<br>· We need to “learn” a probability distribution <em>p</em> i.e., <em>p</em> is a function that satisfies</p>
<script type="math/tex; mode=display">\sum_{x\in\mathcal{V^\dagger}}p(x)=1,\ p(x)\ge0\text{ for all }x\in\mathcal{V^\dagger}</script><p>Definition: A language model consists of a finite set <script type="math/tex">\mathcal{V}</script>, and a function <script type="math/tex">p(x_1,x_2,...,x_n)</script> such that:<br>1.For any <script type="math/tex"><x_1...x_n>\in\mathcal{V^\dagger}</script>, <script type="math/tex">p(x_1,x_2,...,x_n)\ge 0</script><br>2.In addition, <script type="math/tex">\sum_{<x_1...x_n>\in\mathcal{V^\dagger}}p(x_1,x_2,...,x_n)=1</script></p>
<p>Language models are very useful in a broad range of applications, the most obvious perhaps being speech recognition and machine translation. In many applications it is very useful to have a good “prior” distribution <script type="math/tex">p(x_1...x_n)</script> over which sentences are or aren’t probable in a language. For example, in speech recognition the language model is combined with an acoustic model that models the pronunciation of different words: one way to think about it is that the acoustic model generates a large number of candidate sentences, together with probabilities; the language model is then used to reorder these possibilities based on how likely they are to be a sentence in the language.<br>The techniques we describe for defining the function <em>p</em>, and for estimating the parameters of the resulting model from training examples, will be useful in several other contexts during the course: for example in hidden Markov models and in models for natural language parsing.</p>
<h3 id="A-Naive-Method"><a href="#A-Naive-Method" class="headerlink" title="A Naive Method"></a>A Naive Method</h3><p>We have N training sentences.<br>For any sentence <script type="math/tex">x_1...x_n,\ c(x_1...x_n)</script> is the number of times the sentence is seen in our training data.</p>
<script type="math/tex; mode=display">p(x_1...x_n)=\frac{c(x_1...x_n)}{N}</script><p>This is a poor model: in particular it will assign probability 0 to any sentence not seen in the training corpus. Thus it fails to generalize to sentences that have not seen in the training data. The key technical contribution of this chapter will be to introduce methods that do generalize to sentences that are not seen in our training data.</p>
<h3 id="Markov-Models"><a href="#Markov-Models" class="headerlink" title="Markov Models"></a>Markov Models</h3><p><strong>First-Order Markov Processes</strong></p>
<script type="math/tex; mode=display">\begin{aligned}&P(X_1=x_1,X_2=x_2,...,X_n=x_n)\\=&P(X_1=x_1)\prod_{i=2}^nP(X_i=x_i|X_1=x_1,...,X_{i-1}=x_{i-1})\\=&P(X_1=x_1)\prod_{i=2}^nP(X_i=x_i|X_{i-1}=x_{i-1})\end{aligned}</script><p>The first step is exact: by the chain rule of probabilities, <em>any</em> distribution <script type="math/tex">P(X_1=x_1...X_n=x_n)</script> can be written in this form. So we have made no assumptions in this step of the derivation. However, the second step is not necessarily exact: we have made the assumption that for any <script type="math/tex">i\in\{2...n\}</script>, for any <script type="math/tex">x_1...x_i</script>,</p>
<script type="math/tex; mode=display">P(X_i=x_i|X_1=x_1...X_{i-1}=x_{i-1})=P(X_i=x_i|X_{i-1}=x_{i-1})</script><p>This is a first-order <em>Markov assumption</em>. We have assumed that the identity of the i’th word in the sequence depends only on the identity of the previous word, <script type="math/tex">x_{i-1}</script>. More formally, we have assumed that the value of <script type="math/tex">X_i</script> is conditionally independent of <script type="math/tex">X_1...X_{i-2}</script>, given the value of <script type="math/tex">X_{i-1}</script>.</p>
<p><strong>Second-Order Markov Processes</strong></p>
<script type="math/tex; mode=display">\begin{aligned}&P(X_1=x_1,X_2=x_2,...,X_n=x_n)\\=&P(X_1=x_1)
\times P(X_2=x_2|X_1=x_1)\prod_{i=3}^nP(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\\=&\prod_{i=1}^nP(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\end{aligned}</script><p>(For convenience we assume <script type="math/tex">x_0=x_{-1}=*</script>, where * is a special “start” symbol.)</p>
<p>Compared with first-order Markov process, we make a slightly weaker assumption, namely that each word depends on the previous <em>two</em> words in the sequence. And the second-order Markov process will form the basis of trigram language models.</p>
<p>The length of sequence <em>n</em> can itself vary. We just assume that the <em>n</em>‘th word in the sequence, <script type="math/tex">X_n</script> is always equal to a special symbol, the STOP symbol. This symbol can only appear at the end of a sequence and it doesn’t belong to the set <script type="math/tex">\mathcal{V}</script>.</p>
<p>Process:</p>
<ol>
<li>Initialize <em>i</em> = 1, and <script type="math/tex">x_0=x_{-1}=*</script></li>
<li>Generate <script type="math/tex">x_i</script> from the distribution <script type="math/tex">P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})</script></li>
<li>If <script type="math/tex">x_i=STOP</script> then return the sequence <script type="math/tex">x_1...x_n</script>. Otherwise, set <script type="math/tex">i=i+1</script> and return to step 2.</li>
</ol>
<h3 id="Trigram"><a href="#Trigram" class="headerlink" title="Trigram"></a>Trigram</h3><p>A trigram language model consists of a finite set <script type="math/tex">\mathcal{V}</script>, and a parameter <script type="math/tex">q(w|u,v)</script> for each trigram u,v,w such that <script type="math/tex">w\in\mathcal{V}\cup\{STOP\}</script>, and <script type="math/tex">u,v\in\mathcal{V}\cup\{*\}</script>. The value for <script type="math/tex">q(w|u,v)</script> can be interpreted as the probability of seeing the word <em>w</em> immediately after the bigram <em>(u,v)</em>. For any sentence <script type="math/tex">x_1...x_n</script> where <script type="math/tex">x_i\in\mathcal{V}</script> for <em>i</em> = 1…(n-1), and <script type="math/tex">x_n=STOP</script>, the probability of the sentence under the trigram language model is <script type="math/tex">p(x_1...x_n)=\prod_{i=1}^nq(x_i|x_{i-2},x_{i-1})</script>, where we define <script type="math/tex">x_0=x_{-1}=*</script>.</p>
<p>Estimation Problem:<br>A natural estimate (the “maximum likelihood estimate”):</p>
<script type="math/tex; mode=display">q(w_i|w_{i-2},w_{i-1})=\frac{Count(w_{i-2},w_{i-1},w_i)}{Count(w_{i-2},w_{i-1})}</script><p>For example:</p>
<script type="math/tex; mode=display">q(\text{barks|the, dog})=\frac{c(\text{the, dog, barks})}{c(\text{the, dog})}</script><p>This way of estimating parameters runs into a very serious issue. Say our vocabulary size is <script type="math/tex">N=|\mathcal{V}|</script>, then there are <script type="math/tex">N^3</script> parameters in the model. This leads to two problems:<br>· Many of the above estimates will be <script type="math/tex">q(w|u,v)=0</script>, due to the count in the numerator being 0. This will lead to many trigram probabilities being systematically underestimated: it seems unreasonable to assign probability 0 to any trigram not seen in training data, given that the number of parameters of the model is typically very large in comparison to the number of words in the training corpus.<br>· In cases where the denominator <script type="math/tex">c(u,v)</script> is equal to zero, the estimate is not well defined.</p>
<p><strong>Perplexity</strong><br>We have some test data sentences <script type="math/tex">x^{(1)},x^{(2)},...,x^{(m)}</script>. Note that test sentences are “held out”, in the sense that they are not part of the corpus used to estimate the language model. (Just the same as normal machine learning problems.)<br>We could look at the probability under our model <script type="math/tex">\prod_{i=1}^mp(s_i)</script>.<br>More conveniently, the log probability:</p>
<script type="math/tex; mode=display">\log\prod_{i=1}^mp(s_i)=\sum_{i=1}^m\log p(s_i)</script><p>In fact the usual evaluation measure is :</p>
<script type="math/tex; mode=display">\text{Perplexity}=2^{-l}, \text{where }l=\frac{1}{M}\sum_{i=1}^m\log p(s_i)</script><p>and M is the total number of words in the test data <script type="math/tex">M=\sum_{i=1}^mn_i</script>. <script type="math/tex">n_i</script> is the length of the <em>i</em>‘th test sentence.</p>
<h3 id="Linear-Interpolation"><a href="#Linear-Interpolation" class="headerlink" title="Linear Interpolation"></a>Linear Interpolation</h3><script type="math/tex; mode=display">q_{ML}(w|u,v)=\frac{c(u,v,w)}{c(u,v)}</script><script type="math/tex; mode=display">q_{ML}(w|v)=\frac{c(v,w)}{c(v)}</script><script type="math/tex; mode=display">q_{ML}(w)=\frac{c(w)}{c()}</script><p>The subscript <em>ML</em> means <em>maximum-likelihood</em> estimation. And <script type="math/tex">c(w)</script> is the number of times word <em>w</em> is seen in the training corpus, and <script type="math/tex">c()</script> is the total number of words seen in the training corpus.<br>The trigram, bigram and unigram estimates have different strengths and weaknesses. The idea in linear interpolation is to use all three estimates, by taking a weighted average of the three estimates:</p>
<script type="math/tex; mode=display">q(w|u,v)=\lambda_1\times q_{ML}(w|u,v)+\lambda_2\times q_{ML}(w|v)+\lambda_3\times q_{ML}(w)</script><p>Here <script type="math/tex">\lambda_1</script>, <script type="math/tex">\lambda_2</script> and <script type="math/tex">\lambda_3</script> are three additional parameters of the model, which satisfies <script type="math/tex">lambda_1+\lambda_2+\lambda_3=1</script> and <script type="math/tex">\lambda_i\ge0</script> for all <em>i</em>.<br>(Our estimate correctly defines a distribution.)</p>
<p>There are various ways of estimating the λ values. A common one is as follows. Say we have some additional held-out data(development data), which is separate from both our training and test corpora.<br>Define <script type="math/tex">c'(u,v,w)</script> to be the number of times that the trigram (u,v,w) is seen in the development data. </p>
<script type="math/tex; mode=display">L(\lambda_1,\lambda_2,\lambda_3)=\sum_{u,v,w}c'(u,v,w)\log q(w|u,v)\\=\sum_{u,v,w}c'(u,v,w)\log(lambda_1\times q_{ML}(w|u,v)+\lambda_2\times q_{ML}(w|v)+\lambda_3\times q_{ML}(w))</script><p>We would like to choose our λ values to maximize <script type="math/tex">L(\lambda_1,\lambda_2,\lambda_3)</script>.(It means minimize perplexity.)</p>
<p>The three parameters <script type="math/tex">\lambda_1,\lambda_2,\lambda_3</script> can be interpreted as an indication of the confidence, or weight, placed on each of the trigram, bigram and unigram estimates.</p>
<p>In practice, it is important to add an additional degree of freedom, by allowing the values for <script type="math/tex">\lambda_1,\lambda_2,\lambda_3</script> to vary.<br>Take a function <script type="math/tex">\Pi</script> that partitions histories<br>e.g.,</p>
<script type="math/tex; mode=display">\Pi(w_{i-2},w_{i-1})=\begin{cases}1\text{  If Count}(w_{i-1},w_{i-2})=0\\2\text{  If }1\le Count(w_{i-1},w_{i-2})\le2\\3\text{  If }3\le Count(w_{i-1},w_{i-2})\le5\\4\text{  Otherwise}\end{cases}</script><p>Introduce a dependence of the λ’s on the partition:</p>
<script type="math/tex; mode=display">\begin{aligned}q(w_i|w_{i-2},w_{i-1})=&\lambda_1^{\Pi(w_{i-2},w_{i-1})}\times q_{ML}(w_i|w_{i-2},w_{i-1})\\ +&\lambda_2^{\Pi(w_{i-2},w_{i-1})}\times q_{ML}(w_i|w_{i-1})\\ +&\lambda_3^{\Pi(w_{i-2},w_{i-1})}\times q_{ML}(w_i)\end{aligned}</script><p>where <script type="math/tex">\lambda_1^{\Pi(w_{i-2},w_{i-1})}+\lambda_2^{\Pi(w_{i-2},w_{i-1})}+\lambda_3^{\Pi(w_{i-2},w_{i-1})}=1</script>, and <script type="math/tex">\lambda_i^{\Pi(w_{i-2},w_{i-1})}\ge0</script> for all <em>i</em>.</p>
<p>By the way, <script type="math/tex">lambda_1</script> should be 0 if <script type="math/tex">c(u,v)=0</script>, because in this case the trigram estimate <script type="math/tex">q_{ML}(w|u,v)=\frac{c(u,v,w)}{c(u,v)}</script> is undefined. Similarly, if both <script type="math/tex">c(u,v)</script> and <script type="math/tex">c(v)</script> are equal to zero, we need <script type="math/tex">\lambda_1=\lambda_2=0</script></p>
<p>In the referencing note, a simple method is introduced:</p>
<script type="math/tex; mode=display">\lambda_1=\frac{c(u,v)}{c(u,v)+\gamma}</script><script type="math/tex; mode=display">\lambda_2=(1-\lambda_1)\times\frac{c(v)}{c(v)+\gamma}</script><script type="math/tex; mode=display">\lambda_3=1-\lambda_1-\lambda_2</script><p>γ&gt;0.<br>Under this definition, it can be seen that <script type="math/tex">\lambda_1</script> increases as <script type="math/tex">c(u,v)</script> increases, and similarly that <script type="math/tex">\lambda_2</script> increases as <script type="math/tex">c(v)</script> increases. This method is relatively crude, and is not likely to be optimal. It is, however, very simple, and in practice it can work well in some applications.</p>
<h3 id="Discounting-Methods"><a href="#Discounting-Methods" class="headerlink" title="Discounting Methods"></a>Discounting Methods</h3><p>For any bigram <script type="math/tex">Count(w_{i-1},w)</script> such that <script type="math/tex">Count(w_{i-1},w)>0</script>, we define the discounted count as</p>
<script type="math/tex; mode=display">Count^*(w_{i-1},w)=Count(w_{i-1},w)-\beta</script><p>where β is a value between 0 and 1 (a typical value might be β=0.5). Thus we simply subtract a constant value, β, from the count. This reflects the intuition that if we take counts from the training corpus, we will systematically over-estimate the probability of bigrams seen in the corpus(and under-estimate bigrams not seen in the corpus).<br>For any bigram <script type="math/tex">(w_{i-1},w)</script> such that <script type="math/tex">Count(w_{i-1},w)>0</script>, we can then define <script type="math/tex">q(w|w_{i-1})=\frac{Count^*(w_{i-1},w)}{Count(w_{i-1})}</script>. For any context <script type="math/tex">w_{i-1}</script>, this definition leads to some <em>missing probability mass</em>, defined as <script type="math/tex">\alpha(v)=1-\sum_{w:Count(w_{i-1},w)>0}\frac{Count^*(w_{i-1},w)}{Count(w_{i-1})}</script><br>The intuition behind discounted methods is to divide this “missing mass” between the words <em>w</em> such that <script type="math/tex">Count(w_{i-1},w)=0</script>.<br>Define two sets</p>
<script type="math/tex; mode=display">\mathcal{A}(w_{i-1})=\{w:Count(w_{i-1},w)>0</script><script type="math/tex; mode=display">\mathcal{B}(w_{i-1})=\{w:Count(w_{i-1},w)=0</script><p>Then the back-off model is defined as</p>
<script type="math/tex; mode=display">q_{BO}(w_i|w_{i-1})=\begin{cases}\frac{Count^*(w_{i-1},w_i)}{Count(w_{i-1})}\text{  if }w_i\in\mathcal{A}(w_{i-1})\\\alpha(w_{i-1})\frac{q_{ML}(w_i)}{\sum_{w\in\mathcal{B}(w_{i-1})}q_{ML}(w)}\text{  if }w_i\in\mathcal{B}(w_{i-1})\end{cases}</script><p>Thus if <script type="math/tex">Count(w_{i-1},w)>0</script> we return the estimate <script type="math/tex">\frac{Count^*(w_{i-1},w_i)}{Count(w_{i-1})}</script>; otherwise we divide the remaining probability mass <script type="math/tex">\alpha(w_{i-1})</script> in proportion to the unigram estimate <script type="math/tex">q_{ML}(w)</script>.<br>The method can be generalized to trigram language models in a natural, recursive way:<br><img src="/images/NLP/trigramDiscount.jpg" alt=""></p>
<h1 id="Week-Two"><a href="#Week-Two" class="headerlink" title="Week Two"></a>Week Two</h1><h2 id="Tagging-1"><a href="#Tagging-1" class="headerlink" title="Tagging"></a>Tagging</h2><p>Given the input to the tagging model(referred as a <em>sentence</em>) <script type="math/tex">x_1...x_n</script>, use <script type="math/tex">y_1...y_n</script> to denote the output of the tagging model(referred as the <em>state sequence</em> or <em>tag sequence</em>).<br>This type of problem, where the task is to map a sentence <script type="math/tex">x_1...x_n</script> to a tag sequence <script type="math/tex">y_1...y_n</script> is often referred to as a <strong>sequence labeling problem</strong>, or a <strong>tagging problem</strong>.<br>We will assume that we have a set of training examples, <script type="math/tex">(x^{(i)},y^{(i)})</script> for <script type="math/tex">i=1...m</script>, where each <script type="math/tex">x^{(i)}</script> is a sentence <script type="math/tex">x_1^{(i)}...x_{n_i}^{(i)}</script>, and each <script type="math/tex">y^{(i)}</script> is a tag sequence <script type="math/tex">y^{(i)}_1...y^{(i)}_{n_i}</script>(we assume that the <em>i</em>‘th training example is of length <script type="math/tex">n_i</script>). Hence <script type="math/tex">x_j^{(i)}</script> is the <em>j</em>‘th word in the <em>i</em>‘th training example, and <script type="math/tex">y_j^{(i)}</script> is the tag for that word. Our task is to learn a function that maps sentences to tag sequences from these training examples.</p>
<h3 id="POS-Tagging"><a href="#POS-Tagging" class="headerlink" title="POS Tagging"></a>POS Tagging</h3><p>POS Tagging: Part-of-Speech tagging.<br>The input to the problem is a sentence. The output is a tagged sentence, where each word in the sentence is annotated with its part of speech. Our goal will be to construct a model that recovers POS tags for sentences with high accuracy. POS tagging is one of the most basic problems in NLP, and is useful in many natural language applications.</p>
<h4 id="Tags"><a href="#Tags" class="headerlink" title="Tags"></a>Tags</h4><ul>
<li>D: determiner (a, an, the)</li>
<li>N: noun</li>
<li>V: verb</li>
<li>P: preposition</li>
<li>Adv: adverb</li>
<li>Adj: adjective</li>
<li>…</li>
</ul>
<h4 id="Challenges-1"><a href="#Challenges-1" class="headerlink" title="Challenges"></a>Challenges</h4><p><strong>Ambiguity</strong><br>Many words in English can take several possible parts of speech(as well as in Chinese and many other languages). A word can be a noun as well as a verb. E.g., <em>look</em>, <em>result</em>…<br><strong>Rare words</strong><br>Some words are rare and may not be seen in the training examples. Even with say a million words of training data, there will be many words in new sentences which have not been seen in training. It will be important to develop methods that deal effectively with words which have not been seen in training data.</p>
<h4 id="Sources-of-information"><a href="#Sources-of-information" class="headerlink" title="Sources of information"></a>Sources of information</h4><p><strong>Local</strong><br>Individual words have statistical preferences for their part of speech.<br>E.g., <em>can</em> is more likely to be a modal verb rather than a noun.</p>
<p><strong>Contextual</strong><br>The context has an important effect on the part of speech for a word. In particular, some sequences of POS tags are much more likely than others. If we consider POS trigrams, the sequence <code>D N V</code> will be frequent in English, whereas the sequence <code>D V N</code> is much less likely.<br>Sometimes these two sources of evidence are in conflict. For example, in the sentence <em>The trash can is hard to find</em>, the part of speech for <em>can</em> is a noun-however, <em>can</em> can also be a modal verb, and in fact it is much more frequently seen as a modal verb in English. In this sentence the context has overridden the tendency for <em>can</em> to be a verb as opposed to a noun.</p>
<h3 id="Named-Entity"><a href="#Named-Entity" class="headerlink" title="Named-Entity"></a>Named-Entity</h3><p>For named-entity problem, the input is again a sentence. The output is the sentence with entity-boundaries marked. Recognizing entities such as people, locations and organizations has many applications, and name-entity recognition has been widely studies in NLP research.</p>
<p>Once this mapping has been performed on training examples, we can train a tagging model on these training examples. Given a new test sentence we can then recover the sequence of tags from the model, and it is straightforward to identify the entities identified by the model.</p>
<h2 id="Generative-Models"><a href="#Generative-Models" class="headerlink" title="Generative Models"></a>Generative Models</h2><p><strong>Supervised Learning:</strong><br>Assume training examples <script type="math/tex">(x^{(1)},y^{(1)})...(x^{(m)},y^{(m)})</script>, where each example consists of an input <script type="math/tex">x^{(i)}</script> paired with a label <script type="math/tex">y^{(i)}</script>. We use <script type="math/tex">\mathcal{X}</script> to refer to the set of possible inputs, and <script type="math/tex">\mathcal{Y}</script> to refer to the set of possible labels. Our task is to learn a function <script type="math/tex">f:\mathcal{X}\to\mathcal{Y}</script> that maps any input x to a label f(x).<br>One way to define the function f(x) is through a <em>conditional model</em>. In this approach we define a model that defines the conditional probability <script type="math/tex">p(y|x)</script> for any x,y pair. The parameters of the model are estimated from the training examples. Given a new test example x, the output from the model is <script type="math/tex">f(x)=\arg\max_{y\in\mathcal{Y}}p(y|x)</script><br>Thus we simply take the most likely label <em>y</em> as the output from the model. If our model <script type="math/tex">p(y|x)</script> is close to the true conditional distribution of labels given inputs, the function f(x) will be close to optimal.</p>
<p><strong>Generative Models</strong><br>Rather than directly estimating the conditional distribution <script type="math/tex">p(y|x)</script>, in generative models we instead model the <em>joint</em> probability <script type="math/tex">p(x,y)</script> over <script type="math/tex">(x,y)</script> pairs. The parameters of the model <script type="math/tex">p(x,y)</script> are again estimated from the training examples <script type="math/tex">(x^{(i)},y^{(i)})</script> for <script type="math/tex">I=1...n</script>. In many cases we further decompose the probability <script type="math/tex">p(x,y)</script> as <script type="math/tex">p(x,y)=p(y)p(x|y)</script> and then estimate the models for <script type="math/tex">p(y)</script> and <script type="math/tex">p(x|y)</script> separately. These two model components have the following interpretations:<br>· <script type="math/tex">p(y)</script> is a <em>prior</em> probability distribution over labels <em>y</em>.<br>· <script type="math/tex">p(x|y)</script> is the probability of generating the input <em>x</em>, given that the underlying label is <em>y</em>.</p>
<p>Given a generative model, we can use Bayes rule to derive the condition probability <script type="math/tex">p(y|x)</script> for any <script type="math/tex">(x,y)</script> pair:</p>
<script type="math/tex; mode=display">p(y|x)=\frac{p(y)p(x|y)}{p(x)}</script><p>where</p>
<script type="math/tex; mode=display">p(x)=\sum_{y\in\mathcal{Y}}p(x,y)=\sum_{y\in\mathcal{Y}}p(y)p(x|y)</script><p>We use Bayes rule directly in applying the joint model to a new test example. Given an input <em>x</em>, the output of our model, f(x), can be derived as follows:</p>
<script type="math/tex; mode=display">\begin{aligned}f(x)&=\arg\max_yp(y|x)\\&=\arg\max_y\frac{p(y)p(x|y)}{p(x)}\text{  (2)}\\&=\arg\max_yp(y)p(x|y)\text{  (3)}\end{aligned}</script><p>Eq. 2 follows by Bayes rule. Eq. 3 follows because the denominator, <script type="math/tex">p(x)</script>, does not depend on <em>y</em>, and hence does not affect the arg max. This is convenient, because it mean that we do not need calculate <script type="math/tex">p(x)</script>, which can be an expensive operation.<br>Models that decompose a joint probability into terms <script type="math/tex">p(y)</script> and <script type="math/tex">p(x|y)</script> are often called <em>noisy-channel</em> models. Intuitively, when we see a test example <em>x</em>, we assume that has been generated in two steps: first, a label <em>y</em> has been chosen with probability <script type="math/tex">p(y)</script>; second, the example <em>x</em> has been generated from the distribution <script type="math/tex">p(x|y)</script>. The model <script type="math/tex">p(x|y)</script> can be interpreted as a “channel” which takes a label <em>y</em> as its input, and corrupts it to produce <em>x</em> as its output. Our task is to find the most likely label <em>y</em>, given that we observe <em>x</em>.</p>
<p>In summary:</p>
<ul>
<li>Our task is to learn a function from inputs <em>x</em> to labels <script type="math/tex">y=f(x)</script>. We assume training examples <script type="math/tex">(x^{(i)},y^{(i)})</script> for <script type="math/tex">i=1...n</script>.</li>
<li>In the noisy channel approach, we use the training example to estimate models <script type="math/tex">p(y)</script> and <script type="math/tex">p(x|y)</script>. These models define a joint(generative) model: <script type="math/tex">p(x,y)=p(y)p(x|y)</script></li>
<li>Given a new test example <em>x</em>, we predict the label <script type="math/tex">f(x)=\arg\max_{y\in\mathcal{Y}}p(y)p(x|y)</script>. Finding the output <em>f(x)</em> for an input <em>x</em> is often referred to as the <em>decoding problem</em>.</li>
</ul>
<p>Assume a finite set of words <script type="math/tex">\mathcal{V}</script>, and a finite set of tags <script type="math/tex">\mathcal{K}</script>. Define <script type="math/tex">\mathcal{S}</script> to be the set of all sequence/tag-sequence pairs <script type="math/tex"><x_1...x_n,y_1...y_n></script> such that <script type="math/tex">n\ge0,x_i\in\mathcal{V}for\ i=1...n,and\ y_i\in\mathcal{K}for\ i=1...n</script>. A generative tagging model is then a function p such that:</p>
<ol>
<li>For any <script type="math/tex"><x_1...x_n,y_1...y_n>\in\mathcal{S},p(x_1...x_n,y_1...y_n)\ge0</script></li>
<li>In addition, <script type="math/tex">\sum_{<x_1...x_n,y_1...y_n>\in\mathcal{S}}p(x_1...x_n,y_1...y_n)=1</script></li>
</ol>
<p>Hense <script type="math/tex">p(x_1...x_n,y_1...y_n)</script> is a probability distribution over pairs of sequences(i.e., a probability distribution over the set <script type="math/tex">\mathcal{S}</script>).<br>Given a generative tagging model, the function from sentences <script type="math/tex">x_1...x_n</script> to tag sequence <script type="math/tex">y_1...y_n</script> is defined as</p>
<script type="math/tex; mode=display">f(x_1...x_n)=\arg\max_{y_1...y_n}p(x_1...x_n,y_1...y_n)</script><p>where the <em>arg max</em> is taken over all sequences <script type="math/tex">y_1...y_n</script> such that <script type="math/tex">y_i\in\mathcal{K}for\ i\in\{1...n\}</script>. Thus for any input <script type="math/tex">x_1...x_n</script>, we take the highest probability tag sequence as the output from the model.</p>
<h2 id="Hidden-Markov-Models"><a href="#Hidden-Markov-Models" class="headerlink" title="Hidden Markov Models"></a>Hidden Markov Models</h2><h3 id="Trigram-HMMs"><a href="#Trigram-HMMs" class="headerlink" title="Trigram HMMs"></a>Trigram HMMs</h3><p>A trigram HMM consists of a finite set <script type="math/tex">\mathcal{V}</script> of possible words, and a finite set <script type="math/tex">\mathcal{K}</script> of possible tags, together with the following parameters:<br>· A parameter <script type="math/tex">q(s|u,v)</script> for any trigram <script type="math/tex">(u,v,s)</script> such that <script type="math/tex">s\in\mathcal{K]\cup\{STOP\}</script>, and <script type="math/tex">u,v\in\mathcal{K}\cup\{*\}</script>. The value for <script type="math/tex">q(s|u,v)</script> can be interpreted as the probability of seeing the tag <em>s</em> immediately after the bigram of tags <script type="math/tex">(u,v)</script>.<br>· A parameter <script type="math/tex">e(x|s)</script> for any <script type="math/tex">x\in\mathcal{V},s\in\mathcal{K}</script>. The value for <script type="math/tex">e(x|s)</script> can be interpreted as the probability of seeing observation <em>x</em> paired with state <em>s</em>.<br>Define <script type="math/tex">\mathcal{S}</script> to be the set of all sequence/tag-sequence pairs <script type="math/tex"><x_1...x_n,y_1...y_{n+1}></script> such that <script type="math/tex">n\ge0,x_i\in\mathcal{V}for\ i=1...n,y_i\in\mathcal{K}for\ i=1...n,and\ y_{n+1}=STOP</script>.<br>We then define the probability for any <script type="math/tex"><x_1...x_n,y_1...y_{n+1}\in\mathcal{S}</script> as</p>
<script type="math/tex; mode=display">p(x_1...x_n,y_1...y_{n+1})=\prod_{i=1}^{n+1}q(y_i|y_{i-2},y_{i-1})\prod_{i=1}^ne(x_i|y_i)</script><p>where we have assumed that <script type="math/tex">y_0=y_{-1}=*</script>.</p>
<p>E.g.<br>If we have n = 3, <script type="math/tex">x_1x_2x_3</script> equal to the sentence <em>the dog laughs</em>, and <script type="math/tex">y_1y_2y_3y_4</script> equal to the tag sequence <code>D N V STOP</code>, then</p>
<script type="math/tex; mode=display">p(x_1x_2x_3y_1y_2y_3y_4)=q(D|*,*)\times q(N|*,D)\times q(V|D,N)\times q(STOP|N,V)\\\times e(the|D)\times e(dog|N)\times e(laughs|V)</script><p>The quantity <script type="math/tex">q(D|*,*)\times q(N|*,D)\times q(V|D,N)\times q(STOP|N,V)</script> is the prior probability of seeing the tag sequence <code>D N V STOP</code>, where we have used a second-order Markov model(a trigram model).<br>The quantity <script type="math/tex">e(the|D)\times e(dog|N)\times e(laughs|V)</script> can be interpreted as the conditional probability <script type="math/tex">p(the\ dog\ laughs|D\ N\ V\ STOP)</script>: that is, the conditional probability <script type="math/tex">p(x|y)</script> where <em>x</em> is the sentence <em>the dog laughs</em>, and <em>y</em> is the sequence <code>D N V STOP</code>.</p>
<p>Consider a pair of sentences of random variables <script type="math/tex">X_1...X_n</script> and <script type="math/tex">Y_1...Y_n</script>, where <em>n</em> is the length of the sequences. To model the joint probability</p>
<script type="math/tex; mode=display">P(X_1=x_1...X_n=x_n,Y_1=y_1...Y_n=y_n</script><p>for any observation sequence <script type="math/tex">x_1...x_n</script> paired with a state sequence <script type="math/tex">y_1...y_n</script>, where each <script type="math/tex">x_i</script> is a member of <script type="math/tex">\mathcal{V}</script>, and each <script type="math/tex">y_i</script> is a member of <script type="math/tex">\mathcal{K}</script>.<br>Define one additional variable <script type="math/tex">Y_{n+1}</script> which always takes the value STOP, just as we did in variable-Markov sequences.<br>The key idea in hidden Markov models is the following definition:</p>
<script type="math/tex; mode=display">P(X_1=x_1...X_n=x_n,Y_1=y_1...Y_{n+1}=y_{n+1})=\prod_{i=1}^{n+1}P(Y_i=y_i|Y_{i-2}=y_{i-2},Y_{i-1}=y_{i-1})\prod_{i=1}^nP(X_i=x_i|Y_i=y_i)</script><p>We have assumed that for any <em>i</em>, for any values of <script type="math/tex">y_{i-2},y_{i-1},y_i</script>,</p>
<script type="math/tex; mode=display">P(Y_i=y_i|Y_{i-2}=y_{i-2},Y_{i-1}=y_{i-1})=q(y_i|y_{i-2},y_{i-1})</script><p>and that for any value of <em>i</em>, for any values of <script type="math/tex">x_i</script> and <script type="math/tex">y_i</script>,</p>
<script type="math/tex; mode=display">P(X_i=x_i|Y_i=y_i)=e(x_i|y_i)</script><p><strong>The derivation of hidden Markov models:</strong></p>
<script type="math/tex; mode=display">P(X_1=x_1...X_n=x_n,Y_1=y_1...Y_{n+1}=y_{n+1})=P(Y_1=y_1...Y_{n+1}=y_{n+1})\times P(X_1=x_1...X_n=x_n|Y_1=y_1...Y_{n+1}=y_{n+1})</script><p>Just by the chain rule of probabilities. The joint probability is decomposed into two terms: first, the probability of choosing tag sequence <script type="math/tex">y_1...y_{n+1}</script>; second, the probability of choosing the word sequence <script type="math/tex">x_1...x_n</script>, conditioned on the choice of tag sequence.</p>
<script type="math/tex; mode=display">P(Y_1=y_1...Y_{n+1}=y_{n+1})=\prod_{i=1}^{n+1}P(Y_i=y_i|Y_{i-2}=y_{i-2},Y_{i-1}=y_{i-1})</script><p>Consider the first item: Assume the sequence is a second-order Markov sequence.</p>
<script type="math/tex; mode=display">P(X_1=x_1...X_n=x_n|Y_1=y_1...Y_{n+1}=y_{n+1})=\prod_{i=1}^nP(X_i=x_i|X_1=x_1...X_{i-1}=x_{i-1},Y_1=y_1...Y_{n+1})=y_{n+1}=\prod_{i=1}^nP(X_i=x_i|Y_i=y_i)</script><p>Consider the second item: Assume that the value for the random variable <script type="math/tex">X_i</script> depends only on the value <script type="math/tex">Y_i</script>.</p>
<p><strong>Stochastic process</strong></p>
<ol>
<li>Initialize <script type="math/tex">i=1</script> and <script type="math/tex">y_0=y_{-1}=*</script>.</li>
<li>Generate <script type="math/tex">y_i</script> from the distribution <script type="math/tex">q(y_i|y_{i-1},y_{i-1})</script></li>
<li>If <script type="math/tex">y_i=STOP</script> then return <script type="math/tex">y_1...,x_1...x_{i-1}</script>. Otherwise, generate <script type="math/tex">x_i</script> from the distribution <script type="math/tex">e(x_i|y_i)</script>, set <script type="math/tex">i=i+1</script>, and return to step 2.</li>
</ol>
<h3 id="Estimating"><a href="#Estimating" class="headerlink" title="Estimating"></a>Estimating</h3><p>Define <script type="math/tex">c(u,v,s)</script> to be the number of times the sequence of three states <script type="math/tex">(u,v,s)</script> is seen in training data. Similarly, define <script type="math/tex">c(u,v)</script> to be the number of times the tag bigram<script type="math/tex">(u,v)</script> is seen and <script type="math/tex">c(s)</script> to be the number of times that the state <script type="math/tex">s</script> is seen in the corpus.<br>Define <script type="math/tex">c(s\leadsto x)</script> to be the number of times state <script type="math/tex">s</script> is seen paired with observation <script type="math/tex">x</script> in the corpus.<br>The <em>maximum-likelihood</em> estimates are</p>
<script type="math/tex; mode=display">q(s|u,v)=\frac{c(u,v,s)}{c(u,v)}</script><script type="math/tex; mode=display">e(x|s)=\frac{c(s\leadsto x)}{c(s)}</script><p>In some cases it is useful to smooth estimates of <script type="math/tex">q(s|u,v)</script>, using the techniques of smoothing:</p>
<script type="math/tex; mode=display">q(s|u,v)=\lambda_1\times q_{ML}(s|u,v)+\lambda_2\times q_{ML}(s|v)+\lambda_3\times q_ML(s)</script><h3 id="Dealing-with-Low-Frequency-Words"><a href="#Dealing-with-Low-Frequency-Words" class="headerlink" title="Dealing with Low-Frequency Words"></a>Dealing with Low-Frequency Words</h3><p>A common method is as follows:</p>
<p>· Step 1: Split vocabulary into two sets<br><strong>Frequent words</strong> : words occurring ≥ 5 times in training<br><strong>Low-frequency words</strong> : all other words<br>· Step 2: Map low frequency words into a small, finite set, depending on prefixes, suffixes etc.</p>
<p><img src="/images/NLP/lowFrequency.jpg" alt=""></p>
<h3 id="The-Viterbi-Algorithm"><a href="#The-Viterbi-Algorithm" class="headerlink" title="The Viterbi Algorithm"></a>The Viterbi Algorithm</h3><p>Problem: for an input <script type="math/tex">x_1...x_n</script>,  find <script type="math/tex">\arg\max_{y_1...y_{n+1}}p(x_1...x_n,y_1...y_{n+1})</script> where the arg max is taken over all sequences <script type="math/tex">y_1...y_{n+1}</script> such that <script type="math/tex">y_i\in\mathcal{K}</script> for <script type="math/tex">i=1...n</script>, and <script type="math/tex">y_{n+1}=STOP</script>.<br>We assume that <em>p</em> again takes the form <script type="math/tex">p(x_1...x_n,y_1...y_{n+1})=\prod_{i=1}^{n+1}q(y_i|y_{i-2},y_{i-1})\prod_{i=1}^ne(x_i|y_i)</script></p>
<p>The naive brute force method would be hopelessly inefficient. Instead, we can efficiently find the highest probability tag sequence using a dynamic programming algorithm that is often called <em>the Viterbi algorithm</em>. The input to the algorithm is a sentence <script type="math/tex">x_1...x_n</script>.</p>
<h1 id="Week-Three"><a href="#Week-Three" class="headerlink" title="Week Three"></a>Week Three</h1><h2 id="Parsing-1"><a href="#Parsing-1" class="headerlink" title="Parsing"></a>Parsing</h2><p>Input: a sentence<br>Output: a parse tree<br><img src="/images/NLP/parsingSyntactic.jpg" alt=""></p>
<h3 id="Parse-tree"><a href="#Parse-tree" class="headerlink" title="Parse tree"></a>Parse tree</h3><p>A <em>parse tree</em> is a tree structure with the words in the sentence at the leaves of the tree, and the tree has labels on the internal nodes.</p>
<p>Syntactic Formalisms: minimalism, lexical functional grammar(LFG), head-driven phrase-structure grammar(HPSG), tree adjoining grammars(TAG), categorial grammars, etc.<br>The lecture focuses on context-free grammars, which are fundamental and form the basis for all modern form atoms.</p>
<p>Data: Penn WSJ Treebank: 50000 sentences with associated trees(annotated by hand).</p>
<h4 id="The-Information-Conveyed-by-Parse-Tree"><a href="#The-Information-Conveyed-by-Parse-Tree" class="headerlink" title="The Information Conveyed by Parse Tree"></a>The Information Conveyed by Parse Tree</h4><h5 id="Part-of-speech-for-each-word"><a href="#Part-of-speech-for-each-word" class="headerlink" title="Part of speech for each word"></a>Part of speech for each word</h5><p>It plays the same role as POS tagging. For each word in the sentence, just put a tag for the word.<br>N = noun, V = verb, DT = determiner<br><img src="/images/NLP/parseInfo1.jpg" alt=""></p>
<h5 id="Phrases"><a href="#Phrases" class="headerlink" title="Phrases"></a>Phrases</h5><p>Phrases, or what are often called constituents, are compositions of words.<br>NP = noun phrase, VP = verb phrase<br><img src="/images/NLP/parseInfo2.jpg" alt=""></p>
<h5 id="Useful-Relationships"><a href="#Useful-Relationships" class="headerlink" title="Useful Relationships"></a>Useful Relationships</h5><p>Parse trees encode important grammatical relationships within a sentence.<br>Some templates: <em>subject+verb</em>, <em>verb+DIRECT OBJECT</em>.<br><img src="/images/NLP/parseInfo3.jpg" alt=""></p>
<h2 id="Context-Free-Grammar"><a href="#Context-Free-Grammar" class="headerlink" title="Context-Free Grammar"></a>Context-Free Grammar</h2><p>A <strong>context free grammar</strong> <script type="math/tex">G=(N,\Sigma,R,S)</script> where:</p>
<ul>
<li><script type="math/tex">N</script> is a set of non-terminal symbols</li>
<li><script type="math/tex">\Sigma</script> is a set of terminal symbols (the set of words in the dictionary)</li>
<li><script type="math/tex; mode=display">R$$ is a set of rules of the form $$X\toY_1Y_2...Y_n$$ for $$n\ge0,X\inN,Y_i\in(N\cup\Sigma)</script></li>
<li><script type="math/tex">S\inN</script> is a distinguished start symbol</li>
</ul>
<p><strong>Properties</strong></p>
<ul>
<li>A CFG defines a set of possible derivations</li>
<li>A string <script type="math/tex">s\in\Sigma^*</script> is in the <em>language</em> defined by the CFG if there is at least one derivation that yields <em>s</em></li>
<li>Each string in the language generated by the CFG may have more than one derivation(“ambiguity”)</li>
</ul>
<h3 id="Left-Most-Derivations"><a href="#Left-Most-Derivations" class="headerlink" title="Left-Most Derivations"></a>Left-Most Derivations</h3><p>A left-most derivation is a sequence of strings <script type="math/tex">s_1...s_n</script>, where </p>
<ul>
<li><script type="math/tex">s_1=S</script>, the start symbol</li>
<li><script type="math/tex">s_n\in\Sigma^*</script>, i.e. <script type="math/tex">s_n</script> is made up of terminal symbols only (<script type="math/tex">\Sigma^*</script> denotes the set of all possible strings made up of sequences of words taken from <script type="math/tex">\Sigma</script>)</li>
<li>Each <script type="math/tex">s_i</script> for <script type="math/tex">i=2...n</script> is derived from <script type="math/tex">s_{i-1}</script> by picking the left-most non-terminal <em>X</em> in <script type="math/tex">s_{i-1}</script> and replacing it by some <script type="math/tex">\beta</script> where <script type="math/tex">X\to\beta</script> is a rule in <em>R</em></li>
</ul>
<p>E.g. [S]→[NP VP]→[D N VP]→[the N VP]→[the man VP]→[the man Vi]→[the man sleeps]</p>
<h2 id="A-brief-sketch-of-the-syntax-of-English"><a href="#A-brief-sketch-of-the-syntax-of-English" class="headerlink" title="A brief sketch of the syntax of English"></a>A brief sketch of the syntax of English</h2><h3 id="Tags-1"><a href="#Tags-1" class="headerlink" title="Tags"></a>Tags</h3><p>· Nouns<br>NN: singular noun (e.g., man, dog, park)<br>NNS: plural noun (e.g., books, pens)<br>NNP: proper noun (e.g., Bob, IBM)<br>NP: noun phrase (e.g., the girl)</p>
<p>· Determiners<br>DT: determiner (e.g., the, a, some, every)</p>
<p>· Adjectives<br>JJ: adjective (e.g., good, quick, big)</p>
<p>· Prepositions<br>IN: preposition (e.g., of, in, out, beside, as)<br>PP: prepositional phrase</p>
<p>· Basic Verb Types<br>Vi: intransitive verb (e.g., sleep, walk)<br>Vt: transitive verb (e.g., like, see)<br>Vd: ditransitive verb (e.g., give)<br>VP: verb phrase (e.g., sleep in the car, go to school)</p>
<p>· New Verb Types<br>V[5]: clause directly followed by the verb (e.g., say, report)<br>V[6]: clause followed by the verb with one objective (e.g., tell, inform)<br>V[7]: clause followed by the verb with two objectives (e.g., bet)</p>
<p>· Complementizers<br>COMP: complementizer (e.g., that)</p>
<p>· Coordinators<br>CC: coordinator (e.g., and, or, but)</p>
<p>· Sentences<br>S: sentence (e.g., the dog sleeps)</p>
<h3 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h3><blockquote>
<p>N(bar) =&gt; NN<br>N(bar) =&gt; NN N(bar)<br>N(bar) =&gt; JJ N(bar)<br>N(bar) =&gt; N(bar) N(bar)<br>NP =&gt; DT N(bar)<br>PP =&gt; IN NP<br>N(bar) =&gt; N(bar) PP<br>VP =&gt; Vi<br>VP =&gt; Vt NP<br>VP =&gt; Vd NP NP<br>VP =&gt; VP PP<br>S =&gt; NP VP<br>SBAR =&gt; COMP S<br>VP =&gt; V[5] SBAR<br>VP =&gt; V[6] NP SBAR<br>VP =&gt; V[7] NP NP SBAR<br>NP =&gt; NP CC NP<br>N(bar) =&gt; N(bar) CC N(bar)<br>VP =&gt; VP CC VP<br>S =&gt; S CC S<br>SBAR =&gt; SBAR CC SBAR</p>
</blockquote>
<p>What is discussed in the lecture is only a small part of the syntax of English.<br>There’re some problems:<br><strong>Agreement</strong><br><em>The dogs laugh</em> vs. <em>The dog laughs</em><br><strong>Wh-movement</strong><br><em>The dog [that the cat] liked…</em><br><strong>Active vs. passive</strong><br><em>The dog saw the cat</em> vs. <em>The cat was seen by the dog</em></p>
<h1 id="Week-Four"><a href="#Week-Four" class="headerlink" title="Week Four"></a>Week Four</h1><h2 id="Probabilistic-Context-free-Grammar"><a href="#Probabilistic-Context-free-Grammar" class="headerlink" title="Probabilistic Context-free Grammar"></a>Probabilistic Context-free Grammar</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>A probabilistic context-free grammar consists of:</p>
<ul>
<li>A context-free grammar <script type="math/tex">G=(N,\Sigma,S,R)</script>.</li>
<li>A parameter <script type="math/tex">q(\alpha\to\beta)</script> for each rule <script type="math/tex">\alpha\to\beta\in R</script>. The parameter <script type="math/tex">q(\alpha\to\beta)</script> can be interpreted as the conditional probability of choosing rule <script type="math/tex">\alpha\to\beta</script> in a left-most derivation, given that the non-terminal being expanded is α.</li>
</ul>
<p>Constraint: </p>
<script type="math/tex; mode=display">\sum_{\alpha\to\beta\in R:\alpha=X}q(\alpha\to\beta)=1,X\in N</script><script type="math/tex; mode=display">q(\alpha\to\beta)\ge0,\alpha\to\beta\in R</script><p>Given a parse-tree <script type="math/tex">t\in\mathcal{T}_G</script> containing rules <script type="math/tex">\alpha_1\to\beta_1,\alpha_2\to\beta_2,...,\alpha_n\to\beta_n</script>, the probability of t under the PCFG is </p>
<script type="math/tex; mode=display">p(t)=\prod_{i=1}^nq(\alpha_i\to\beta_i)</script><h3 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h3><p>Assigns a probability to each left-most derivation, or parse-tree, allowed by the underlying CFG<br>Say we have a sentence <em>s</em>, set of derivations for that sentence is <script type="math/tex">\mathcal{T}(s)</script>. Then a PCFG assigns a probability <script type="math/tex">p(t)</script> to each member of <script type="math/tex">\mathcal{T}(s)</script>. i.e., we now have a ranking in order of probability.<br>The most likely parse tree for a sentence <em>s</em> is </p>
<script type="math/tex; mode=display">\arg\max_{t\in\mathcal{T}(s)}p(t)</script>
      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>Post author:</strong>
    Hael Chan
  </li>
  <li class="post-copyright-link">
    <strong>Post link:</strong>
    <a href="http://haelchan.me/2018/03/31/NLP-note/" title="COMS W4705 Natural Language Processing Note">http://haelchan.me/2018/03/31/NLP-note/</a>
  </li>
  <li class="post-copyright-license">
    <strong>Copyright Notice: </strong>
    All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/learning-note/" rel="tag"># learning note</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/14/oop-note/" rel="next" title="面向对象程序设计课堂笔记">
                <i class="fa fa-chevron-left"></i> 面向对象程序设计课堂笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/08/composition-model/" rel="prev" title="Learning note of two papers about composition model">
                Learning note of two papers about composition model <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>
  


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Hael Chan" />
            
              <p class="site-author-name" itemprop="name">Hael Chan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">8</span>
                  <span class="site-state-item-name">categories</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/haelchan" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:f.procumbens@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://twitter.com/Procumbens" target="_blank" title="Twitter">
                    
                      <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/hael-c/activities" target="_blank" title="知乎">
                    
                      <i class="fa fa-fw fa-compass"></i>知乎</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-globe"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://ruder.io" title="Sebastian Ruder" target="_blank">Sebastian Ruder</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://smerity.com" title="Semerity" target="_blank">Semerity</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://einstein.ai/research/blog" title="Salesforce Research" target="_blank">Salesforce Research</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://www.reddit.com/r/LanguageTechnology/" title="Reddit" target="_blank">Reddit</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://learnerwn.github.io" title="WN_Blog" target="_blank">WN_Blog</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://hey-yahei.cn" title="YaHei" target="_blank">YaHei</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://godweiyang.com" title="WeiYang Blog" target="_blank">WeiYang Blog</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.linzehui.me" title="Weekly Review" target="_blank">Weekly Review</a>
                  </li>
                
                  <li class="links-of-blogroll-item">
                    <a href="https://jalammar.github.io" title="Jay Alammar" target="_blank">Jay Alammar</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-One"><span class="nav-number">1.</span> <span class="nav-text">Week One</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Applications"><span class="nav-number">1.1.1.</span> <span class="nav-text">Applications</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Basic-NLP-Problems"><span class="nav-number">1.1.2.</span> <span class="nav-text">Basic NLP Problems</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tagging"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Tagging</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Parsing"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">Parsing</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Challenges"><span class="nav-number">1.1.3.</span> <span class="nav-text">Challenges</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Ambiguity"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">Ambiguity</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Language-Modeling"><span class="nav-number">1.2.</span> <span class="nav-text">Language Modeling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Naive-Method"><span class="nav-number">1.2.1.</span> <span class="nav-text">A Naive Method</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Markov-Models"><span class="nav-number">1.2.2.</span> <span class="nav-text">Markov Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Trigram"><span class="nav-number">1.2.3.</span> <span class="nav-text">Trigram</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Interpolation"><span class="nav-number">1.2.4.</span> <span class="nav-text">Linear Interpolation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discounting-Methods"><span class="nav-number">1.2.5.</span> <span class="nav-text">Discounting Methods</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-Two"><span class="nav-number">2.</span> <span class="nav-text">Week Two</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tagging-1"><span class="nav-number">2.1.</span> <span class="nav-text">Tagging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#POS-Tagging"><span class="nav-number">2.1.1.</span> <span class="nav-text">POS Tagging</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tags"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">Tags</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Challenges-1"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">Challenges</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sources-of-information"><span class="nav-number">2.1.1.3.</span> <span class="nav-text">Sources of information</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Named-Entity"><span class="nav-number">2.1.2.</span> <span class="nav-text">Named-Entity</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generative-Models"><span class="nav-number">2.2.</span> <span class="nav-text">Generative Models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Hidden-Markov-Models"><span class="nav-number">2.3.</span> <span class="nav-text">Hidden Markov Models</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Trigram-HMMs"><span class="nav-number">2.3.1.</span> <span class="nav-text">Trigram HMMs</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Estimating"><span class="nav-number">2.3.2.</span> <span class="nav-text">Estimating</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dealing-with-Low-Frequency-Words"><span class="nav-number">2.3.3.</span> <span class="nav-text">Dealing with Low-Frequency Words</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#The-Viterbi-Algorithm"><span class="nav-number">2.3.4.</span> <span class="nav-text">The Viterbi Algorithm</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-Three"><span class="nav-number">3.</span> <span class="nav-text">Week Three</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Parsing-1"><span class="nav-number">3.1.</span> <span class="nav-text">Parsing</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Parse-tree"><span class="nav-number">3.1.1.</span> <span class="nav-text">Parse tree</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#The-Information-Conveyed-by-Parse-Tree"><span class="nav-number">3.1.1.1.</span> <span class="nav-text">The Information Conveyed by Parse Tree</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Part-of-speech-for-each-word"><span class="nav-number">3.1.1.1.1.</span> <span class="nav-text">Part of speech for each word</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Phrases"><span class="nav-number">3.1.1.1.2.</span> <span class="nav-text">Phrases</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Useful-Relationships"><span class="nav-number">3.1.1.1.3.</span> <span class="nav-text">Useful Relationships</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Context-Free-Grammar"><span class="nav-number">3.2.</span> <span class="nav-text">Context-Free Grammar</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Left-Most-Derivations"><span class="nav-number">3.2.1.</span> <span class="nav-text">Left-Most Derivations</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#A-brief-sketch-of-the-syntax-of-English"><span class="nav-number">3.3.</span> <span class="nav-text">A brief sketch of the syntax of English</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tags-1"><span class="nav-number">3.3.1.</span> <span class="nav-text">Tags</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rules"><span class="nav-number">3.3.2.</span> <span class="nav-text">Rules</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-Four"><span class="nav-number">4.</span> <span class="nav-text">Week Four</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Probabilistic-Context-free-Grammar"><span class="nav-number">4.1.</span> <span class="nav-text">Probabilistic Context-free Grammar</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Definition"><span class="nav-number">4.1.1.</span> <span class="nav-text">Definition</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Properties"><span class="nav-number">4.1.2.</span> <span class="nav-text">Properties</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hael Chan</span>

  
</div>









<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<span id="busuanzi_container_site_uv">
  Welcome~ Thanks for visiting my blog. You are the №.<span id="busuanzi_value_site_uv"></span> visitor.
</span>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://hael.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://haelchan.me/2018/03/31/NLP-note/';
          this.page.identifier = '2018/03/31/NLP-note/';
          this.page.title = 'COMS W4705 Natural Language Processing Note';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://hael.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'manual') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("3HhWLTewaKTSPj5DC3qp5b2m-gzGzoHsz", "zjxN2hpHQEmyMaz0XBicI3bn");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
