<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png?v=5.1.3">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png?v=5.1.3">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.3" color="#222">





  <meta name="keywords" content="natural language processing,learning note," />










<meta name="description" content="Week OneIntroductionWhat is natural language processing?computers using natural language as input and/or outputNLP contains two types: understanding(NLU) and generation(NLG). ApplicationsMachine Trans">
<meta name="keywords" content="natural language processing,learning note">
<meta property="og:type" content="article">
<meta property="og:title" content="Natural Language Processing Note">
<meta property="og:url" content="http://haelchan.me/2018/03/31/NLP-note/index.html">
<meta property="og:site_name" content="Hael&#39;s Blog">
<meta property="og:description" content="Week OneIntroductionWhat is natural language processing?computers using natural language as input and/or outputNLP contains two types: understanding(NLU) and generation(NLG). ApplicationsMachine Trans">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://haelchan.me/images/NLP/overview.jpg">
<meta property="og:image" content="http://haelchan.me/images/NLP/parsing.jpg">
<meta property="og:image" content="http://haelchan.me/images/NLP/syntactic.jpg">
<meta property="og:image" content="http://haelchan.me/images/NLP/trigramDiscount.jpg">
<meta property="og:updated_time" content="2018-04-11T06:59:46.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Natural Language Processing Note">
<meta name="twitter:description" content="Week OneIntroductionWhat is natural language processing?computers using natural language as input and/or outputNLP contains two types: understanding(NLU) and generation(NLG). ApplicationsMachine Trans">
<meta name="twitter:image" content="http://haelchan.me/images/NLP/overview.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://haelchan.me/2018/03/31/NLP-note/"/>





  <title>Natural Language Processing Note | Hael's Blog</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Hael's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <h1 class="site-subtitle" itemprop="description"></h1>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br />
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off"
             placeholder="搜索..." spellcheck="false"
             type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://haelchan.me/2018/03/31/NLP-note/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Hael Chan">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hael's Blog">
    </span>

    
      <header class="post-header">

        
        
          <h2 class="post-title" itemprop="name headline">Natural Language Processing Note</h2>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-03-31T23:45:27+08:00">
                2018-03-31
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-04-11T14:59:46+08:00">
                2018-04-11
              </time>
            
          </span>

          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/03/31/NLP-note/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2018/03/31/NLP-note/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
             <span id="/2018/03/31/NLP-note/" class="leancloud_visitors" data-flag-title="Natural Language Processing Note">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Week-One"><a href="#Week-One" class="headerlink" title="Week One"></a>Week One</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>What is natural language processing?<br>computers using natural language as input and/or output<br>NLP contains two types: understanding(NLU) and generation(NLG).<br><img src="/images/NLP/overview.jpg" alt=""></p>
<h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><p><strong>Machine Translation</strong><br>Translate from one language to another.<br><strong>Information Extraction</strong><br>Take some text as input, and produce structured(database) representation of some key content in the text.<br>Goal: Map a document collection to structured database<br>Motivation: Complex searches, Statistical queries.</p>
<p><strong>Text Summarization</strong><br>Take a single document, or potentially a group of several documents, and try to condense them down to summarize main information in those documents.</p>
<p><strong>Dialogue Systems</strong><br>Human can interact with computer.</p>
<h3 id="Basic-NLP-Problems"><a href="#Basic-NLP-Problems" class="headerlink" title="Basic NLP Problems"></a>Basic NLP Problems</h3><h4 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h4><p>Strings to Tagged Sequences<br>Examples:<br><strong>Part-of-speech tagging</strong><br>Profits(/N) soared(/V) at(/P) Boeing(/N) Co.(/N) .(/.) easily(/ADV) topping(/V) forecasts(/N) on (/P) Wall(/N) Street(/N) .(/.)</p>
<p><strong>Name Entity Recognition</strong><br>Profits(/NA) soared(/NA) at(/NA) Boeing(/SC) Co.(/CC) .(/NA) easily(/NA) topping(/NA) forecasts(/NA) on (/NA) Wall(/SL) Street(/CL) .(/.)<br>/NA: not any entity<br>/SC: start of company<br>/CC: continuation of company<br>/SL: start of location<br>/CL: continuation of location</p>
<h4 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h4><p><img src="/images/NLP/parsing.jpg" alt=""></p>
<h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><blockquote>
<p>At last, a computer that understands you like your mother.</p>
</blockquote>
<h4 id="Ambiguity"><a href="#Ambiguity" class="headerlink" title="Ambiguity"></a>Ambiguity</h4><p>This sentence can be interpreted in different ways:<br>1.It understands you as well as your mother understands you.<br>2.It understands (that) you like your mother.<br>3.It understands you as well as it understands your mother.</p>
<p>Ambiguity at Many Levels:<br><strong>Acoustic</strong> level<br>In speech recognition:<br>1.”… a computer that understands <em>you like</em> your mother”<br>2.”… a computer that understands <em>lie cured</em> mother”</p>
<p><strong>Syntactic</strong> level<br><img src="/images/NLP/syntactic.jpg" alt=""></p>
<p><strong>Semantic</strong> level<br>A word may have a variety of meanings and it may cause word sense ambiguity.</p>
<p><strong>Discourse</strong> (multi-clause) level<br>Alice says they’ve built a computer that understands you like your mother<br>But she…<br>  … doesn’t know any details<br>  … doesn’t understand me at all<br>This is an instance of <strong>anaphora</strong>, where <em>she</em> referees to some other discourse entity.</p>
<h2 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h2><p>· We have some (finite) vocabulary, say <script type="math/tex">\mathcal{V}={\text{the, a, man, telescope, Beckham, two, ...}}</script></p>
<p>· We have an (infinite) set of strings, <script type="math/tex">\mathcal{V}^\dagger</script>:</p>
<blockquote>
<p>the STOP<br>a STOP<br>the fan STOP<br>the fan saw Beckham STOP<br>the fan saw saw STOP<br>the fan saw Beckham play for Real Madrid STOP</p>
</blockquote>
<p>· We have a <em>training sample</em> of example sentences in English<br>· We need to “learn” a probability distribution <em>p</em> i.e., <em>p</em> is a function that satisfies</p>
<script type="math/tex; mode=display">\sum_{x\in\mathcal{V^\dagger}}p(x)=1,\ p(x)\ge0\text{ for all }x\in\mathcal{V^\dagger}</script><p>Definition: A language model consists of a finite set <script type="math/tex">\mathcal{V}</script>, and a function <script type="math/tex">p(x_1,x_2,...,x_n)</script> such that:<br>1.For any <script type="math/tex"><x_1...x_n>\in\mathcal{V^\dagger}</script>, <script type="math/tex">p(x_1,x_2,...,x_n)\ge 0</script><br>2.In addition, <script type="math/tex">\sum_{<x_1...x_n>\in\mathcal{V^\dagger}}p(x_1,x_2,...,x_n)=1</script></p>
<p>Language models are very useful in a broad range of applications, the most obvious perhaps being speech recognition and machine translation. In many applications it is very useful to have a good “prior” distribution <script type="math/tex">p(x_1...x_n)</script> over which sentences are or aren’t probable in a language. For example, in speech recognition the language model is combined with an acoustic model that models the pronunciation of different words: one way to think about it is that the acoustic model generates a large number of candidate sentences, together with probabilities; the language model is then used to reorder these possibilities based on how likely they are to be a sentence in the language.<br>The techniques we describe for defining the function <em>p</em>, and for estimating the parameters of the resulting model from training examples, will be useful in several other contexts during the course: for example in hidden Markov models and in models for natural language parsing.</p>
<h3 id="A-Naive-Method"><a href="#A-Naive-Method" class="headerlink" title="A Naive Method"></a>A Naive Method</h3><p>We have N training sentences.<br>For any sentence <script type="math/tex">x_1...x_n,\ c(x_1...x_n)</script> is the number of times the sentence is seen in our training data.</p>
<script type="math/tex; mode=display">p(x_1...x_n)=\frac{c(x_1...x_n)}{N}</script><p>This is a poor model: in particular it will assign probability 0 to any sentence not seen in the training corpus. Thus it fails to generalize to sentences that have not seen in the training data. The key technical contribution of this chapter will be to introduce methods that do generalize to sentences that are not seen in our training data.</p>
<h3 id="Markov-Models"><a href="#Markov-Models" class="headerlink" title="Markov Models"></a>Markov Models</h3><p><strong>First-Order Markov Processes</strong></p>
<script type="math/tex; mode=display">\begin{aligned}&P(X_1=x_1,X_2=x_2,...,X_n=x_n)\\=&P(X_1=x_1)\prod_{i=2}^nP(X_i=x_i|X_1=x_1,...,X_{i-1}=x_{i-1})\\=&P(X_1=x_1)\prod_{i=2}^nP(X_i=x_i|X_{i-1}=x_{i-1})\end{aligned}</script><p>The first step is exact: by the chain rule of probabilities, <em>any</em> distribution <script type="math/tex">P(X_1=x_1...X_n=x_n)</script> can be written in this form. So we have made no assumptions in this step of the derivation. However, the second step is not necessarily exact: we have made the assumption that for any <script type="math/tex">i\in\{2...n\}</script>, for any <script type="math/tex">x_1...x_i</script>,</p>
<script type="math/tex; mode=display">P(X_i=x_i|X_1=x_1...X_{i-1}=x_{i-1})=P(X_i=x_i|X_{i-1}=x_{i-1})</script><p>This is a first-order <em>Markov assumption</em>. We have assumed that the identity of the i’th word in the sequence depends only on the identity of the previous word, <script type="math/tex">x_{i-1}</script>. More formally, we have assumed that the value of <script type="math/tex">X_i</script> is conditionally independent of <script type="math/tex">X_1...X_{i-2}</script>, given the value of <script type="math/tex">X_{i-1}</script>.</p>
<p><strong>Second-Order Markov Processes</strong></p>
<script type="math/tex; mode=display">\begin{aligned}&P(X_1=x_1,X_2=x_2,...,X_n=x_n)\\=&P(X_1=x_1)
\times P(X_2=x_2|X_1=x_1)\prod_{i=3}^nP(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\\=&\prod_{i=1}^nP(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\end{aligned}</script><p>(For convenience we assume <script type="math/tex">x_0=x_{-1}=*</script>, where * is a special “start” symbol.)</p>
<p>Compared with first-order Markov process, we make a slightly weaker assumption, namely that each word depends on the previous <em>two</em> words in the sequence. And the second-order Markov process will form the basis of trigram language models.</p>
<p>The length of sequence <em>n</em> can itself vary. We just assume that the <em>n</em>‘th word in the sequence, <script type="math/tex">X_n</script> is always equal to a special symbol, the STOP symbol. This symbol can only appear at the end of a sequence and it doesn’t belong to the set <script type="math/tex">\mathcal{V}</script>.</p>
<p>Process:</p>
<ol>
<li>Initialize <em>i</em> = 1, and <script type="math/tex">x_0=x_{-1}=*</script></li>
<li>Generate <script type="math/tex">x_i</script> from the distribution <script type="math/tex">P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})</script></li>
<li>If <script type="math/tex">x_i=STOP</script> then return the sequence <script type="math/tex">x_1...x_n</script>. Otherwise, set <script type="math/tex">i=i+1</script> and return to step 2.</li>
</ol>
<h3 id="Trigram"><a href="#Trigram" class="headerlink" title="Trigram"></a>Trigram</h3><p>A trigram language model consists of a finite set <script type="math/tex">\mathcal{V}</script>, and a parameter <script type="math/tex">q(w|u,v)</script> for each trigram u,v,w such that <script type="math/tex">w\in\mathcal{V}\cup\{STOP\}</script>, and <script type="math/tex">u,v\in\mathcal{V}\cup\{*\}</script>. The value for <script type="math/tex">q(w|u,v)</script> can be interpreted as the probability of seeing the word <em>w</em> immediately after the bigram <em>(u,v)</em>. For any sentence <script type="math/tex">x_1...x_n</script> where <script type="math/tex">x_i\in\mathcal{V}</script> for <em>i</em> = 1…(n-1), and <script type="math/tex">x_n=STOP</script>, the probability of the sentence under the trigram language model is <script type="math/tex">p(x_1...x_n)=\prod_{i=1}^nq(x_i|x_{i-2},x_{i-1})</script>, where we define <script type="math/tex">x_0=x_{-1}=*</script>.</p>
<p>Estimation Problem:<br>A natural estimate (the “maximum likelihood estimate”):</p>
<script type="math/tex; mode=display">q(w_i|w_{i-2},w_{i-1})=\frac{Count(w_{i-2},w_{i-1},w_i)}{Count(w_{i-2},w_{i-1})}</script><p>For example:</p>
<script type="math/tex; mode=display">q(\text{barks|the, dog})=\frac{c(\text{the, dog, barks})}{c(\text{the, dog})}</script><p>This way of estimating parameters runs into a very serious issue. Say our vocabulary size is <script type="math/tex">N=|\mathcal{V}|</script>, then there are <script type="math/tex">N^3</script> parameters in the model. This leads to two problems:<br>· Many of the above estimates will be <script type="math/tex">q(w|u,v)=0</script>, due to the count in the numerator being 0. This will lead to many trigram probabilities being systematically underestimated: it seems unreasonable to assign probability 0 to any trigram not seen in training data, given that the number of parameters of the model is typically very large in comparison to the number of words in the training corpus.<br>· In cases where the denominator <script type="math/tex">c(u,v)</script> is equal to zero, the estimate is not well defined.</p>
<p><strong>Perplexity</strong><br>We have some test data sentences <script type="math/tex">x^{(1)},x^{(2)},...,x^{(m)}</script>. Note that test sentences are “held out”, in the sense that they are not part of the corpus used to estimate the language model. (Just the same as normal machine learning problems.)<br>We could look at the probability under our model <script type="math/tex">\prod_{i=1}^mp(s_i)</script>.<br>More conveniently, the log probability:</p>
<script type="math/tex; mode=display">\log\prod_{i=1}^mp(s_i)=\sum_{i=1}^m\log p(s_i)</script><p>In fact the usual evaluation measure is :</p>
<script type="math/tex; mode=display">\text{Perplexity}=2^{-l}, \text{where }l=\frac{1}{M}\sum_{i=1}^m\log p(s_i)</script><p>and M is the total number of words in the test data <script type="math/tex">M=\sum_{i=1}^mn_i</script>. <script type="math/tex">n_i</script> is the length of the <em>i</em>‘th test sentence.</p>
<h3 id="Linear-Interpolation"><a href="#Linear-Interpolation" class="headerlink" title="Linear Interpolation"></a>Linear Interpolation</h3><script type="math/tex; mode=display">q_{ML}(w|u,v)=\frac{c(u,v,w)}{c(u,v)}</script><script type="math/tex; mode=display">q_{ML}(w|v)=\frac{c(v,w)}{c(v)}</script><script type="math/tex; mode=display">q_{ML}(w)=\frac{c(w)}{c()}</script><p>The subscript <em>ML</em> means <em>maximum-likelihood</em> estimation. And <script type="math/tex">c(w)</script> is the number of times word <em>w</em> is seen in the training corpus, and <script type="math/tex">c()</script> is the total number of words seen in the training corpus.<br>The trigram, bigram and unigram estimates have different strengths and weaknesses. The idea in linear interpolation is to use all three estimates, by taking a weighted average of the three estimates:</p>
<script type="math/tex; mode=display">q(w|u,v)=\lambda_1\times q_{ML}(w|u,v)+\lambda_2\times q_{ML}(w|v)+\lambda_3\times q_{ML}(w)</script><p>Here <script type="math/tex">\lambda_1</script>, <script type="math/tex">\lambda_2</script> and <script type="math/tex">\lambda_3</script> are three additional parameters of the model, which satisfies <script type="math/tex">lambda_1+\lambda_2+\lambda_3=1</script> and <script type="math/tex">\lambda_i\ge0</script> for all <em>i</em>.<br>(Our estimate correctly defines a distribution.)</p>
<p>There are various ways of estimating the λ values. A common one is as follows. Say we have some additional held-out data(development data), which is separate from both our training and test corpora.<br>Define <script type="math/tex">c'(u,v,w)</script> to be the number of times that the trigram (u,v,w) is seen in the development data. </p>
<script type="math/tex; mode=display">L(\lambda_1,\lambda_2,\lambda_3)=\sum_{u,v,w}c'(u,v,w)\log q(w|u,v)\\=\sum_{u,v,w}c'(u,v,w)\log(lambda_1\times q_{ML}(w|u,v)+\lambda_2\times q_{ML}(w|v)+\lambda_3\times q_{ML}(w))</script><p>We would like to choose our λ values to maximize <script type="math/tex">L(\lambda_1,\lambda_2,\lambda_3)</script>.(It means minimize perplexity.)</p>
<p>The three parameters <script type="math/tex">\lambda_1,\lambda_2,\lambda_3</script> can be interpreted as an indication of the confidence, or weight, placed on each of the trigram, bigram and unigram estimates.</p>
<p>In practice, it is important to add an additional degree of freedom, by allowing the values for <script type="math/tex">\lambda_1,\lambda_2,\lambda_3</script> to vary.<br>Take a function <script type="math/tex">\Pi</script> that partitions histories<br>e.g.,</p>
<script type="math/tex; mode=display">\Pi(w_{i-2},w_{i-1})=\begin{cases}1\text{  If Count}(w_{i-1},w_{i-2})=0\\2\text{  If }1\le Count(w_{i-1},w_{i-2})\le2\\3\text{  If }3\le Count(w_{i-1},w_{i-2})\le5\\4\text{  Otherwise}\end{cases}</script><p>Introduce a dependence of the λ’s on the partition:</p>
<script type="math/tex; mode=display">\begin{aligned}q(w_i|w_{i-2},w_{i-1})=&\lambda_1^{\Pi(w_{i-2},w_{i-1})}\times q_{ML}(w_i|w_{i-2},w_{i-1})\\ +&\lambda_2^{\Pi(w_{i-2},w_{i-1})}\times q_{ML}(w_i|w_{i-1})\\ +&\lambda_3^{\Pi(w_{i-2},w_{i-1})}\times q_{ML}(w_i)\end{aligned}</script><p>where <script type="math/tex">\lambda_1^{\Pi(w_{i-2},w_{i-1})}+\lambda_2^{\Pi(w_{i-2},w_{i-1})}+\lambda_3^{\Pi(w_{i-2},w_{i-1})}=1</script>, and <script type="math/tex">\lambda_i^{\Pi(w_{i-2},w_{i-1})}\ge0</script> for all <em>i</em>.</p>
<p>By the way, <script type="math/tex">lambda_1</script> should be 0 if <script type="math/tex">c(u,v)=0</script>, because in this case the trigram estimate <script type="math/tex">q_{ML}(w|u,v)=\frac{c(u,v,w)}{c(u,v)}</script> is undefined. Similarly, if both <script type="math/tex">c(u,v)</script> and <script type="math/tex">c(v)</script> are equal to zero, we need <script type="math/tex">\lambda_1=\lambda_2=0</script></p>
<p>In the referencing note, a simple method is introduced:</p>
<script type="math/tex; mode=display">\lambda_1=\frac{c(u,v)}{c(u,v)+\gamma}</script><script type="math/tex; mode=display">\lambda_2=(1-\lambda_1)\times\frac{c(v)}{c(v)+\gamma}</script><script type="math/tex; mode=display">\lambda_3=1-\lambda_1-\lambda_2</script><p>γ&gt;0.<br>Under this definition, it can be seen that <script type="math/tex">\lambda_1</script> increases as <script type="math/tex">c(u,v)</script> increases, and similarly that <script type="math/tex">\lambda_2</script> increases as <script type="math/tex">c(v)</script> increases. This method is relatively crude, and is not likely to be optimal. It is, however, very simple, and in practice it can work well in some applications.</p>
<h3 id="Discounting-Methods"><a href="#Discounting-Methods" class="headerlink" title="Discounting Methods"></a>Discounting Methods</h3><p>For any bigram <script type="math/tex">Count(w_{i-1},w)</script> such that <script type="math/tex">Count(w_{i-1},w)>0</script>, we define the discounted count as</p>
<script type="math/tex; mode=display">Count^*(w_{i-1},w)=Count(w_{i-1},w)-\beta</script><p>where β is a value between 0 and 1 (a typical value might be β=0.5). Thus we simply subtract a constant value, β, from the count. This reflects the intuition that if we take counts from the training corpus, we will systematically over-estimate the probability of bigrams seen in the corpus(and under-estimate bigrams not seen in the corpus).<br>For any bigram <script type="math/tex">(w_{i-1},w)</script> such that <script type="math/tex">Count(w_{i-1},w)>0</script>, we can then define <script type="math/tex">q(w|w_{i-1})=\frac{Count^*(w_{i-1},w)}{Count(w_{i-1})}</script>. For any context <script type="math/tex">w_{i-1}</script>, this definition leads to some <em>missing probability mass</em>, defined as <script type="math/tex">\alpha(v)=1-\sum_{w:Count(w_{i-1},w)>0}\frac{Count^*(w_{i-1},w)}{Count(w_{i-1})}</script><br>The intuition behind discounted methods is to divide this “missing mass” between the words <em>w</em> such that <script type="math/tex">Count(w_{i-1},w)=0</script>.<br>Define two sets</p>
<script type="math/tex; mode=display">\mathcal{A}(w_{i-1})=\{w:Count(w_{i-1},w)>0</script><script type="math/tex; mode=display">\mathcal{B}(w_{i-1})=\{w:Count(w_{i-1},w)=0</script><p>Then the back-off model is defined as</p>
<script type="math/tex; mode=display">q_{BO}(w_i|w_{i-1})=\begin{cases}\frac{Count^*(w_{i-1},w_i)}{Count(w_{i-1})}\text{  if }w_i\in\mathcal{A}(w_{i-1})\\\alpha(w_{i-1})\frac{q_{ML}(w_i)}{\sum_{w\in\mathcal{B}(w_{i-1})}q_{ML}(w)}\text{  if }w_i\in\mathcal{B}(w_{i-1})\end{cases}</script><p>Thus if <script type="math/tex">Count(w_{i-1},w)>0</script> we return the estimate <script type="math/tex">\frac{Count^*(w_{i-1},w_i)}{Count(w_{i-1})}</script>; otherwise we divide the remaining probability mass <script type="math/tex">\alpha(w_{i-1})</script> in proportion to the unigram estimate <script type="math/tex">q_{ML}(w)</script>.</p>
<p>The method can be generalized to trigram language models in a natural, recursive way:<br><img src="/images/NLP/trigramDiscount.jpg" alt=""></p>
<h1 id="Week-Two"><a href="#Week-Two" class="headerlink" title="Week Two"></a>Week Two</h1><h2 id="Tagging-1"><a href="#Tagging-1" class="headerlink" title="Tagging"></a>Tagging</h2><p>Given the input to the tagging model(referred as a <em>sentence</em>) <script type="math/tex">x_1...x_n</script>, use <script type="math/tex">y_1...y_n</script> to denote the output of the tagging model(referred as the <em>state sequence</em> or <em>tag sequence</em>).<br>This type of problem, where the task is to map a sentence <script type="math/tex">x_1...x_n</script> to a tag sequence <script type="math/tex">y_1...y_n</script> is often referred to as a <strong>sequence labeling problem</strong>, or a <strong>tagging problem</strong>.<br>We will assume that we have a set of training examples, <script type="math/tex">(x^{(i)},y^{(i)})</script> for <script type="math/tex">i=1...m</script>, where each <script type="math/tex">x^{(i)}</script> is a sentence <script type="math/tex">x_1^{(i)}...x_{n_i}^{(i)}</script>, and each <script type="math/tex">y^{(i)}</script> is a tag sequence <script type="math/tex">y^{(i)}_1...y^{(i)}_{n_i}</script>(we assume that the <em>i</em>‘th training example is of length <script type="math/tex">n_i</script>). Hence <script type="math/tex">x_j^{(i)}</script> is the <em>j</em>‘th word in the <em>i</em>‘th training example, and <script type="math/tex">y_j^{(i)}</script> is the tag for that word. Our task is to learn a function that maps sentences to tag sequences from these training examples.</p>
<h3 id="POS-Tagging"><a href="#POS-Tagging" class="headerlink" title="POS Tagging"></a>POS Tagging</h3><p>POS Tagging: Part-of-Speech tagging.<br>The input to the problem is a sentence. The output is a tagged sentence, where each word in the sentence is annotated with its part of speech. Our goal will be to construct a model that recovers POS tags for sentences with high accuracy. POS tagging is one of the most basic problems in NLP, and is useful in many natural language applications.</p>
<h4 id="Tags"><a href="#Tags" class="headerlink" title="Tags"></a>Tags</h4><ul>
<li>D: determiner (a, an, the)</li>
<li>N: noun</li>
<li>V: verb</li>
<li>P: preposition</li>
<li>Adv: adverb</li>
<li>Adj: adjective</li>
<li>…</li>
</ul>
<h4 id="Challenges-1"><a href="#Challenges-1" class="headerlink" title="Challenges"></a>Challenges</h4><p><strong>Ambiguity</strong><br>Many words in English can take several possible parts of speech(as well as in Chinese and many other languages). A word can be a noun as well as a verb. E.g., <em>look</em>, <em>result</em>…<br><strong>Rare words</strong><br>Some words are rare and may not be seen in the training examples. Even with say a million words of training data, there will be many words in new sentences which have not been seen in training. It will be important to develop methods that deal effectively with words which have not been seen in training data.</p>
<h4 id="Sources-of-information"><a href="#Sources-of-information" class="headerlink" title="Sources of information"></a>Sources of information</h4><p><strong>Local</strong><br>Individual words have statistical preferences for their part of speech.<br>E.g., <em>can</em> is more likely to be a modal verb rather than a noun.</p>
<p><strong>Contextual</strong><br>The context has an important effect on the part of speech for a word. In particular, some sequences of POS tags are much more likely than others. If we consider POS trigrams, the sequence <code>D N V</code> will be frequent in English, whereas the sequence <code>D V N</code> is much less likely.<br>Sometimes these two sources of evidence are in conflict. For example, in the sentence <em>The trash can is hard to find</em>, the part of speech for <em>can</em> is a noun-however, <em>can</em> can also be a modal verb, and in fact it is much more frequently seen as a modal verb in English. In this sentence the context has overridden the tendency for <em>can</em> to be a verb as opposed to a noun.</p>
<h3 id="Named-Entity"><a href="#Named-Entity" class="headerlink" title="Named-Entity"></a>Named-Entity</h3><p>For named-entity problem, the input is again a sentence. The output is the sentence with entity-boundaries marked. Recognizing entities such as people, locations and organizations has many applications, and name-entity recognition has been widely studies in NLP research.</p>
<p>Once this mapping has been performed on training examples, we can train a tagging model on these training examples. Given a new test sentence we can then recover the sequence of tags from the model, and it is straightforward to identify the entities identified by the model.</p>
<h2 id="Generative-Models"><a href="#Generative-Models" class="headerlink" title="Generative Models"></a>Generative Models</h2><p><strong>Supervised Learning:</strong><br>Assume training examples <script type="math/tex">(x^{(1)},y^{(1)})...(x^{(m)},y^{(m)})</script>, where each example consists of an input <script type="math/tex">x^{(i)}</script> paired with a label <script type="math/tex">y^{(i)}</script>. We use <script type="math/tex">\mathcal{X}</script> to refer to the set of possible inputs, and <script type="math/tex">\mathcal{Y}</script> to refer to the set of possible labels. Our task is to learn a function <script type="math/tex">f:\mathcal{X}\to\mathcal{Y}</script> that maps any input x to a label f(x).<br>One way to define the function f(x) is through a <em>conditional model</em>. In this approach we define a model that defines the conditional probability <script type="math/tex">p(y|x)</script> for any x,y pair. The parameters of the model are estimated from the training examples. Given a new test example x, the output from the model is <script type="math/tex">f(x)=\arg\max_{y\in\mathcal{Y}}p(y|x)</script><br>Thus we simply take the most likely label <em>y</em> as the output from the model. If our model <script type="math/tex">p(y|x)</script> is close to the true conditional distribution of labels given inputs, the function f(x) will be close to optimal.</p>
<p><strong>Generative Models</strong><br>Rather than directly estimating the conditional distribution <script type="math/tex">p(y|x)</script>, in generative models we instead model the <em>joint</em> probability <script type="math/tex">p(x,y)</script> over <script type="math/tex">(x,y)</script> pairs. The parameters of the model <script type="math/tex">p(x,y)</script> are again estimated from the training examples <script type="math/tex">(x^{(i)},y^{(i)})</script> for <script type="math/tex">I=1...n</script>. In many cases we further decompose the probability <script type="math/tex">p(x,y)</script> as <script type="math/tex">p(x,y)=p(y)p(x|y)</script> and then estimate the models for <script type="math/tex">p(y)</script> and <script type="math/tex">p(x|y)</script> separately. These two model components have the following interpretations:<br>· <script type="math/tex">p(y)</script> is a <em>prior</em> probability distribution over labels <em>y</em>.<br>· <script type="math/tex">p(x|y)</script> is the probability of generating the input <em>x</em>, given that the underlying label is <em>y</em>.</p>
<p>Given a generative model, we can use Bayes rule to derive the condition probability <script type="math/tex">p(y|x)</script> for any <script type="math/tex">(x,y)</script> pair:</p>
<script type="math/tex; mode=display">p(y|x)=\frac{p(y)p(x|y)}{p(x)}</script><p>where</p>
<script type="math/tex; mode=display">p(x)=\sum_{y\in\mathcal{Y}}p(x,y)=\sum_{y\in\mathcal{Y}}p(y)p(x|y)</script><p>We use Bayes rule directly in applying the joint model to a new test example. Given an input <em>x</em>, the output of our model, f(x), can be derived as follows:</p>
<script type="math/tex; mode=display">\begin{aligned}f(x)&=\arg\max_yp(y|x)\\&=\arg\max_y\frac{p(y)p(x|y)}{p(x)}\text{  (2)}\\&=\arg\max_yp(y)p(x|y)\text{  (3)}\end{aligned}</script><p>Eq. 2 follows by Bayes rule. Eq. 3 follows because the denominator, <script type="math/tex">p(x)</script>, does not depend on <em>y</em>, and hence does not affect the arg max. This is convenient, because it mean that we do not need calculate <script type="math/tex">p(x)</script>, which can be an expensive operation.<br>Models that decompose a joint probability into terms <script type="math/tex">p(y)</script> and <script type="math/tex">p(x|y)</script> are often called <em>noisy-channel</em> models. Intuitively, when we see a test example <em>x</em>, we assume that has been generated in two steps: first, a label <em>y</em> has been chosen with probability <script type="math/tex">p(y)</script>; second, the example <em>x</em> has been generated from the distribution <script type="math/tex">p(x|y)</script>. The model <script type="math/tex">p(x|y)</script> can be interpreted as a “channel” which takes a label <em>y</em> as its input, and corrupts it to produce <em>x</em> as its output. Our task is to find the most likely label <em>y</em>, given that we observe <em>x</em>.</p>
<p>In summary:</p>
<ul>
<li>Our task is to learn a function from inputs <em>x</em> to labels <script type="math/tex">y=f(x)</script>. We assume training examples <script type="math/tex">(x^{(i)},y^{(i)})</script> for <script type="math/tex">i=1...n</script>.</li>
<li>In the noisy channel approach, we use the training example to estimate models <script type="math/tex">p(y)</script> and <script type="math/tex">p(x|y)</script>. These models define a joint(generative) model: <script type="math/tex">p(x,y)=p(y)p(x|y)</script></li>
<li>Given a new test example <em>x</em>, we predict the label <script type="math/tex">f(x)=\arg\max_{y\in\mathcal{Y}}p(y)p(x|y)</script>. Finding the output <em>f(x)</em> for an input <em>x</em> is often referred to as the <em>decoding problem</em>.</li>
</ul>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    Hael Chan
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://haelchan.me/2018/03/31/NLP-note/" title="Natural Language Processing Note">http://haelchan.me/2018/03/31/NLP-note/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/natural-language-processing/" rel="tag"># natural language processing</a>
          
            <a href="/tags/learning-note/" rel="tag"># learning note</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/03/14/oop-note/" rel="next" title="面向对象程序设计课堂笔记">
                <i class="fa fa-chevron-left"></i> 面向对象程序设计课堂笔记
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/04/08/composition-model/" rel="prev" title="composition-model">
                composition-model <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  
    <div class="comments" id="comments">
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    </div>
  


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/avatar.jpg"
                alt="Hael Chan" />
            
              <p class="site-author-name" itemprop="name">Hael Chan</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">17</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
              
                <span class="links-of-author-item">
                  <a href="https://github.com/haelchan" target="_blank" title="GitHub">
                    
                      <i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="mailto:f.procumbens@gmail.com" target="_blank" title="E-Mail">
                    
                      <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://twitter.com/Procumbens" target="_blank" title="Twitter">
                    
                      <i class="fa fa-fw fa-twitter"></i>Twitter</a>
                </span>
              
                <span class="links-of-author-item">
                  <a href="https://www.zhihu.com/people/hael-c/activities" target="_blank" title="知乎">
                    
                      <i class="fa fa-fw fa-zhihu"></i>知乎</a>
                </span>
              
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-inline">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-globe"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="https://learnerwn.github.io" title="WN_Blog" target="_blank">WN_Blog</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-One"><span class="nav-number">1.</span> <span class="nav-text">Week One</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Introduction"><span class="nav-number">1.1.</span> <span class="nav-text">Introduction</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Applications"><span class="nav-number">1.1.1.</span> <span class="nav-text">Applications</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Basic-NLP-Problems"><span class="nav-number">1.1.2.</span> <span class="nav-text">Basic NLP Problems</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tagging"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Tagging</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Parsing"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">Parsing</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Challenges"><span class="nav-number">1.1.3.</span> <span class="nav-text">Challenges</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Ambiguity"><span class="nav-number">1.1.3.1.</span> <span class="nav-text">Ambiguity</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Language-Modeling"><span class="nav-number">1.2.</span> <span class="nav-text">Language Modeling</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#A-Naive-Method"><span class="nav-number">1.2.1.</span> <span class="nav-text">A Naive Method</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Markov-Models"><span class="nav-number">1.2.2.</span> <span class="nav-text">Markov Models</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Trigram"><span class="nav-number">1.2.3.</span> <span class="nav-text">Trigram</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linear-Interpolation"><span class="nav-number">1.2.4.</span> <span class="nav-text">Linear Interpolation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Discounting-Methods"><span class="nav-number">1.2.5.</span> <span class="nav-text">Discounting Methods</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Week-Two"><span class="nav-number">2.</span> <span class="nav-text">Week Two</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Tagging-1"><span class="nav-number">2.1.</span> <span class="nav-text">Tagging</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#POS-Tagging"><span class="nav-number">2.1.1.</span> <span class="nav-text">POS Tagging</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tags"><span class="nav-number">2.1.1.1.</span> <span class="nav-text">Tags</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Challenges-1"><span class="nav-number">2.1.1.2.</span> <span class="nav-text">Challenges</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sources-of-information"><span class="nav-number">2.1.1.3.</span> <span class="nav-text">Sources of information</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Named-Entity"><span class="nav-number">2.1.2.</span> <span class="nav-text">Named-Entity</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Generative-Models"><span class="nav-number">2.2.</span> <span class="nav-text">Generative Models</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Hael Chan</span>

  
</div>









<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
</script>

<span id="busuanzi_container_site_uv">
  欢迎~您是本站的第<span id="busuanzi_value_site_uv"></span>位访客
</span>

        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.3"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  

    
      <script id="dsq-count-scr" src="https://hael.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'http://haelchan.me/2018/03/31/NLP-note/';
          this.page.identifier = '2018/03/31/NLP-note/';
          this.page.title = 'Natural Language Processing Note';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://hael.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  










  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'manual') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("3HhWLTewaKTSPj5DC3qp5b2m-gzGzoHsz", "zjxN2hpHQEmyMaz0XBicI3bn");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
