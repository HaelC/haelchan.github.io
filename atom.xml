<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hael&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://haelchan.me/"/>
  <updated>2020-02-17T00:43:10.000Z</updated>
  <id>http://haelchan.me/</id>
  
  <author>
    <name>Hael Chan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Visual Studio Code for Web Development</title>
    <link href="http://haelchan.me/2020/02/16/vscode-for-webdev/"/>
    <id>http://haelchan.me/2020/02/16/vscode-for-webdev/</id>
    <published>2020-02-16T08:42:30.000Z</published>
    <updated>2020-02-17T00:43:10.000Z</updated>
    
    <content type="html"><![CDATA[<p>Visual Studio Code (VS Code) is powerful and it helps to improve our productivity significantly. In this post I would like to share some efficient tips for web development.</p><h2 id="Built-in-Features"><a href="#Built-in-Features" class="headerlink" title="Built-in Features"></a>Built-in Features</h2><h3 id="Shortcuts"><a href="#Shortcuts" class="headerlink" title="Shortcuts"></a>Shortcuts</h3><p>There are countless keyboard shortcuts in VS code. You can view them by pressing <code>⌘+K ⌘+R</code> (Keep command ⌘ pressed, then press K and R successively) on macOS, or <code>Ctrl+K Ctrl+R</code> on Windows. Here I will list the most useful shortcuts. I use symbols (<code>⌘</code> for command, <code>⇧</code> for shift, <code>⌥</code> for option and <code>⌃</code> for control) for macOS and <code>Ctrl, Shift, Alt</code> for Windows.</p><h4 id="General"><a href="#General" class="headerlink" title="General"></a>General</h4><ul><li><code>⇧ ⌘ P</code>/<code>Ctrl+Shift+P</code> <strong>Show All Commands</strong><br>  You can access all available commands based on your current context. If you forget some shortcuts, just use this command. You can also use <code>F1</code> to show all commands.</li><li><code>Ctrl+` </code> <strong>Toggle Integrated Terminal</strong><br>  This is a command that is the same on Windows and macOS. With the integrated terminal, we can use some commands like <code>npm install</code>, <code>npm start</code> easily.</li></ul><h4 id="Navigating-Files"><a href="#Navigating-Files" class="headerlink" title="Navigating Files"></a>Navigating Files</h4><ul><li><code>⌘ P</code>/<code>Ctrl+P</code> <strong>Quick Open</strong><br>  With quick open, we can view recent active files. We can type filenames or use array keys to open the target file. Furthermore, if we repeat quick open twice (i.e., <code>⌘+P ⌘+P</code>/<code>Ctrl+P Ctrl+P</code>, we can keep <code>⌘</code> or <code>Ctrl</code> pressed, meanwhile press <code>P</code> twice), we can cycle quickly between recently opened files. </li><li><code>⌃ 1</code>/<code>Alt+1</code> <strong>Open 1st Tab</strong><br>  We can switch to the tab based on its index. The first tab is the left most one. It’s useful when the number of tabs are small.</li><li><code>⇧ ⌘ [</code>/<code>Ctrl+PageUp</code>, <code>⇧ ⌘ ]</code>/<code>Ctrl+PageDown</code> <strong>Open Previous/Next Tab</strong><br>  If we are at the first tab and we use the command to open the previous tab, we will be navigated to the last tab.</li></ul><h4 id="Coding"><a href="#Coding" class="headerlink" title="Coding"></a>Coding</h4><ul><li><code>⌘ C</code>/<code>Ctrl + C</code> <strong>Copy Line</strong><br>  It will copy the whole line where the cursor locates. We don’t need to manually select the whole line first. Similarly, <code>⌘ X</code>/<code>Ctrl + X</code> will cut the whole line.</li><li><code>⇧ ⌘ K</code>/<code>Ctrl + Shift + K</code> <strong>Delete Line</strong><br>  Use this command to delete the line in which the cursor locates (without interfering clipboard).</li><li><code>⌥ ↑</code>/<code>Alt + ↑</code>, <code>⌥ ↓</code>/<code>Alt + ↓</code> <strong>Move Line Up/Down</strong><br>  We can use this command to move the current line up/down (i.e., exchange with its previous/next line). </li></ul><h3 id="Emmet"><a href="#Emmet" class="headerlink" title="Emmet"></a>Emmet</h3><p><a href="https://emmet.io/" target="_blank" rel="noopener">Emmet</a> is one of the most important tools in web development. It is built-in in VS Code, but it is not exclusive to VS Code. You can download Emmet for other editors as well, e.g., Sublime Text, Atom. Here I’d like to introduce the most frequent used Emmet syntax (for me). You can refer to <a href="https://www.youtube.com/watch?v=EzGWXTASWWo&amp;feature=youtu.be" target="_blank" rel="noopener">this video</a> or the <a href="https://docs.emmet.io/cheat-sheet/" target="_blank" rel="noopener">cheat sheet</a> for more information.</p><h4 id="HTML-Boilerplate"><a href="#HTML-Boilerplate" class="headerlink" title="HTML Boilerplate"></a>HTML Boilerplate</h4><p>By typing an exclamation mark <code>!</code> and hit tab (or enter, depends on personal preference), we can generate the boilerplate for HTML 5 so we don’t have to type those <code>DOCTYPE</code> on our own. You can also type <code>html:5</code> or <code>doc</code> to generate the boilerplate. They perform the same function, so why not just type one character.</p><h4 id="HTML-Tags"><a href="#HTML-Tags" class="headerlink" title="HTML Tags"></a>HTML Tags</h4><p>By typing the HTML tag and hit tab/enter (I will omit the tab/enter step in the following introduction), it will generate complete opening tag and closing tag, sometimes with some common attributes. E.g., <code>div</code> will generate <code>&lt;div&gt;&lt;/div&gt;</code>, <code>a</code> will generate <code>&lt;a href=&quot;&quot;&gt;&lt;/a&gt;</code>, <code>img</code> will generate <code>&lt;img src=&quot;&quot; alt=&quot;&quot;&gt;</code>.  </p><p>Some tags’ name is a bit long, and you can type abbreviations as well. E.g., <code>bq</code> is equivalent to <code>blockquote</code> and will generate <code>&lt;blockquote&gt;&lt;/blockquote&gt;</code>. <code>inp</code> for <code>input</code>, <code>tarea</code> for <code>textarea</code>, etc.</p><h4 id="Classes-IDs-and-Attributes"><a href="#Classes-IDs-and-Attributes" class="headerlink" title="Classes, IDs and Attributes"></a>Classes, IDs and Attributes</h4><p>In CSS, we can target classes with selector <code>.</code> and ids with <code>#</code>, these rules also apply in Emmet. If we want to declare a <code>div</code> with class <code>row</code>, we can type <code>div.row</code> and it will generate <code>&lt;div class=&quot;row&quot;&gt;&lt;/div&gt;</code>. Similarly, <code>div#username</code> will produce <code>&lt;div id=&quot;username&quot;&gt;&lt;/div&gt;</code>.<br>We can chain the rules together. E.g., <code>div.col-12.col-md-8#address</code> generates <code>&lt;div class=&quot;col-12 col-md-8&quot; id=&quot;address&quot;&gt;&lt;/div&gt;</code>.<br>In fact, if the tag is <code>div</code>, we can ommit the tag name and directly type <code>.col-12.col-md-8#address</code>, and we will get the same result as above. This is called <em>implicit tags</em> in Emmet.<br><code>:</code> is used for state in CSS, and it is about HTML elements’ attributes in Emmet. Here are some examples:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">a</span><br><span class="line">&lt;a href=&quot;&quot;&gt;&lt;/a&gt;</span><br><span class="line"></span><br><span class="line">a:blank</span><br><span class="line">&lt;a href=&quot;http://&quot; target=&quot;_blank&quot; rel=&quot;noopener noreferrer&quot;&gt;&lt;/a&gt;</span><br><span class="line"></span><br><span class="line">a:link</span><br><span class="line">&lt;a href=&quot;http://&quot;&gt;&lt;/a&gt;</span><br><span class="line"></span><br><span class="line">input</span><br><span class="line">&lt;input type=&quot;text&quot;&gt;</span><br><span class="line"></span><br><span class="line">input:number</span><br><span class="line">&lt;input type=&quot;number&quot; name=&quot;&quot; id=&quot;&quot;&gt;</span><br><span class="line"></span><br><span class="line">input:p</span><br><span class="line">&lt;input type=&quot;password&quot; name=&quot;&quot; id=&quot;&quot;&gt;</span><br><span class="line"></span><br><span class="line">button</span><br><span class="line">&lt;button&gt;&lt;/button&gt;</span><br><span class="line"></span><br><span class="line">button:s</span><br><span class="line">&lt;button type=&quot;submit&quot;&gt;&lt;/button&gt;</span><br></pre></td></tr></table></figure></p><p>We can also deal with custom attributes with square bracket <code>[]</code>. The content in the square bracket is the attribute name and the corresponding value. For example, <code>p[title=&quot;Hello world&quot;]</code> will produce <code>&lt;p title=&quot;Hello world&quot;&gt;&lt;/p&gt;</code> The value can be omitted and it will generate an empty string. E.g., <code>td[rowspan=2 colspan=3 title]</code> will produce <code>&lt;td rowspan=&quot;2&quot; colspan=&quot;3&quot; title=&quot;&quot;&gt;&lt;/td&gt;</code>.</p><h4 id="Content"><a href="#Content" class="headerlink" title="Content"></a>Content</h4><p>If we want to add some contents as well, we can use curly brace <code>{}</code>. E.g., <code>p{Hello}</code> would produce <code>&lt;p&gt;hello&lt;/p&gt;</code>.<br>Sometimes we want to add some texts to see how well the website looks like with text, but we don’t want to type so many non-sense words. We can use <code>lorem</code> to generate some dummy text for us. The default <code>lorem</code> will generate a 30-word dummy text, but you can specify how many words should be generated by adding numbers. E.g., <code>lorem100</code>.</p><!-- To add:Multiple pointersFoldESLintPrettier --><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><a href="https://code.visualstudio.com/docs/getstarted/keybindings" target="_blank" rel="noopener">Key Bindings for Visual Studio Code</a></li><li><a href="https://code.visualstudio.com/docs/getstarted/tips-and-tricks" target="_blank" rel="noopener">Visual Studio Code Tips and Tricks</a></li><li><a href="https://www.youtube.com/watch?v=EzGWXTASWWo&amp;feature=youtu.be" target="_blank" rel="noopener">Emmet = Faster HTML &amp; CSS Workflow!</a></li><li><a href="https://docs.emmet.io/" target="_blank" rel="noopener">Emmet</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Visual Studio Code (VS Code) is powerful and it helps to improve our productivity significantly. In this post I would like to share some 
      
    
    </summary>
    
      <category term="Web Development" scheme="http://haelchan.me/categories/Web-Development/"/>
    
    
      <category term="tips" scheme="http://haelchan.me/tags/tips/"/>
    
  </entry>
  
  <entry>
    <title>Data Structures and Algorithms Cheatsheet (C++)</title>
    <link href="http://haelchan.me/2019/08/06/ds-al-cpp/"/>
    <id>http://haelchan.me/2019/08/06/ds-al-cpp/</id>
    <published>2019-08-07T02:24:31.000Z</published>
    <updated>2019-12-27T03:32:05.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Data-Structures"><a href="#Data-Structures" class="headerlink" title="Data Structures"></a>Data Structures</h1><h2 id="Arrays"><a href="#Arrays" class="headerlink" title="Arrays"></a>Arrays</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* Built-in Array (fixed size) */</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Initialization */</span></span><br><span class="line"><span class="comment">//     the dimension must be a constant expression, </span></span><br><span class="line"><span class="comment">//     or be omitted when using list initialization</span></span><br><span class="line"><span class="keyword">int</span> a1[<span class="number">5</span>];                                      <span class="comment">// if defined in a function, it will have undefined values</span></span><br><span class="line"><span class="keyword">int</span> a2[] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;                           <span class="comment">// dimension is 3</span></span><br><span class="line"><span class="keyword">int</span> a3[<span class="number">5</span>] = &#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;                          <span class="comment">// equivalent to a3[] = &#123;1, 2, 3, 0, 0&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Size */</span></span><br><span class="line"><span class="keyword">int</span> size = <span class="keyword">sizeof</span>(a) / <span class="keyword">sizeof</span>(*a);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Iteration */</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; ++i) &#123;                    <span class="comment">// using subscript</span></span><br><span class="line">    <span class="comment">// a[i] ... </span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> element : a) &#123;                            <span class="comment">// traverse the entire array</span></span><br><span class="line">    <span class="comment">// element ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">                                                </span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> *p = begin(a); p != end(a); ++p) &#123;         <span class="comment">// using pointer</span></span><br><span class="line">    <span class="comment">// *p ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Sort */</span></span><br><span class="line">sort(a, a + size);</span><br></pre></td></tr></table></figure><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/* vector */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Initialization */</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v0;                                             <span class="comment">// empty vector</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v1(<span class="number">5</span>, <span class="number">1</span>);                                       <span class="comment">// 5 elements with value 1</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v2(<span class="number">5</span>);                                          <span class="comment">// 5 elements with default value 0</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v3&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;;                                    <span class="comment">// equivalent = vector&lt;int&gt; v3 = &#123;1, 2, 3&#125;;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Copy */</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v4(v0);</span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v5 = v4;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Size */</span></span><br><span class="line">v.size();</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Iteration */</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; v.size(); ++i) &#123;</span><br><span class="line">    <span class="comment">// v[i] ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> ele : v) &#123;</span><br><span class="line">    <span class="comment">// ele ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> it = v.begin(); it != v.end(); ++it) &#123;</span><br><span class="line">    <span class="comment">// *it ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Initialize a 2D vector */</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; vec(m, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(n));                 <span class="comment">// initialize m * n matrix with default value</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&gt; vec(m, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;(n, value));          <span class="comment">// initialize with given value</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* find whether a given value in the vector */</span></span><br><span class="line"><span class="comment">// https://stackoverflow.com/questions/571394/how-to-find-out-if-an-item-is-present-in-a-stdvector</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">if</span> (find(vec.begin(), vec.end(), value) != vec.end())</span><br><span class="line">    <span class="comment">// the vector contains the value</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="comment">// does not contain</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Extract a subvector from the vector */</span></span><br><span class="line"><span class="keyword">auto</span> left = vec.begin() + n1;</span><br><span class="line"><span class="keyword">auto</span> right = vec.begin() + n2;          <span class="comment">// right is not included</span></span><br><span class="line"><span class="built_in">vector</span>&lt;T&gt; subvec(left, right);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Concatenation */</span></span><br><span class="line">v1.insert(v1.end(), v2.begin(), v2.end());</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Sort: see the below section */</span></span><br></pre></td></tr></table></figure><h2 id="String"><a href="#String" class="headerlink" title="String"></a>String</h2><p><a href="http://www.cplusplus.com/reference/string/string/" target="_blank" rel="noopener">http://www.cplusplus.com/reference/string/string/</a><br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Initialization */</span></span><br><span class="line"><span class="built_in">string</span> s1;</span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">s2</span><span class="params">(s1)</span></span>;</span><br><span class="line"><span class="built_in">string</span> s2 = s1;</span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">s3</span><span class="params">(<span class="string">"value"</span>)</span></span>;</span><br><span class="line"><span class="built_in">string</span> s3 = <span class="string">"value"</span>;</span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">s4</span><span class="params">(n, <span class="string">'c'</span>)</span></span>;</span><br><span class="line"><span class="comment">// copy initialize: initialize a variable using =</span></span><br><span class="line"><span class="comment">// direct initialize: omit the =</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* string to number */</span></span><br><span class="line">stoi(str);                  <span class="comment">// to int</span></span><br><span class="line">stof(str);                  <span class="comment">// to float</span></span><br><span class="line">stod(str);                  <span class="comment">// to double</span></span><br><span class="line"><span class="comment">// atoi() is a C-way, not suggested</span></span><br><span class="line"><span class="comment">// from_chars() is introduced in C++17</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* number to string */</span></span><br><span class="line">to_string(value);           <span class="comment">// value type: int, float, double, long, unsigned, etc.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">string</span>::npos                <span class="comment">// the value is -1</span></span><br><span class="line"><span class="comment">// when used as the value for a len, it means until the end of the string</span></span><br><span class="line"><span class="comment">// when used as a return value, it is usually used to indicate no matches</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* sub string */</span></span><br><span class="line">s1.substr(pos = <span class="number">0</span>, len = npos);        <span class="comment">// note that the second parameter is the length of substring, not the ending position</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* find the index of another string/char */</span></span><br><span class="line">s1.find();                              <span class="comment">// return npos if not found</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* cctype functions */</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cctype&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="built_in">isalnum</span>(ch);</span><br><span class="line"><span class="built_in">isalpha</span>(ch);</span><br><span class="line"><span class="built_in">islower</span>(ch);</span><br><span class="line"><span class="built_in">isupper</span>(ch);</span><br><span class="line"><span class="built_in">isdigit</span>(ch);</span><br><span class="line">isblank(ch);</span><br><span class="line"><span class="built_in">ispunct</span>(ch);</span><br><span class="line"><span class="built_in">tolower</span>(ch);</span><br><span class="line"><span class="built_in">toupper</span>(ch);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* split string(sentence) into vector of string(words) */</span></span><br><span class="line"><span class="comment">// reference: https://stackoverflow.com/a/5208977</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;sstream&gt;</span></span></span><br><span class="line"><span class="function"><span class="built_in">string</span> <span class="title">str</span><span class="params">(<span class="string">"the sentence to split"</span>)</span></span>;</span><br><span class="line"><span class="built_in">string</span> buf;                             <span class="comment">// a buffer string</span></span><br><span class="line"><span class="function"><span class="built_in">stringstream</span> <span class="title">ss</span><span class="params">(str)</span></span>;                   <span class="comment">// insert the string into a stream</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="built_in">string</span>&gt; tokens;</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (ss &gt;&gt; buf)                       <span class="comment">// in this way, the delimiter is space</span></span><br><span class="line">    tokens.push_back(buf);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (getline(ss, buf, <span class="string">','</span>))           <span class="comment">// set other delimiter (e.g., ',') using getline</span></span><br><span class="line">    tokens.push_back(buf);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* join the vector into a string */</span></span><br><span class="line"><span class="comment">// reference: https://stackoverflow.com/a/1430774</span></span><br><span class="line"><span class="built_in">stringstream</span> ss;</span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; tokens.size(); ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span> (i)</span><br><span class="line">        ss &lt;&lt; <span class="string">" "</span>;                      <span class="comment">// or ',', or whatever delimiter</span></span><br><span class="line">    ss &lt;&lt; tokens[i];</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">string</span> s = ss.str();</span><br></pre></td></tr></table></figure></p><h2 id="Pair"><a href="#Pair" class="headerlink" title="Pair"></a>Pair</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Declaration and initialiaztion */</span></span><br><span class="line">pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; p1;          <span class="comment">// default constructor</span></span><br><span class="line">pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; p2 (<span class="number">1</span>, <span class="number">2</span>);   <span class="comment">// value init</span></span><br><span class="line">pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; p3 (p2);     <span class="comment">// copy constructor</span></span><br><span class="line"></span><br><span class="line">pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; p4 = &#123;<span class="number">3</span>, <span class="number">4</span>&#125;; <span class="comment">// in C++11</span></span><br><span class="line">pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; p5 = make_pair(<span class="number">5</span>, <span class="number">6</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Element access */</span></span><br><span class="line">p2.first                    <span class="comment">// no parentheses</span></span><br><span class="line">p2.second</span><br></pre></td></tr></table></figure><h2 id="Stack"><a href="#Stack" class="headerlink" title="Stack"></a>Stack</h2><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stack&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Declaration */</span></span><br><span class="line"><span class="built_in">stack</span>&lt;T&gt; s;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Element access */</span></span><br><span class="line">s.top();</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Capacity */</span></span><br><span class="line">s.size();</span><br><span class="line">s.empty();</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Modifiers */</span></span><br><span class="line">s.push();</span><br><span class="line">s.pop();                        <span class="comment">// return type of pop() is void, it doesn't return the top value. </span></span><br><span class="line">                                <span class="comment">// usually used with top() together</span></span><br></pre></td></tr></table></figure><h2 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h2><h3 id="Queue-Normal"><a href="#Queue-Normal" class="headerlink" title="Queue (Normal)"></a>Queue (Normal)</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Declaration */</span></span><br><span class="line"><span class="built_in">queue</span>&lt;T&gt; q;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Element access */</span></span><br><span class="line">q.front();                      <span class="comment">// not top()</span></span><br><span class="line">q.back();                       <span class="comment">// used more in deque</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Capacity */</span></span><br><span class="line">q.size();</span><br><span class="line">q.empty();</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Modifiers */</span></span><br><span class="line">q.push();</span><br><span class="line">q.pop();</span><br></pre></td></tr></table></figure><h3 id="Priority-Queue"><a href="#Priority-Queue" class="headerlink" title="Priority Queue"></a>Priority Queue</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;functional&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Priority Queue */</span></span><br><span class="line">priority_queue&lt;T&gt; pq;</span><br><span class="line">pq.push();</span><br><span class="line">pq.pop();</span><br><span class="line">pq.top();                        <span class="comment">// note that it is not consistent with queue!</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// By default, priority queue would pop largest first (as a max heap). </span></span><br><span class="line"><span class="comment">// If we want to pop smallest first (as a min heap), we can declare as follows:</span></span><br><span class="line">priority_queue&lt;<span class="keyword">int</span>, <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;, greater&lt;<span class="keyword">int</span>&gt;&gt; pq;</span><br></pre></td></tr></table></figure><h2 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h2><h3 id="Hash-Set"><a href="#Hash-Set" class="headerlink" title="Hash Set"></a>Hash Set</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unordered_set&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="built_in">unordered_set</span>&lt;<span class="keyword">int</span>&gt; hashset;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Modifier */</span></span><br><span class="line">hashset.insert();</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Check if the set contains a value */</span></span><br><span class="line"><span class="keyword">if</span> (hashset.count(value) &gt; <span class="number">0</span>)           <span class="comment">// original way, if contains, hashset.count(value) == 1</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (hashset.contains(value))            <span class="comment">// newly introduced in C++20</span></span><br><span class="line">    <span class="comment">// ...</span></span><br></pre></td></tr></table></figure><h3 id="Set-Tree-Set"><a href="#Set-Tree-Set" class="headerlink" title="Set (Tree Set)"></a>Set (Tree Set)</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;set&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span>&lt;<span class="keyword">int</span>&gt; s;</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">    The functions of set are basically the same as unordered_set.</span></span><br><span class="line"><span class="comment">    set containers are generally slower than unordered_set containers to access individual elements by their key, </span></span><br><span class="line"><span class="comment">    but they allow the direct iteration on subsets based on their order. and elements in set are stored in order (typically implemented as binary search trees).</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><h2 id="Map"><a href="#Map" class="headerlink" title="Map"></a>Map</h2><h3 id="Hash-Map"><a href="#Hash-Map" class="headerlink" title="Hash Map"></a>Hash Map</h3><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;unordered_map&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">/* Operator[] */</span></span><br><span class="line">value = hashmap[key];</span><br><span class="line">hashmap[key] = newValue;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Iterator */</span></span><br><span class="line"><span class="keyword">for</span> (<span class="keyword">auto</span> it : hashmap) &#123;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; it.first &lt;&lt; <span class="built_in">endl</span>;           <span class="comment">// key</span></span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; it.second &lt;&lt; <span class="built_in">endl</span>;          <span class="comment">// value</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* Hashmap as counter */</span></span><br><span class="line"><span class="keyword">if</span> (hashmap.count(key) &gt; <span class="number">0</span>)             <span class="comment">// update</span></span><br><span class="line">    ++hashmap[key];</span><br><span class="line"><span class="keyword">else</span>                                    <span class="comment">// insert</span></span><br><span class="line">    hashmap[key] = <span class="number">1</span>;</span><br><span class="line"><span class="comment">// in fact, we can simplify in one line:</span></span><br><span class="line">hashmap[key]++;</span><br></pre></td></tr></table></figure><h1 id="Algorithms"><a href="#Algorithms" class="headerlink" title="Algorithms"></a>Algorithms</h1><p>Well, algorithm is not a separated concept from data structure, and some algorithms have been covered in the above section. Here I’ll just list some more.</p><h2 id="Sort"><a href="#Sort" class="headerlink" title="Sort"></a>Sort</h2><p>For sorting algorithms, refer to my GitHub <a href="https://github.com/HaelChan/LeetCode/blob/master/Explanations/912.%20Sort%20an%20Array.md" target="_blank" rel="noopener">post</a>. The post currently includes insertion sort, merge sort and quick sort.<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="comment">//vector&lt;int&gt; v;</span></span><br><span class="line">sort(v.begin(), v.end());                   <span class="comment">// sort the vector in ascending order</span></span><br><span class="line">sort(v.begin(), v.end(), greater&lt;<span class="keyword">int</span>&gt;())    <span class="comment">// sort the vector in descending order</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// What if we want to sort in custom order?</span></span><br><span class="line"><span class="comment">// E.g., vector&lt;pair&lt;int, int&gt;&gt; v1</span></span><br><span class="line"><span class="comment">// if we want to order the vector with the pairs' second value:</span></span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">compareSecond</span><span class="params">(pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; p1, pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt; p2)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> p1.second &lt; p2.second;</span><br><span class="line">&#125;</span><br><span class="line">sort(v1.begin(), v1.end(), compareSecond);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">// And how about priority queue?</span></span><br><span class="line"><span class="comment">// E.g., we have a pair, which contains a point (expressed as a pair) and its distance to origin.</span></span><br><span class="line"><span class="comment">// We want to build a min-heap with the distance.</span></span><br><span class="line"><span class="keyword">typedef</span> pair&lt;pair&lt;<span class="keyword">int</span>, <span class="keyword">int</span>&gt;, <span class="keyword">int</span>&gt; point_dist;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">myCompare</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>: </span><br><span class="line">    <span class="function"><span class="keyword">bool</span> <span class="title">operator</span><span class="params">()</span><span class="params">(point_dist pd1, point_dist pd2)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pd1.second &gt; pd2.second;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">priority_queue&lt;point_dist, <span class="built_in">vector</span>&lt;point_dist&gt;, myCompare&gt; pq;</span><br></pre></td></tr></table></figure></p><h2 id="Binary-Search"><a href="#Binary-Search" class="headerlink" title="Binary Search"></a>Binary Search</h2><p>Given a sorted array, binary search could find the target with a time complexity of <script type="math/tex">O(n)</script>. Following is the basic template:<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">binarySearch</span><span class="params">(<span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt;&amp; nums, <span class="keyword">int</span> target)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (nums.empty()) </span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> left = <span class="number">0</span>, right = nums.size() - <span class="number">1</span>, mid;</span><br><span class="line">    <span class="keyword">while</span> (left &lt;= right) &#123;</span><br><span class="line">        mid = left + (right - left) / <span class="number">2</span>;        <span class="comment">// avoid overflow</span></span><br><span class="line">        <span class="keyword">if</span> (nums[mid] == target)</span><br><span class="line">            <span class="keyword">return</span> mid;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span> (nums[mid] &lt; target)</span><br><span class="line">            left = mid + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span>                                    <span class="comment">// nums[mid] &gt; target</span></span><br><span class="line">            right = mid - <span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>The pointer <code>left</code> and <code>right</code> are used to control the search area.<br>If the middle value equals our target, then we just make it. If the middle value is less than the target, then it means that we should search the larger part. We need to move the left pointer to right. Otherwise, we need to move the right pointer to left if the middle value is greater.<br>Note that the calculation of mid is a bit counterintuitive. The purpose is to prevent the overflow caused by adding. Suppose that left is 1 and right is INT_MAX, then the adding will cause overflow and lead to errors. Therefore, it’s advised to use <code>left + (right - left) / 2</code> instead.</p>]]></content>
    
    <summary type="html">
    
      Arrays, string, stack, queue, etc. Common data structures and algorithms.
    
    </summary>
    
    
      <category term="C++" scheme="http://haelchan.me/tags/C/"/>
    
      <category term="cheatsheet" scheme="http://haelchan.me/tags/cheatsheet/"/>
    
  </entry>
  
  <entry>
    <title>C++ Notes</title>
    <link href="http://haelchan.me/2019/07/03/cpp-notes/"/>
    <id>http://haelchan.me/2019/07/03/cpp-notes/</id>
    <published>2019-07-03T12:06:23.000Z</published>
    <updated>2019-12-27T03:31:04.000Z</updated>
    
    <content type="html"><![CDATA[<p>I got to know something about C++ when learning <a href="http://web.stanford.edu/class/archive/cs/cs106x/cs106x.1174/" target="_blank" rel="noopener">CS106X-Stanford</a> and <a href="https://haelchan.me/2018/03/14/oop-note/">Object-Oriented Programming-Zhejiang University </a>. However, I haven’t studied C++ specially. In this summer vacation, I will dig into C++ and keep some notes here.  </p><p>The data structure part has been moved to the new post <a href="https://haelchan.me/2019/08/07/ds-al-cpp/">Data Structures and Algorithms Cheatsheet (C++)</a>. The remaining part of this post is mainly about language features.</p><p><strong>Adding Literals and strings</strong>  </p><p>For historical reasons, and for compatibility with C, string literals are not standard library <code>string</code>s. It is important to remember that these types differ when you use string literals and library <code>string</code>s.  </p><p>When we mix <code>string</code>s and string or character literals, at least one operand to each <code>+</code> operator must be of <code>string</code> type. Thus, it is illegal to add two string literals. (<code>error: invalid operands to binary expression (&#39;const char *&#39; and &#39;const char *&#39;)</code>)  </p><h2 id="Unsigned-ints"><a href="#Unsigned-ints" class="headerlink" title="Unsigned ints"></a>Unsigned ints</h2><p>Be careful when using loops like<br><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; v.size() - <span class="number">1</span>; i++) <span class="comment">//...</span></span><br></pre></td></tr></table></figure></p><p>Because <code>.size()</code> returns the type <code>size_type</code> instead of <code>int</code>. It is an unsigned integral type and when <code>v</code> is empty, <code>v.size() - 1</code> would become the maximum integer rather than <code>-1</code>. </p><h2 id="Generic-Programming"><a href="#Generic-Programming" class="headerlink" title="Generic Programming"></a>Generic Programming</h2><p>Use <code>!=</code> rather than <code>&lt;</code> in <code>for</code> loops. </p><p>Use iterators rather than subscripts.  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;I got to know something about C++ when learning &lt;a href=&quot;http://web.stanford.edu/class/archive/cs/cs106x/cs106x.1174/&quot; target=&quot;_blank&quot; re
      
    
    </summary>
    
    
      <category term="tips" scheme="http://haelchan.me/tags/tips/"/>
    
      <category term="learning note" scheme="http://haelchan.me/tags/learning-note/"/>
    
      <category term="C++" scheme="http://haelchan.me/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>From statistical to neural - language modeling in detail</title>
    <link href="http://haelchan.me/2019/02/10/LM-in-detail/"/>
    <id>http://haelchan.me/2019/02/10/LM-in-detail/</id>
    <published>2019-02-10T13:53:50.000Z</published>
    <updated>2019-02-26T15:56:25.000Z</updated>
    
    <content type="html"><![CDATA[<p>Language modeling is the task of predicting the next word or character in a document. Language models were originally developed for the problem of speech recognition; they are also widely used in other NLP applications. For example:</p><ul><li>In machine translation, we convert from text in a source language to text in a target language.</li><li>In summarization, we convert from audio signal to text.</li><li>In dialogue systems, we convert from the user’s input (and perhaps an external knowledge base) into a text response.</li></ul><h2 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h2><p>Formally, the task of language modeling is to assign a probability to any sequences of words <script type="math/tex">w_1^n</script>, i.e., to estimate <script type="math/tex">P(w_1^n)</script>.<br>Notations: the expression <script type="math/tex">w_1^n</script> means the string <script type="math/tex">w_1,w_2,...,w_n</script>. For the joint probability of each word in a sequence having a particular value <script type="math/tex">P(X=w_1,Y=w_2,Z=w_3,...,W=w_n</script>, we’ll use <script type="math/tex">P(w_1,w_2,...w_n)</script>.<br>By using the chain rule of probability, we get</p><script type="math/tex; mode=display">P(w_1^n)=P(w_1)P(w_2|w_1)P(w_3|w_1^2)...P(w_n|w_1^{n-1})=\prod_{k=1}^nP(w_k|w_1^{k-1})</script><p>The chain rule converts the joint probability to the product of conditional probabilities. However, it doesn’t help to simplify the calculation. Therefore, language models make use of the markov-assumption, which approximates the history by just the last few words.<br>The bigram, for example, approximates the probability of a word given all the previous words <script type="math/tex">P(w_n|w_1^{n-1}</script> by using only the conditional probability of the preceding word <script type="math/tex">P(w_n|w_{n-1})</script>, then we can compute the probability of a complete word sequence:  </p><script type="math/tex; mode=display">P(w_1^n)\approx\prod_{k=1}^n(w_k|w_{k-1})</script><p>To estimate the probability, an intuitive way is maximum likelihood estimation(MLE). </p><script type="math/tex; mode=display">P(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)}{C(w_{n-1})}</script><p>To compute the probability of an entire sentence, it is convenient to pad the beginning and end with special symbols. E.g., a special symbol <code>&lt;s&gt;</code> at the beginning of the sentence, and a special end-symbol <code>&lt;/s&gt;</code>.<br>For the general case of MLE n-gram parameter estimation:</p><script type="math/tex; mode=display">P(w_n|w_{n-N+1}^{n-1})=\frac{C(w_{n-N+1}^{n-1}w_n)}{C(w_{n-N+1}^{n-1})}</script><p>The hyperpamater <em>n</em> controls the size of the context used in each conditional probability. The bias-variance tradeoff problem lies in <em>n</em>-gram language model. A small <em>n</em>-gram size may cause high bias, and a large <em>n</em>-gram size could lead to high variance. Since the language is full of long-range dependencies, it is impossible to capture those dependencies with small n. Language is creative and variant as well, so it is hard to set n too large. </p><h3 id="Smoothing"><a href="#Smoothing" class="headerlink" title="Smoothing"></a>Smoothing</h3><p>We have to make some modifications to our probabilities in case it meets some unseen patterns in the test text. There are several ways: add-one smoothing, backoff and interpolation, Kneser-Ney smoothing, etc.</p><h4 id="Laplace-Smoothing"><a href="#Laplace-Smoothing" class="headerlink" title="Laplace Smoothing"></a>Laplace Smoothing</h4><p>Take unigram for example. The unsmoothed maximum likelihood estimate of the unigram probability of the word <script type="math/tex">w_i</script> is its count <script type="math/tex">c_i</script> normalized by the total number of word tokens N: <script type="math/tex">P(w_i)=\frac{c_i}{N}</script>.<br>Laplace smoothing just adds one to each count (that’s why it’s called add-one smoothing). Since there are <script type="math/tex">V</script> words in the vocabulary and each one is incremented, we also need to adjust the denominator to take into account the extra <script type="math/tex">V</script> observations.</p><script type="math/tex; mode=display">P_\text{Laplace}(w_i)=\frac{c_i+1}{N+V}</script><p>For bigram counts, we need to augment the unigram count by the number of total word types in the vocabulary <script type="math/tex">V</script>:</p><script type="math/tex; mode=display">P_\text{Laplace}(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)+1}{C(w_{n-1})+V}</script><h4 id="Lidstone-Smoothing"><a href="#Lidstone-Smoothing" class="headerlink" title="Lidstone Smoothing"></a>Lidstone Smoothing</h4><p>Laplace smoothing is a special case of Lidstone smoothing. The basic framework of Lidstone smoothing:</p><script type="math/tex; mode=display">P_\text{Lidstone}(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)+\alpha}{C(w_{n-1})+V\alpha}</script><p>Instead of changing both the numerator and denominator, it is convenient to describe how a smoothing algorithm affects the numerator, by defining an adjusted count <script type="math/tex">c^*</script>:</p><script type="math/tex; mode=display">c_i^*=(c_i+\alpha)\frac{N}{N+V\alpha}</script><p>Then the probability can be calculated by dividing <script type="math/tex">c^*</script> by <script type="math/tex">N</script>.</p><h4 id="Good-Turing-Estimate"><a href="#Good-Turing-Estimate" class="headerlink" title="Good-Turing Estimate"></a>Good-Turing Estimate</h4><p>The Good-Turing estimate (Good, 1953) states that for any n-gram that occurs <script type="math/tex">r</script> times, we just pretend that the occurrences is <script type="math/tex">r^*</script> where </p><script type="math/tex; mode=display">r^*=(r+1)\frac{n_{r+1}}{n_r}</script><p>and where <script type="math/tex">n_r</script> is the number of n-grams that occur exactly <script type="math/tex">r</script> times in the training data. Then the corresponding probability for an n-gram <script type="math/tex">\alpha</script> with <script type="math/tex">r</script> counts should be normalized as:</p><script type="math/tex; mode=display">p_\text{GT}(\alpha)=\frac{r^*}{N}</script><h4 id="Discounting"><a href="#Discounting" class="headerlink" title="Discounting"></a>Discounting</h4><p>A related way to view smoothing is as discounting (lowering) some non-zero counts in order to get the probability mass that will be assigned to the zero counts. The ratio of the discounted counts to the original counts, discount <script type="math/tex">d_c</script>:</p><script type="math/tex; mode=display">d_c=\frac{c^*}{c}=\frac{c_i+\alpha}{c_i}\frac{N}{N+V\alpha}</script><p>Church and Gale (1991) show empirically that the average Good-Turing discount <script type="math/tex">(r-r^*)</script> associated with n-grams is largely constant over <script type="math/tex">r</script>. Absolute discounting formalizes this by subtracting a fixed (absolute) discount <script type="math/tex">d</script> from each count.</p><script type="math/tex; mode=display">P_\text{AD}(w_i|w_{i-1})=\frac{C(w_{i-1}w_i)-d}{\sum_vC(w_{i-1}v)}+\lambda(w_{i-1})P(w_i)</script><h4 id="Backoff"><a href="#Backoff" class="headerlink" title="Backoff"></a>Backoff</h4><p>When using large n-gram (trigram or more), we may have no corresponding examples. In a backoff n-gram model, if the n-gram we need has zero counts, we approximate it by backing off to the (N-1)-gram. We continue backing off until we reach a history that has some counts.<br>We have to apply backoff with discounting for a correct probability distribution. This kind of backoff is called Katz backoff.</p><script type="math/tex; mode=display">c^*(i,j)=c(i,j)-d</script><script type="math/tex; mode=display">p_\text{Katz}(i|j)=\begin{cases}\frac{c^*(i,j)}{c(j)}&\text{if }c(i,j)>0//\alpha(j)\times\frac{p_\text{unigram}(i)}{\sum_{i':c(i',j)=0}p_\text{unigram}(i')}&\text{if }c(i,j)=0\end{cases}</script><p>The term <script type="math/tex">\alpha(j)</script> indicates the amount of probability mass that has been discounted for context <em>j</em>.<br>Katz backoff is often combined with Good-Turing. The combined Good-Turing backoff algorithm involves quite detailed computation for estimating the Good-Turing smoothing and the <script type="math/tex">P^*</script> and <script type="math/tex">\alpha</script> values.</p><h4 id="Interpolation"><a href="#Interpolation" class="headerlink" title="Interpolation"></a>Interpolation</h4><p>In backoff, we only “back off” to a lower-order n-gram if we have zero evidence for a higher-order n-gram. By contrast, in interpolation, we always mix the probability estimates from all related n-gram estimators.<br>In simplelinear interpolation, we estimate the trigram probability by mixing together the unigram, bigram, and trigram probabilities, each weighted by a <script type="math/tex">\lambda</script>:</p><script type="math/tex; mode=display">\hat{P}(w_n|w_{n-2}w_{n-1})=\lambda_1P(w_n|w_{n-2}w_{n-1})+\lambda_2P(w_n|w_{n-1})+\lambda_3P(w_n)</script><p>To ensure that the estimated probability is valid, <script type="math/tex">\lambda</script>s should sum to 1: <script type="math/tex">\sum_{n=1}^3\lambda_n=1</script>.<br>An elegant way to find the specific <script type="math/tex">\lambda</script> values is expectation-maximization(EM). </p><h2 id="Evaluating-language-modeling"><a href="#Evaluating-language-modeling" class="headerlink" title="Evaluating language modeling"></a>Evaluating language modeling</h2><p><strong>Extrinsic evaluation</strong>: the best way to evaluate the performance of a language model is to embed it in an application and measure how much the application, like machine translation, improves. However, such end-to-end evaluation is hard and often expensive.<br><strong>Intrinsic evaluation</strong>: intrinsic evaluation is task-neutral, which can be used to quickly evaluate potential improvements in a language model.<br>Note that for intrinsic evaluation, we have to use held out data, which is not used during training.</p><h3 id="Perplexity"><a href="#Perplexity" class="headerlink" title="Perplexity"></a>Perplexity</h3><p>Perplexity is an information theoretic measurement of how well a probability model predicts a sample. The lower perplexity, the better.</p><script type="math/tex; mode=display">\text{Perplex}(\boldsymbol{w})=2^{-\frac{1}{n}\sum_{i=1}^n\log_2\text{LM}(w_i|w_1^{i-1})}</script><p>Some special cases:</p><ul><li>In the limit of a perfect language model, probability <script type="math/tex">1</script> is assigned to the held-out corpus, with <script type="math/tex">\text{Perplex}(\boldsymbol{w})=2^{-\frac{1}{n}\log_{2}1}=2^0=1</script></li><li>In the opposite limit, probability zero is assigned to the held-out corpus, which corresponds to an infinite perplexity, <script type="math/tex">\text{Perplex}(\boldsymbol{w})=2^{-\frac{1}{n}\log_{2}1}=2^\infty=\infty</script></li><li>Assume a uniform, uniform model in which <script type="math/tex">\text{p}(w_i)=\frac{1}{V}</script> for all words in the vocabulary. Then,<script type="math/tex; mode=display">\log_{2}(\boldsymbol{w})=\sum_{i=1}^n\log_2\frac{1}{V}=-\sum_{i=1}^n\log_{2}V=-n\log_{2}V</script><script type="math/tex; mode=display">\text{Perplex}(\boldsymbol{w})=2^{\log_{2}V}=V</script></li></ul><p>The perplexity measure is a good indicator of the quality of a language model. However, in many cases improvement in perplexity scores do not transfer to improvement in extrinsic, task-quality scores. In that sense, the perplexity measure is good for comparing different language models in terms of their ability to pick-up regularities in sequences, but is not a good measure for assessing progress in language understanding or language-processing tasks. A model’s improvement in perplexity should always be confirmed by an end-to-end evaluation of a real task before concluding the evaluation of the model.</p><h3 id="Datasets"><a href="#Datasets" class="headerlink" title="Datasets"></a>Datasets</h3><p>A small benchmark dataset is the Penn Treebank, which contains roughly a million tokens; its vocabulary is limited to 10,000 words, with all other tokens mapped a special <script type="math/tex">\langle \text{UNK} \rangle</script> symbol.<br>A larger-scale language modeling dataset is the 1B word Benchmark, which contains text from Wikipedia.<br>I’ll complement this section after I read the relevant papers. Besides, the state-of-the-art leaderboards can be viewed <a href="https://paperswithcode.com/task/language-modeling" target="_blank" rel="noopener">here</a>.</p><h2 id="Neural-language-models"><a href="#Neural-language-models" class="headerlink" title="Neural language models"></a>Neural language models</h2><h2 id="Language-model-pretraining"><a href="#Language-model-pretraining" class="headerlink" title="Language model pretraining"></a>Language model pretraining</h2><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p>Jacob Eisenstein. 2018. Natural Language Processing (draft of November 13, 2018). <a href="https://github.com/jacobeisenstein/gt-nlp-class/tree/master/notes" target="_blank" rel="noopener">https://github.com/jacobeisenstein/gt-nlp-class/tree/master/notes</a>, pages 125-143.</p><p>Dan Jurafsky, James H. Martin. 2018. Speech and Language Processing (3rd ed. draft, Sep. 23, 2018). <a href="https://web.stanford.edu/%7Ejurafsky/slp3/" target="_blank" rel="noopener">https://web.stanford.edu/%7Ejurafsky/slp3/</a>, pages 37-62.</p><p>Yoav Goldberg. 2017. Neural Network Methods for Natural Language Processing. <a href="https://doi.org/10.2200/S00762ED1V01Y201703HLT037" target="_blank" rel="noopener">https://doi.org/10.2200/S00762ED1V01Y201703HLT037</a>, Pages 105-113.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Language modeling is the task of predicting the next word or character in a document. Language models were originally developed for the p
      
    
    </summary>
    
      <category term="Natural Language Processing" scheme="http://haelchan.me/categories/Natural-Language-Processing/"/>
    
    
      <category term="language modeling" scheme="http://haelchan.me/tags/language-modeling/"/>
    
  </entry>
  
  <entry>
    <title>Processing Corpus with Python</title>
    <link href="http://haelchan.me/2018/12/12/processing-corpus-with-python/"/>
    <id>http://haelchan.me/2018/12/12/processing-corpus-with-python/</id>
    <published>2018-12-12T10:58:56.000Z</published>
    <updated>2019-01-26T15:55:24.000Z</updated>
    
    <content type="html"><![CDATA[<p>Recently I’ve been doing a corpus-based research. There are several corpus tools available. Some are free, like <a href="http://www.laurenceanthony.net/software/antconc/" target="_blank" rel="noopener">AntConc</a>, <a href="http://corpora.lancs.ac.uk/lancsbox/" target="_blank" rel="noopener">#LancsBox</a>; and some are paid, like <a href="https://www.lexically.net/wordsmith/" target="_blank" rel="noopener">WordSmith</a> and <a href="https://www.powergrep.com" target="_blank" rel="noopener">PowerGREP</a>. The paid softwares are too expensive, and the free softwares just provide some general functions and cannot meet my requirements. Therefore, I decided to implement the processing procedure using Python on my own. <a href="http://www.nltk.org" target="_blank" rel="noopener">NLTK</a> offers methods to load corpus, but it doens’t support tagged corpus. So I just start it from pure Python. In this post, I’d keep my experience with code. I’ll try to make it as detailed as possible so that some of my best friends majoring in linguistics can benefit from this post and maybe process corpus with Python at their will :D  (Or perhaps you are a bit frustrated with code. Don’t worry. Later I will upload some helper Python programs so that you can execute them directly, without dealing with code.)</p><h2 id="Before-Getting-Started"><a href="#Before-Getting-Started" class="headerlink" title="Before Getting Started"></a>Before Getting Started</h2><h3 id="Jupyter-Notebook"><a href="#Jupyter-Notebook" class="headerlink" title="Jupyter Notebook"></a>Jupyter Notebook</h3><h4 id="Download-Anaconda"><a href="#Download-Anaconda" class="headerlink" title="Download Anaconda"></a>Download Anaconda</h4><p>It is okay whether you have known Python before or not. As long as you have learned some programming language, e.g., Java or C,  that’s enough. Python is user-friendly and easy to use. You can refer to <a href="https://docs.python.org/3/tutorial/index.html" target="_blank" rel="noopener">Python’s official tutorial</a> any time for better understanding, or you can just leave it out and follow my post step by step. Anyway, I’d suggest that you download <a href="https://www.anaconda.com/download/" target="_blank" rel="noopener">Anaconda</a>. It is straight-forward and helps to manage trouble-some settings. You can write code freely once you installed it. And I’d highly recommend writing in Jupyter Notebook, which is installed together with Anaconda.  </p><h4 id="Launch-Jupyter-Notebook"><a href="#Launch-Jupyter-Notebook" class="headerlink" title="Launch Jupyter Notebook"></a>Launch Jupyter Notebook</h4><ul><li><p>For Windows users, click Open Menu (or press <code>Windows key</code> on the keyboard) - Anaconda3 - Jupyter Notebook. </p></li><li><p>For macOS users, open Launchpad (or press <code>F4</code> on the keyboard) - Anaconda Navigator - notebook.</p></li></ul><p>After that, a command window (black in Windows, white in macOS) will pop up. You can ignore it (but don’t close it!). Then your default browser will open automatically. Click <em>New</em> on the top right corner and select <em>Python 3</em>, you can start writing Python code now!  </p><p><img src="/images/corpus/clickNew.jpg" alt></p><h4 id="Basic-of-Jupyter-Notebook"><a href="#Basic-of-Jupyter-Notebook" class="headerlink" title="Basic of Jupyter Notebook"></a>Basic of Jupyter Notebook</h4><p>The notebook consists of a sequence of cells. There are three types of cells: <strong>code cells</strong>, <strong>markdown cells</strong>, and <strong>raw cells</strong>. By default, the cell is code cell, and you can write Python code directly. You don’t need to worry about markdown or raw cells.  </p><p>The cell has two modes: <strong>command mode</strong> and <strong>edit mode</strong>. Generally, the cell is in edit mode and you can type freely. If it’s in command mode, you can navigate around the notebook using keyboard shortcuts.  </p><h4 id="Keyboard-Shortcuts"><a href="#Keyboard-Shortcuts" class="headerlink" title="Keyboard Shortcuts"></a>Keyboard Shortcuts</h4><p><code>Shift + Enter</code>: run cell  </p><p><code>Esc</code>: turn cell into command mode  </p><p><code>Enter</code>: turn cell into edit mode  </p><p><code>D, D</code>: delete the cell  </p><p><code>A</code>: insert cell above  </p><p><code>B</code>: insert cell below  </p><p>These are shortcuts that I use most frequently. For the full list of available shortcuts, click Help - Keyboard Shortcuts in the notebook menus.</p><p>Well, this is a simple introduction to Jupyter Notebook. You can download my <a href="https://github.com/HaelChan/Processing-Corpus-with-Python/archive/master.zip" target="_blank" rel="noopener">code</a> and open processing.ipynb using Jupyter Notebook, or you can continue reading this post and type the code on your own. The following content is almost the same.</p><h4 id="Shutdown"><a href="#Shutdown" class="headerlink" title="Shutdown"></a>Shutdown</h4><p>Oops, I almost forget to tell you how you shut down Jupyter Notebook when you’ve finished your work. Return to the command window, press <code>Ctrl+C</code> twice quickly. Then Jupyter Notebook will shut down and everything will be okay.</p><h3 id="Get-an-Overview-of-Your-Corpus"><a href="#Get-an-Overview-of-Your-Corpus" class="headerlink" title="Get an Overview of Your Corpus"></a>Get an Overview of Your Corpus</h3><p>You have to glance through your corpus before processing. Generally the format of corpus is <code>.txt</code> or <code>.xml</code>. You can use the default text editor in your operating system to open the file, or you can use some advanced text editor like <a href="https://www.sublimetext.com" target="_blank" rel="noopener">Sublime Text</a> for better view.</p><p><img src="/images/corpus/lcmc.jpg" alt></p><p>The above  image shows the heading of LCMC_A.xml. (<em>The Lancaster Corpus of Mandarin Chinese (LCMC)</em> addresses an increasing need within the research community for a publicly available balanced corpus of Mandarin Chinese. Click <a href="http://www.lancaster.ac.uk/fass/projects/corpus/LCMC/" target="_blank" rel="noopener">here</a> for more information about this corpus.) We don’t need to care the heading, which indicate the information about creator, date, etc. By focusing the main body, we can know each word is tagged with  <code>&lt;w POS=&quot;&quot;&gt;</code> and followed by <code>&lt;/w&gt;</code>, like <code>&lt;w POS=&quot;a&quot;&gt;大&lt;/w&gt;</code>. Different corpora may have different taggings, here (and in the following sections) I just take LCMC as an example.</p><h2 id="Process-a-Single-File"><a href="#Process-a-Single-File" class="headerlink" title="Process a Single File"></a>Process a Single File</h2><p>All right. Let’s get started with a single file. </p><h3 id="Read-the-File"><a href="#Read-the-File" class="headerlink" title="Read the File"></a>Read the File</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">filename = <span class="string">'LCMC_A.xml'</span></span><br><span class="line"><span class="keyword">with</span> open(filename, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    read_data = f.read()</span><br></pre></td></tr></table></figure><p>The first line declares a variable called <em>filename</em>. The second line tells the system to open the file. Note that <code>encoding=&#39;utf-8&#39;</code> indicates the encoding of the file, and currently <em>utf-8</em> is the most frequently used encoding. If you don’t know the file’s encoding, take utf-8 by default. Later I’ll write how to dealing with other encodings. The third line just read the entire content and pass it to a variable <em>read_data</em>.  </p><p>You can type <code>read_data</code> in a new cell and run the cell to see whether it’s successful.</p><h3 id="Extract-Useful-Parts"><a href="#Extract-Useful-Parts" class="headerlink" title="Extract Useful Parts"></a>Extract Useful Parts</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">tag = re.findall(<span class="string">r'&lt;w POS="(\w&#123;0,4&#125;)"&gt;.&#123;0,7&#125;&lt;/w&gt;'</span>, read_data)</span><br><span class="line">token = re.findall(<span class="string">r'&lt;w POS="\w&#123;0,4&#125;"&gt;(.&#123;0,7&#125;)&lt;/w&gt;'</span>, read_data)</span><br></pre></td></tr></table></figure><p>The most useful information is the tags and the tokens. With the above code we can extract these parts parallelly (in two seperate lists). Here I use the regular expression (re) and maybe it is a bit abstruce. Well, currently you just need to know what it does: recall the word form: <code>&lt;w POS=&quot;a&quot;&gt;大&lt;/w&gt;</code>, the second line extract the content between the quotes(<code>a</code>, which is a tag); the third line extract the content between the right angle bracket and the left angle bracket(<code>大</code>, which is a token).  </p><p>Note that with the above codes I just extract words, not including punctuations. If you need to treat punctuations as tokens, use the following code instead:  </p><p><code>tag = re.findall(r&#39;&lt;[wc] POS=&quot;(\w{0,4})&quot;&gt;.{0,7}&lt;/[wc]&gt;&#39;, read_data)</code>,  </p><p><code>token = re.findall(r&#39;&lt;[wc] POS=&quot;\w{0,4}&quot;&gt;(.{0,7})&lt;/[wc]&gt;&#39;, read_data)</code>.  </p><p>You can refer to my <a href="http://haelchan.me/2018/05/23/regular-expression/">post</a> for more information about regular expression.</p><h3 id="Count-Frequency"><a href="#Count-Frequency" class="headerlink" title="Count Frequency"></a>Count Frequency</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> collections</span><br><span class="line">counter = collections.Counter(token)</span><br><span class="line">counter.most_common()</span><br></pre></td></tr></table></figure><p>Since we have extract the useful part, we can count the frequencies. The first line import a library <code>collections</code> for counting. The second line counts the token list and the third line outputs the frequency in reverse order. </p><h3 id="Index-Target-Word"><a href="#Index-Target-Word" class="headerlink" title="Index Target Word"></a>Index Target Word</h3><p>Well, the freguency just gives us an overview. Then let’s focus on our target words. We need to know the word’s occurrence in the corpus. For example, the following line helps to find all indices of the word ‘是’.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">indices = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(token) <span class="keyword">if</span> x == <span class="string">'是'</span>]</span><br></pre></td></tr></table></figure><p>With the indices we get, now we can perform a variety of tasks. We can see the concordance of the word, n-gram, etc.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">pre_token = [token[i<span class="number">-1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> indices]</span><br><span class="line">next_token = [token[i+<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> indices]</span><br><span class="line"></span><br><span class="line">pre_tag = [tag[i<span class="number">-1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> indices]</span><br><span class="line">next_tag = [tag[i+<span class="number">1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> indices]</span><br></pre></td></tr></table></figure><p>The first two lines stores the tokens before ‘是’ and the tokens next ‘是’ separately, and the last two lines stores the tags before ‘是’ and the tags next ‘是’.</p><h3 id="KWIC"><a href="#KWIC" class="headerlink" title="KWIC"></a>KWIC</h3><p>Key Word In Context(KWIC) is a typical application in corpus linguistics. We can print KWIC easily with the indices we get:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> indices:</span><br><span class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> range(<span class="number">-5</span>, <span class="number">6</span>):</span><br><span class="line">        print(token[i+w], end=<span class="string">' '</span>)</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure><p>(Well, here the code is simplified, without considering OutOfIndex exception.)</p><h3 id="Export-to-Excel"><a href="#Export-to-Excel" class="headerlink" title="Export to Excel"></a>Export to Excel</h3><p>Maybe you are more familiar to Excel. You can export the statistics you get by Python as well.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">pd.DataFrame(counter.most_common()).to_excel(<span class="string">'most_common.xlsx'</span>, header=<span class="keyword">False</span>, index=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p>With these two lines of code executed, you can see that a new excel file is generated in your current directory. Then you can view the data in your Excel.</p><h2 id="Process-All-Files"><a href="#Process-All-Files" class="headerlink" title="Process All Files"></a>Process All Files</h2><h3 id="Load-Files"><a href="#Load-Files" class="headerlink" title="Load Files"></a>Load Files</h3><p>Generally, a corpus may contain several files. Now that we have done with a single file, it’s easy to  process all the files as well. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">files = [f <span class="keyword">for</span> f <span class="keyword">in</span> os.listdir() <span class="keyword">if</span> f.endswith(<span class="string">'.xml'</span>)]</span><br></pre></td></tr></table></figure><p>The above code will list the current directory’s files, whose filename extension is <code>.xml</code>. With these files, we can read all files within a <code>for</code> loop.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tags = []</span><br><span class="line">tokens = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> files:</span><br><span class="line">    <span class="keyword">with</span> open(filename, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    read_data = f.read()</span><br><span class="line">    tag = re.findall(<span class="string">r'&lt;w POS="(\w&#123;0,4&#125;)"&gt;.&#123;0,7&#125;&lt;/w&gt;'</span>, read_data)</span><br><span class="line">token = re.findall(<span class="string">r'&lt;w POS="\w&#123;0,4&#125;"&gt;(.&#123;0,7&#125;)&lt;/w&gt;'</span>, read_data)</span><br><span class="line">    tags.extend(tag)</span><br><span class="line">    tokens.extend(token)</span><br></pre></td></tr></table></figure><p>Here the <code>tag</code> and <code>token</code> is just the same as what was mentioned in the previous section. The difference lies in that I declared two new variables, <code>tags</code> and <code>tokens</code> to store the entire tags and tokens in the corpus (while the <code>tag</code> and <code>token</code> only store those in only one file). The last two lines just merge the <code>tag</code> to <code>tags</code>.  </p><h3 id="Define-Functions"><a href="#Define-Functions" class="headerlink" title="Define Functions"></a>Define Functions</h3><p>Some lines of code have its particular purpose and may be executed several times. It’s a good idea to wrap those code in a <em>function</em> for reutilization. It helps to simplify our programs and make it easier to use and comprehend.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">previous_tokens</span><span class="params">(target)</span>:</span></span><br><span class="line">    indices = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(token) <span class="keyword">if</span> x == target]</span><br><span class="line">    pre_token = [token[i<span class="number">-1</span>] <span class="keyword">for</span> i <span class="keyword">in</span> indices]</span><br><span class="line">    <span class="keyword">return</span> pre_token</span><br></pre></td></tr></table></figure><p>For example, we define a function called <code>previous_tokens</code>. With the above board written, we can call it easily. <code>previous_tokens(&#39;的&#39;)</code> will give us the previous tags of ‘的’.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Recently I’ve been doing a corpus-based research. There are several corpus tools available. Some are free, like &lt;a href=&quot;http://www.laure
      
    
    </summary>
    
    
      <category term="tips" scheme="http://haelchan.me/tags/tips/"/>
    
      <category term="Python" scheme="http://haelchan.me/tags/Python/"/>
    
      <category term="linguistics" scheme="http://haelchan.me/tags/linguistics/"/>
    
  </entry>
  
  <entry>
    <title>Points on PyTorch</title>
    <link href="http://haelchan.me/2018/12/08/pytorch-note/"/>
    <id>http://haelchan.me/2018/12/08/pytorch-note/</id>
    <published>2018-12-08T15:29:28.000Z</published>
    <updated>2019-04-06T02:14:56.000Z</updated>
    
    <content type="html"><![CDATA[<p>Today, December 8th, 2018, PyTorch 1.0 stable has been released. It is a milestone and I’d like to keep notes on PyTorch as I learn and use PyTorch. The resource mainly comes from <a href="https://pytorch.org/tutorials/index.html" target="_blank" rel="noopener">PyTorch official tutorial</a> and <a href="https://www.udacity.com/course/deep-learning-pytorch--ud188" target="_blank" rel="noopener">Intro to Deep Learning with PyTorch</a> on Udacity.  </p><h3 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h3><p>Simply put, <code>TENSORS</code> are a generalization of vectors and matrices. In PyTorch, they are a multi-dimensional matrix containing elements of a single data type.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># From the slide PyTorch under the hood</span></span><br><span class="line"><span class="comment"># by Christian S. Perone</span></span><br><span class="line"><span class="comment"># https://speakerdeck.com/perone/pytorch-under-the-hood?slide=12</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">1.</span>, <span class="number">-1.</span>], [<span class="number">1.</span>, <span class="number">-1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([[ <span class="number">1.</span>, <span class="number">-1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>, <span class="number">-1.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.dtype <span class="comment"># They have a type</span></span><br><span class="line">torch.float32</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.shape <span class="comment"># a shape</span></span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.device <span class="comment"># and live in some device</span></span><br><span class="line">device(type=<span class="string">'cpu'</span>)</span><br></pre></td></tr></table></figure><p><strong>Resizing the tensor</strong>  </p><p>There are a few options to use: <code>.reshape()</code>, <code>.resize()</code> and <code>.view()</code>.</p><ul><li><code>w.reshape(a, b)</code> will return a new tensor with the same data as <code>w</code> with size <code>(a, b)</code>sometimes, and sometimes a clone, as in it copies the data to another part of memory</li><li><code>w.resize_(a, b)</code> returns the same tensor with a different shape. However, if the new shape results in fewer elements than the original tensor, some elements will be removed from the tensor (but not from memory). If the new shape results in more elements than the original tensor, new elements will be uninitialized in memory.</li><li><code>w.view(a, b)</code> will return a new tensor with the same data as <code>w</code> with size <code>(a, b)</code></li></ul><p>The above three methods are introduced in Intro to Deep Learning with PyTorch. PyTorch official tutorial only introduces <code>w.view()</code>, so generally I’d use <code>w.view()</code> for resizing.  </p><p><strong>Convenience of -1</strong>  </p><p>When resizing the tensor, <code>-1</code> is helpful to determine the only unknown size when we already know the other sizes. It can be inferred from other dimensions.</p><p>E.g.,</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line">y = x.view(<span class="number">-1</span>, <span class="number">8</span>)</span><br></pre></td></tr></table></figure><p>The size of y is <code>torch.Size([2, 8])</code>, just as we want.  </p><p><strong>In-place operation</strong>  </p><p>An in-place operation is an operation that changes directly the content of a given tensor without making a copy. In-place operations in PyTorch are always postfixed with a <code>_</code>, like <code>.add_()</code>. The <code>.resize_()</code> mentioned above is also an in-place operation.  </p><p><strong>NumPy to Torch and back</strong>  </p><p>PyTorch has a great feature for converting between NumPy arrays and Torch tensors. To create a tensor from a NumPy array, use <code>torch.from_numpy()</code>. To convert a tensor to a NumPy array, use the <code>.numpy()</code> method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.random.rand(<span class="number">4</span>, <span class="number">3</span>)</span><br><span class="line">b = torch.from_numpy(a)</span><br><span class="line">b.numpy()</span><br></pre></td></tr></table></figure><p>The memory is shared between the NumPy array and Torch tensor.  </p><p><strong>sum() method</strong>  </p><p>Setting the <code>dim</code> keyword <code>dim=0</code> takes the sum across the rows, i.e., compute the sum of the column vector. Similarly, <code>dim=1</code> takes the sum across the columns (compute the sum of the row vector).</p><h3 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h3><p>The general process with PyTorch:  </p><ul><li>Make a forward pass through the network</li><li>Use the network output to calculate the loss</li><li>Perform a backward pass through the network with <code>loss.backward()</code> to calculate the gradients</li><li>Take a step with the optimizer to update the weights</li></ul><p><strong>Constructing Neural Networks</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Network</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.hidden = nn.Linear(<span class="number">784</span>, <span class="number">256</span>)</span><br><span class="line">        self.output = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.sigmoid(self.hidden(x))</span><br><span class="line">        x = F.softmax(self.output(x), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><p>It is mandatory to inherit from <code>nn.Module</code> when creating a class for our network. The name of the class itself can be anything.  </p><p>PyTorch networks created with <code>nn.Module</code> must have a <code>forward</code> method defined. It takes in a tensor <code>x</code> and passes it through the operations you defined in the <code>__init__</code> method. And the <code>backward</code> function (where gradients are computed) is automatically defined for you using <code>autograd</code>.  </p><p>Another way is mentioned in the course: <code>nn.Sequential</code>. (See <a href="https://pytorch.org/docs/master/nn.html#torch.nn.Sequential" target="_blank" rel="noopener">Doc</a> in detail)  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">model = nn.Sequential(nn.Linear(input_size, hidden_sizes[<span class="number">0</span>]),</span><br><span class="line">                      nn.ReLU(),</span><br><span class="line">                      nn.Linear(hidden_sizes[<span class="number">0</span>], hidden_sizes[<span class="number">1</span>]),</span><br><span class="line">                      nn.ReLU(),</span><br><span class="line">                      nn.Linear(hidden_sizes[<span class="number">1</span>], output_size),</span><br><span class="line">                      nn.Softmax(dim=<span class="number">1</span>))</span><br></pre></td></tr></table></figure><p><strong>Loss</strong>  </p><p>PyTorch provides losses such as the cross-entropy loss (<code>nn.CrossEntropyLoss</code>). You’ll usually see the loss assigned to <code>criterion</code>. This criterion combines <code>nn.LogSoftmax()</code> and <code>nn.NLLLoss()</code> in one single class.  </p><p>The input is expected to contain scores for each class.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>loss = nn.CrossEntropyLoss()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>input = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>target = torch.empty(<span class="number">3</span>, dtype=torch.long).random_(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = loss(input, target)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output.backward()</span><br></pre></td></tr></table></figure><p><strong>Autograd</strong>  </p><p>The <code>autograd</code> package provides automatic differentiation for all operations on Tensors. If the attribute <code>requires_grad</code> of <code>torch.Tensor</code> is set as <code>True</code>, it starts to track all operations on it. When you finished your computation you can call <code>.backward()</code> and have all the gradients computed automatically. The gradient for this tensor will be accumulated into <code>.grad</code> attribute.  </p><p>For more information, refer to <a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html" target="_blank" rel="noopener">autograd tutorial</a> and <a href="https://pytorch.org/docs/stable/autograd.html" target="_blank" rel="noopener">autograd doc</a>.</p><h3 id="Testing-and-Validating"><a href="#Testing-and-Validating" class="headerlink" title="Testing and Validating"></a>Testing and Validating</h3><p>The most common method to reduce overfitting is dropout, where we randomly drop input units. Adding dropout in PyTorch is straightforward using the <code>nn.Dropout</code> module.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">self.dropout = nn.Dropout(p=<span class="number">0.2</span>)</span><br></pre></td></tr></table></figure><p>Turning off gradient descent when tesing or validating can help accelerate the process. Generally we can do the testing in the following mode:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad()</span><br><span class="line">    <span class="comment"># testing</span></span><br></pre></td></tr></table></figure><p>During training we want to use dropout to prevent overfitting, but during inference we want to use the entire network. So, we need to turn off dropout during validation, testing, and whenever we’re using the network to make predictions. To do this, you use <code>model.eval()</code>. This sets the model to evaluation mode where the dropout probability is 0. You can turn dropout back on by setting the model to train mode with <code>model.train()</code>.</p><h3 id="Convolutional-Neural-Network"><a href="#Convolutional-Neural-Network" class="headerlink" title="Convolutional Neural Network"></a>Convolutional Neural Network</h3><p><strong>Convolutional Layer</strong><br><a href="https://pytorch.org/docs/stable/nn.html#conv2d" target="_blank" rel="noopener">Conv2d Documentation</a><br>We typically define a convolutional layer in PyTorch using <code>nn.Conv2d</code>:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0)</span><br></pre></td></tr></table></figure></p><ul><li><code>in_channels</code> number of channels in the input image. For a grayscale image, this depth = 1; for a RGB image, this depth = 3.</li><li><code>out_channels</code> number of channels produced by the convolution</li><li><code>kernel_size</code> size of the convolutional kernel (most commonly 3 for a 3x3 kernel)</li><li><code>stride</code> stride of the convolution (default: 1, a single number or a tuple)</li><li><code>padding</code> zero-padding added to both sides of the input (default: 0, a number or a tuple)</li></ul><p><strong>Pooling Layer</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nn.MaxPool2d(kernel_size, stride=None)</span><br><span class="line">nn.AvgPool2d(kernel_size, stride=None)</span><br></pre></td></tr></table></figure></p><ul><li><code>kernel_size</code> the size of the window to take a max over</li><li><code>stride</code> the stride of the window (default value is <code>kernel_size</code>)</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Today, December 8th, 2018, PyTorch 1.0 stable has been released. It is a milestone and I’d like to keep notes on PyTorch as I learn and u
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://haelchan.me/categories/Deep-Learning/"/>
    
    
      <category term="learning note" scheme="http://haelchan.me/tags/learning-note/"/>
    
      <category term="PyTorch" scheme="http://haelchan.me/tags/PyTorch/"/>
    
  </entry>
  
  <entry>
    <title>Step into Neural Machine Translation</title>
    <link href="http://haelchan.me/2018/11/01/step-into-nmt/"/>
    <id>http://haelchan.me/2018/11/01/step-into-nmt/</id>
    <published>2018-11-01T13:50:16.000Z</published>
    <updated>2018-12-09T15:53:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>This post is mainly the reading note of the tutorial <a href="https://arxiv.org/abs/1703.01619" target="_blank" rel="noopener">Neural Machine Translation and Sequence-to-sequence Models: A Tutorial</a> and the solutions to its questions(maybe code, if possible).  </p><h2 id="Concepts"><a href="#Concepts" class="headerlink" title="Concepts"></a>Concepts</h2><p><strong>Machine translation</strong> is the technology used to translate between human language.  </p><p><strong>Source language</strong>: the language input to the machine translation system  </p><p><strong>Target language</strong>: the output language  </p><p>Machine learning can be described as the task of converting a sequence of words in the source, and converting into a sequence of words in the target.  </p><p><strong>Sequence-to-sequence models</strong> refers to the broader class of models that include all models that map one sequence to another. E.g., machine translation, tagging, dialog, speech recognition, etc.  </p><h3 id="Statistical-Machine-Learning"><a href="#Statistical-Machine-Learning" class="headerlink" title="Statistical Machine Learning"></a>Statistical Machine Learning</h3><p>We define our task of machine learning as translating a source sentence <script type="math/tex">F=f_1,...,f_J=f_1^{|F|}</script> into a target sentence <script type="math/tex">E=e_1,...,e_I=e_1^{|E|}</script>. Here, the subscript <script type="math/tex">_1</script> at the end of the equations may be a bit misleading (at least for me at the first glance).  It means that the identity of the first word in the sentence is <script type="math/tex">e_1</script>, the identity of the second word in the sentence is <script type="math/tex">e_2</script>, up until the last word in the sentence being <script type="math/tex">e_{|E|}</script>.  </p><p>Then, translation system can be defined as a function <script type="math/tex">\hat{E}=\text{mt}(F)</script>, which returns a translation hypothesis <script type="math/tex">\hat{E}</script> given a source sentence <script type="math/tex">F</script> as input.  </p><p><strong>Statistical machine translation</strong> systems are systems that perform translation by creating a probabilistic model for the probability of <script type="math/tex">E</script> given <script type="math/tex">F</script>, <script type="math/tex">P(E|F;\theta)</script>, and finding the target sentence that maximizes the probability</p><script type="math/tex; mode=display">\hat{E}=\underset{E}{\operatorname{argmax}}P(E|F;\theta)</script><p>, where <script type="math/tex">\theta</script> are the parameters of the model specifying the probability distribution.  </p><p>The parameters <script type="math/tex">\theta</script> are learned from data consisting of aligned sentences in the source and target languages, which are called parallel corpora.</p><h2 id="n-gram-Language-Models"><a href="#n-gram-Language-Models" class="headerlink" title="n-gram Language Models"></a><em>n</em>-gram Language Models</h2><p>Instead of calculating the original joint probability <script type="math/tex">P(E)=P(|E|=T,e_1^T)</script>, it’s more manageable to calculate by multiplying together conditional probabilities for each of its elements: </p><script type="math/tex; mode=display">P(E)=\prod_{t=1}^{T+1}P(e_t|e_1^{t-1})</script><p>where $e_{T+1}=\langle/s\rangle$. It’s an implicit <em>sentence end</em> symbol, which we will indicate when we have terminated the sentence. By examining the position of the $\langle/s\rangle$ symbol, we can determine whether <script type="math/tex">|E|=T</script>.  </p><p>Then how to calculate the next word given the previous words <script type="math/tex">P(e_t|e_1^{t-1})</script>? The first way is simple: prepare a set of training data from which we can count word strings, count up the number of times we have seen a particular string of words, and divide it by the number of times we have seen the context.</p><script type="math/tex; mode=display">P_{ML(e_t|e_t^{t-1})}=\frac{c_{prefix}(e_1^t)}{c_{prefix}(e_1^{t-1})}</script><p>Here <script type="math/tex">c_{prefix}(\cdot)</script> is the count of the number of times this particular word string appeared at the beginning of a sentence in the training data.   </p><p>However, this language model will assign a probability of zero to every sentenec that it hasn’t seen before in the training corpus, which is not very useful.  </p><p>To solve the problem, we set a fixed window of previous words upon which we will base our probability calculations instead of calculating probabilities from the beginning of the sentence. If we limit our context to <script type="math/tex">n-1</script> previous words, this would amount to:</p><script type="math/tex; mode=display">P(e_t|e_1^{t-1})\approx P_{ML}(e_t|e_{t-n+1}^{t-1})</script><p>Models that make this assumption are called <em>n-**</em>gram models**. Specifically, when models where <script type="math/tex">n=1</script> are called unigram models, <script type="math/tex">n=2</script> bigram models, <script type="math/tex">n=3</script> trigram models, etc.  </p><p>In the simplest form, the parameters <script type="math/tex">\theta</script> of <em>n</em>-gram models consist of probabilities of the next word given <script type="math/tex">n-1</script> previous words can be calculated using maximum likelihood estimation as follows:</p><script type="math/tex; mode=display">\theta_{e_{t-n+1}^t}=P_{ML}(e_t|e_{t-n+1}^{t-1})=\frac{c(e_{t-n+1}^t)}{e_{t-n+1}^{t-1}}</script><h3 id="Smoothing"><a href="#Smoothing" class="headerlink" title="Smoothing"></a>Smoothing</h3><p>However, what if we encounter a two-word string that has never appeared in the training corpus? <em>n</em>-gram models fix this problem by <strong>smoothing</strong> probabilities, combining the maximum likelihood estimates for various values of <em>n</em>.  In the simple case of smoothing unigram and bigram probabilities, we can think of a model that combines together the probabilities as follows:</p><script type="math/tex; mode=display">P(e_t|e_{t-1})=(1-\alpha)P_{ML}(e_t|e_{t-1})+\alpha P_{ML}(e_t)</script><p>where <script type="math/tex">\alpha</script> is a variable specifying how much probability mass we hold out for the unigram distribution. As long as $\alpha&gt;0$, all the words in our vocabulary will be assigned some probability. This method is called <strong>interpolation</strong>, and is one of the standard ways to make probabilistic models more robust to low-frequency phenomena.  </p><p>Some more sophisticated methods for smoothing: Context-dependent smoothing coefficients, Back-off, Modified distributions, Modified Kneser-Ney smoothing.</p><h3 id="Evaluation"><a href="#Evaluation" class="headerlink" title="Evaluation"></a>Evaluation</h3><p><strong>Likelihood</strong>  </p><script type="math/tex; mode=display">P(\mathcal{E}_{test};\theta)=\prod_{E\in\mathcal{E}_{test}}P(E;\theta)</script><p>The most straight-forward way of defining accuracy is the likelihood of the model with respect to the development or test data. The likelihood of the parameters <script type="math/tex">\theta</script> with respect to this data is equal to the probability that the model assigns to the data. </p><p><strong>Log likelihood</strong>  </p><script type="math/tex; mode=display">\log P(\mathcal{E_{test};\theta})=\sum_{E\in\mathcal{E}_{test}}\log P(E;\theta)</script><p>The log likelihood is used for a couple reasons. The first is because the probability of any particular sentence according to the language model can be a very small number, and the product of these small numbers can become a very small number that will cause numerical precision problems on standard computing hardware. The second is because sometimes</p><ol><li>product of small probability leads to very small number</li><li>mathematically more convenient</li></ol><p><strong>Perplexity</strong>  </p><script type="math/tex; mode=display">\text{ppl}(\mathcal{E}_{test};\theta)=e^{-(\log P(\mathcal{E}_{test};\theta))/\text{length}(\mathcal{E_{test}})}</script><p>An intuitive explanation of the perplexity is “how confused is the model about its decision?” More accurately, it expresses the value “if we randomly picked words from the probability distribution calculated by the language model at each time step, on average how many words would it have to pick to get the correct one?”  </p><h3 id="Handling-Unknown-Words"><a href="#Handling-Unknown-Words" class="headerlink" title="Handling Unknown Words"></a>Handling Unknown Words</h3><ul><li>Assume closed vocabulary</li><li>Interpolate with an unknown words distribution</li><li>Add an <script type="math/tex">\langle \text{unk}\rangle</script> word</li></ul><hr><p>Further reading includes large-scale language modeling, language model adaptation, longer-distance language count-based models, syntex-based language models.   </p><p><a href="http://www.cs.columbia.edu/~mcollins/lm-spring2013.pdf" target="_blank" rel="noopener">Course note</a> by Michael Collins from Columbia Univeristy is another good material.</p><h2 id="Log-linear-Langauge-Models"><a href="#Log-linear-Langauge-Models" class="headerlink" title="Log-linear Langauge Models"></a>Log-linear Langauge Models</h2><h3 id="Model-Formulation"><a href="#Model-Formulation" class="headerlink" title="Model Formulation"></a>Model Formulation</h3><p>Log-linear language models revolve around the concept of <strong>features</strong>. We define a feature function $\phi(e_{t-n+1}^{t-1})$ that takes a context as input, and outputs a real-valued <strong>feature vector</strong> <script type="math/tex">\boldsymbol{x}\in\mathbb{R}^N</script> that describe the context using <em>N</em> different features.  </p><p>Feature vector can be a <strong>one-hot vector</strong>. The function <script type="math/tex">\text{onehot}(i)</script> returns a vector where only the <script type="math/tex">i\text{th}</script> element is one and the rest are zero (assume the length of the vector is the appropriate length given the context).  </p><p><strong>Calculating scores</strong>: We calculate a score vector <script type="math/tex">\boldsymbol{s}\in\mathbb{R}^{|V|}</script> that corresponds to the likelihood of each word: words with higher scores in the vector will also have higher probabilities. The model parameters <script type="math/tex">\theta</script> specifically come in two varieties: a <strong>bias vector</strong> <script type="math/tex">\boldsymbol{b}\in\mathbb{R}^{|V|}</script>, which tells us how likely each word in the vocabulary is overall, and a <strong>weight matrix</strong> <script type="math/tex">\mathit{W}=\mathbb{R}^{|V|\times N}</script>, which describes the relationship between feature values and scores. </p><script type="math/tex; mode=display">\boldsymbol{s}=\mathit{W}\boldsymbol{x}+\boldsymbol{b}</script><p>To make computation more efficient (because many elements are zero in one-hot vectors or other sparse vectors), we can add together the columns of the weight matrix for all active (non-zero) features:</p><script type="math/tex; mode=display">\boldsymbol{s}=\sum_{\{j:x_j\ne0\}}\mathit{W}_{\cdot,j}x_j+\boldsymbol{b},</script><p>where <script type="math/tex">\mathit{W}_{\cdot,j}</script> is the <script type="math/tex">j\text{th}</script> column of <script type="math/tex">\mathit{W}</script>.   </p><p><strong>Calculating probabilities</strong>: </p><script type="math/tex; mode=display">p_j=\frac{\exp{(s_j)}}{\sum_\tilde{j}\exp(s_\tilde{j})}</script><p>By taking the exponent and dividing by the sum of the values over the entire vocabulary, these scores can be turned into probabilities that are between 0 and 1 and sum to 1. This function is called the <strong>softmax</strong> function, and often expressed in vector form as follows:</p><script type="math/tex; mode=display">\boldsymbol{p}=\text{softmax}(\boldsymbol{s})</script><h3 id="Learning-Model-Parameters"><a href="#Learning-Model-Parameters" class="headerlink" title="Learning Model Parameters"></a>Learning Model Parameters</h3><p>Sentence level loss function</p><script type="math/tex; mode=display">\ell(\mathcal{E_{train},\theta})=\log P(\mathcal{E}_{train}|\theta)=-\sum_{E\in\mathcal{E}_{train}}\log P(E|\theta)</script><p>Per-word level</p><script type="math/tex; mode=display">\ell(e_{t-n+1}^t,\theta)=\log P(e_t|e_{t-n+1}^{t-1})</script><p>(In the original tutorial, the equation is as below. But I guess it should be $-\log P(e<em>t|e</em>(t-n+1)^(t-1))$?)</p><p><strong>Methods for SGD</strong></p><p>Adjusting the learning rate, early stopping, shuffling training order, SGD with momentum, AdaGrad, Adam. (Common methods in machine learning)</p><p><strong>Derivative</strong></p><script type="math/tex; mode=display">\frac{d\ell(e_{t-n+1}^t,\theta)}{d\theta}</script><script type="math/tex; mode=display">\theta\larr\theta-\eta\frac{d\ell(e_{t-n+1}^t,\theta)}{d\theta}</script><p>Stepping through the full loss function in one pass:</p><script type="math/tex; mode=display">\boldsymbol{x}=\phi(e_{t-n+1}^{t-1})</script><script type="math/tex; mode=display">\boldsymbol{s}=\sum_{\{j:x_j\ne0\}}\mathit{W}_{\cdot,j}x_j+\boldsymbol{b}</script><script type="math/tex; mode=display">\boldsymbol{p}=\text{softmax}(\boldsymbol{s})</script><script type="math/tex; mode=display">\ell=-\log\boldsymbol{p}_{e_t}</script><p>Using the chain rule:</p><script type="math/tex; mode=display">\frac{d\ell(e_{t-n+1}^t,W,\boldsymbol{b})}{d\boldsymbol{b}}=\frac{d\ell}{d\boldsymbol{p}}\frac{d\boldsymbol{p}}{d\boldsymbol{s}}\frac{d\boldsymbol{s}}{d\boldsymbol{b}}=\boldsymbol{p}-\text{onehot}(e_t)</script><script type="math/tex; mode=display">\frac{d\ell(e_{t-n+1}^t,W,\boldsymbol{b})}{dW_{\cdot,j}}=\frac{d\ell}{d\boldsymbol{p}}\frac{d\boldsymbol{p}}{d\boldsymbol{s}}\frac{d\boldsymbol{s}}{dW_{\cdot,j}}=x_j(\boldsymbol{p}-\text{onehot}(e_t))</script><hr><p>One reason why log-linear models are nice is because they allow us to flexibly design features that we think might be useful for predicting the next word. Features include <em>Context word features, Context class, Context suffix features, Bag-of-words features,</em> etc.  </p><p>Further reading includes <em>Whole-sentence language models, Discriminative language models</em>.</p><h2 id="Neural-Networks-and-Feed-forward-Language-Models"><a href="#Neural-Networks-and-Feed-forward-Language-Models" class="headerlink" title="Neural Networks and Feed-forward Language Models"></a>Neural Networks and Feed-forward Language Models</h2><p>Multi-layer perceptrons(MLPs) consist one or more hidden layers that consist of an affine transform (a fancy name for a multiplication followed by an addition) followed by a non-linear function (step function, tanh, relu, etc), culminating in an output layer that calculates some variety of output.  </p><p>Neural networks can be thought of as a chain of functions that takes some input and calculates some desired output. The power of neural networks lies in the fact that chaining together a variety of simpler functions makes it possible to represent more complicated functions in an easily trainable, parameter-efficient way. </p><h3 id="Training-Neural-Networks"><a href="#Training-Neural-Networks" class="headerlink" title="Training Neural Networks"></a>Training Neural Networks</h3><p>Calculating loss function:</p><script type="math/tex; mode=display">\boldsymbol{h}'=W_{xh}\boldsymbol{x}+\boldsymbol{b}_h</script><script type="math/tex; mode=display">\boldsymbol{h}=\tanh(\boldsymbol{h}')</script><script type="math/tex; mode=display">y=\boldsymbol{w}_{hy}\boldsymbol{h}+b_y</script><script type="math/tex; mode=display">\ell=(y^*-y)^2</script><p>The derivatives:</p><script type="math/tex; mode=display">\frac{d\ell}{db_y}=\frac{d\ell}{dy}\frac{d_y}{db_y}</script><script type="math/tex; mode=display">\frac{d\ell}{d\boldsymbol{w}_{hy}}=\frac{d\ell}{dy}\frac{dy}{d\boldsymbol{w}_{hy}}</script><script type="math/tex; mode=display">\frac{d\ell}{d\boldsymbol{b}_h}=\frac{d\ell}{dy}\frac{dy}{d\boldsymbol{h}}\frac{d\boldsymbol{h}}{d\boldsymbol{h}'}\frac{d\boldsymbol{h}'}{d\boldsymbol{b}_h}</script><script type="math/tex; mode=display">\frac{d\ell}{dW_{xh}}=\frac{d\ell}{dy}\frac{dy}{d\boldsymbol{h}}\frac{d\boldsymbol{h}}{d\boldsymbol{h}'}\frac{d\boldsymbol{h}'}{dW_{xh}}</script><p>We could go through all of the derivations above by hand and precisely calculate the gradients of all parameters in the model. But even for a simple model like the one above, it is quite a lot of work and error prone. Fortunately, when we actually implement neural networks on a computer, there is a very useful tool that saves us a large portion of this pain: <strong>automatic differentiation</strong> (autodiff). To understand automatic differentiation, it is useful to think of our computation in a data structure called a <strong>computation graph</strong>. As shown in the following figure (figure 10 from the original paper), each node represents either an input to the network or the result of one computational operation, such as a multiplication, addition, tanh, or squared error. The first graph in the figure calculates the function of interest itself and would be used when we want to make predictions using our model, and the second graph calculates the loss function and would be used in training.</p><p><img src="/images/nmt/ComputationGraph.jpg" alt></p><p>Automatic differentiation is a two-step dynamic programming algorithm that operates over the second graph and performs:</p><ul><li><strong>Forward calculation</strong>, which traverses the nodes in the graph in topological order, calculating the actual result of the computation</li><li><strong>Back propagation</strong>, which traverses the nodes in reverse topological order, calculating the gradients</li></ul><h3 id="Neural-network-Language-Models"><a href="#Neural-network-Language-Models" class="headerlink" title="Neural-network Language Models"></a>Neural-network Language Models</h3><p><img src="/images/nmt/NNLM.jpg" alt></p><p>A tri-gram neural network model with a single layer is structured as shown in the figure above.</p><script type="math/tex; mode=display">\boldsymbol{m}=\text{concat}(M_{\cdot,e_{t-2}},M_{\cdot,e_{t-1}})</script><script type="math/tex; mode=display">\boldsymbol{h}=\tanh(W_{mh}\boldsymbol{m}+\boldsymbol{b}_h)</script><script type="math/tex; mode=display">\boldsymbol{s}=W_{hs}\boldsymbol{h}+\boldsymbol{b}_s</script><script type="math/tex; mode=display">\boldsymbol{p}=\text{softmax}(\boldsymbol{s})</script><p>In the first line, we obtain a vector <script type="math/tex">\boldsymbol{m}</script> representing the context $e_{i-n+1}^{i-1}$. Here, <em>M</em> is a matrix with <script type="math/tex">|V|</script> columns and <script type="math/tex">L_m</script> rows, where each column corresponds to an <script type="math/tex">L_m</script>-length vector representing a single word in the vocabulary. This vector is called a <strong>word embedding</strong> or a <strong>word representation</strong>, which is a vector of real numbers corresponding to particular words in the vocabulary.  </p><p>The vecror <script type="math/tex">\boldsymbol{m}</script> then results from the concatenation of the word vectors for all of the words in the context, so <script type="math/tex">|\boldsymbol{m}|=L_m*(n-1)</script>. Once we have this <script type="math/tex">\boldsymbol{m}</script>, we run the vectors through a hidden layer to obtain vector <script type="math/tex">\boldsymbol{h}</script>. By doing so, the model can learn combination features that reflect information regarding multiple words in the context.  </p><p>Next, we calculate the score vector for each word: <script type="math/tex">\boldsymbol{s}\in\mathbb{R}^{|V|}</script>. This is done by performing an affine transform of the hidden vector <script type="math/tex">\boldsymbol{h}</script> with a weight matrix <script type="math/tex">W_{hs}\in\mathbb{R}^{|V|\times|\boldsymbol{h}|}</script> and adding a bias vector <script type="math/tex">\boldsymbol{b}_s\in\mathbb{R}^{|V|}</script>. Finally, we get a probability estimate <script type="math/tex">\boldsymbol{p}</script> by running the calculated scores through a softmax function, like we did in the log-linear language models. For training, if we know <script type="math/tex">e_t</script> we can also calculate the loss function <script type="math/tex">\ell=-\log(p_{e_t})</script>.  </p><p>The advantage of neural network formulation: Better generalization of contexts, More generalizable combination of words into contexts and Ability to skip previous words.  </p><p>Further reading includes <em>Softmax approximations</em>, <em>Other softmax structures</em> and <em>Other models to learn word representations</em>.  </p><h2 id="Recurrent-Neural-Network-Language-Models"><a href="#Recurrent-Neural-Network-Language-Models" class="headerlink" title="Recurrent Neural Network Language Models"></a>Recurrent Neural Network Language Models</h2><p>Language models based on recurrent neural networks (RNNs) have the ability to capture long-distance dependencies in language.  </p><p>Some examples of long-distance dependencies in language: reflexive form (himself, herself) should match the gender, the conjugation of the verb based on the subject of the sentence, selectional preferences, topic and register.  </p><p>Recurrent neural networks are a variety of neural network that makes it possible to model these long-distance dependencies. The idea is simply to add a connection that references the previous hidden state <script type="math/tex">\boldsymbol{h}_{t-1}</script> when calculating hidden state <script type="math/tex">\boldsymbol{h}</script>.</p><script type="math/tex; mode=display">\boldsymbol{h}_t=\begin{cases}\tanh(W_{xh}\boldsymbol{x}_t+W_{hh}\boldsymbol{h}_{t-1}+\boldsymbol{b}_h)&t\ge1,\\0&\text{otherwise}.\end{cases}</script><p>For time steps <script type="math/tex">t\ge1</script>, the only difference from the hidden layer in a standard neural network is the addition of the connection <script type="math/tex">W_{hh}\boldsymbol{h}_{t-1}</script> form the hidden state at time step <script type="math/tex">t-1</script> connecting to that at time step <script type="math/tex">t</script>. </p><p><img src="/images/nmt/RNN.jpg" alt></p><p>RNNs make it possible to model long distance dependencies  because they have the ability to pass information between timesteps. For example, if some of the nodes in <script type="math/tex">\boldsymbol{h}_{t-1}</script> encode the information that “the subject of the sentence is male”, it is possible to pass on this information to <script type="math/tex">\boldsymbol{h}_t</script>, which can in turn pass it on to $\boldsymbol{h}_{t+1}$ and on to the end of the sentence. This ability to pass information across an arbitrary number of consecutive time steps is the strength of recurrent neural networks, and allows them to handle the long-distance dependencies.  </p><p>Feed-forward language model:</p><script type="math/tex; mode=display">\boldsymbol{m}_t=M_{\cdot,e_{t-1}}</script><script type="math/tex; mode=display">\boldsymbol{h}_t=\text{RNN}(\boldsymbol{m}_t,\boldsymbol{h}_{t-1})</script><script type="math/tex; mode=display">\boldsymbol{p}_t=\text{softmax}(W_{hs}\boldsymbol{h}_t+b_s)</script><h3 id="The-Vanishing-Gradient-and-Long-Short-term-Memory"><a href="#The-Vanishing-Gradient-and-Long-Short-term-Memory" class="headerlink" title="The Vanishing Gradient and Long Short-term Memory"></a>The Vanishing Gradient and Long Short-term Memory</h3><p>The <strong>vanishing gradient</strong> problem and the <strong>exploding gradient</strong> problem are the problems that simple RNNs are facing. The gradient in back propagation will gets smaller and smaller if <script type="math/tex">\frac{d\boldsymbol{h}_{t-1}}{d\boldsymbol{t}}\lt1</script> and then diminish the gradient <script type="math/tex">\frac{d\ell}{d\boldsymbol{h}_t}</script> (amplified if <script type="math/tex">\frac{d\boldsymbol{h}_{t-1}}{d\boldsymbol{t}}\gt1</script>).  </p><p>One method to solve this problem, in the case of diminishing gradients, is the use of a neural network architecture, the <strong>long short-term memory</strong> (LSTM), that is specifically designed to ensure that the derivate of the recurrent function is exactly one. The most fundamental idea behind the LSTM is that in addition to the standard hidden state <script type="math/tex">\boldsymbol{h}</script> used by most neural networks, it also has a <strong>memory cell <em>c</em> </strong>, for which the gradient <script type="math/tex">\frac{d\boldsymbol{c}_t}{d\boldsymbol{c}_{t-1}}</script> is exactly one. Because this gradient is exactly one, information stored in the memory cell does not suffer from vanishing gradients, and thus LSTMs can capture long-distance dependencies more effectively than standard recurrent neural networks.</p><script type="math/tex; mode=display">\boldsymbol{u}_t=\tanh(W_{xu}\boldsymbol{x}_t+W_{hu}h_{t-1}+\boldsymbol{b}_u)</script><script type="math/tex; mode=display">\boldsymbol{i}_t=\sigma(W_{xi}\boldsymbol{x}_t+W_{hi}h_{t-1}+\boldsymbol{b}_i)</script><script type="math/tex; mode=display">\boldsymbol{o}_t=\sigma(W_{xo}\boldsymbol{x}_t+W_{ho}h_{t-1}+\boldsymbol{b}_o)</script><script type="math/tex; mode=display">\boldsymbol{c}_t=\boldsymbol{i}_t\odot\boldsymbol{u}_t+\boldsymbol{c}_{t-1}</script><script type="math/tex; mode=display">\boldsymbol{h}_t=\boldsymbol{o}_t\odot\tanh(\boldsymbol{c}_t)</script><p>The first equation is the update, which is basically the same as the RNN update. It takes in the input and hidden state, performs an affine transform and runs it through the tanh non-linearity.  </p><p>The following two equations are the <strong>input gate</strong> and <strong>output gate</strong> of the LSTM respectively. The function of “gates”, as indicated by their name, is to either allow information to pass through or block it from passing. Both of these gates perform an affine transform followed by the sigmoid function. The output of the sigmoid is then used to perform a compoenntwise multiplication, <script type="math/tex">\boldsymbol{z}=\boldsymbol{x}\odot\boldsymbol{y}</script>, which means <script type="math/tex">z_i=x_i*y_i</script>, with the output of another function.  </p><p>The next is the most important equation in the LSTM. This equation sets <script type="math/tex">\boldsymbol{c}_t</script> to be equal to the update <script type="math/tex">\boldsymbol{u}_t</script> modulated by the input gate <script type="math/tex">\boldsymbol{i}_t</script> pllus the cell value for the previous time step <script type="math/tex">\boldsymbol{c}_{t-1}</script>. Since we are directly adding <script type="math/tex">\boldsymbol{c}_{t-1}</script> to <script type="math/tex">\boldsymbol{c}_t</script>, the gradient would be one.  </p><p>The final equation calculates the next hidden state of the LSTM. This is calculated by using a tanh function to scale the cell value between -1 and 1, then modulating the output using the output gate value <script type="math/tex">\boldsymbol{o}_t</script>. This will be the value actually used in any downstream calculation.</p><h3 id="Other-RNN-Variants"><a href="#Other-RNN-Variants" class="headerlink" title="Other RNN Variants"></a>Other RNN Variants</h3><h4 id="LSTM-with-a-forget-gate"><a href="#LSTM-with-a-forget-gate" class="headerlink" title="LSTM with a forget gate:"></a>LSTM with a forget gate:</h4><p>One modification to the standard LSTM that is used widely (in fact so widely that most people who refer to “LSTM” are now referring to this variant) is the addition of a <strong>forget gate</strong>. The equations:</p><script type="math/tex; mode=display">\boldsymbol{u}_t=\tanh(W_{xu}\boldsymbol{x}_t+W_{hu}h_{t-1}+\boldsymbol{b}_u)</script><script type="math/tex; mode=display">\boldsymbol{i}_t=\sigma(W_{xi}\boldsymbol{x}_t+W_{hi}h_{t-1}+\boldsymbol{b}_i)</script><script type="math/tex; mode=display">\boldsymbol{f}_t=\sigma(W_{xf}\boldsymbol{x}_t+W_{hf}h_{t-1}+\boldsymbol{b}_f)</script><script type="math/tex; mode=display">\boldsymbol{o}_t=\sigma(W_{xo}\boldsymbol{x}_t+W_{ho}h_{t-1}+\boldsymbol{b}_o)</script><script type="math/tex; mode=display">\boldsymbol{c}_t=\boldsymbol{i}_t\odot\boldsymbol{u}_t+\boldsymbol{f}_t\odot\boldsymbol{c}_{t-1}</script><script type="math/tex; mode=display">\boldsymbol{h}_t=\boldsymbol{o}_t\odot\tanh(\boldsymbol{c}_t)</script><p>The difference lies in the forget gate, which modulates the passing of the previous cell <script type="math/tex">\boldsymbol{c}_{t-1}</script> to the current cell <script type="math/tex">\boldsymbol{c}_t</script>. This forget gate is useful in that it allows the cell to easily clear its memory when justified. Forget gates have the advantage of allowing the sort of find-grained information flow control, but they also come with the risk that if <script type="math/tex">\boldsymbol{f}_t</script> is set to zero all the time, the model will forget everything and lose its ability to handle long-distance dependencies. Thus, at the beginning of neural network training, it is common to initialize the bias <script type="math/tex">\boldsymbol{b}_f</script> of the forget gate to be a somewhat large value (e.g. 1), which will make the neural net start training without using the forget gate, and only gradually start forgetting content after the net has been trained to some extent.</p><h4 id="Gated-Recurrent-Unit"><a href="#Gated-Recurrent-Unit" class="headerlink" title="Gated Recurrent Unit"></a>Gated Recurrent Unit</h4><p>Gated recurrent unit (GRU) is one simpler RNN variant than LSTM:</p><script type="math/tex; mode=display">\boldsymbol{r}_t=\sigma(W_{xr}\boldsymbol{x}_t+W_{hr}h_{t-1}+\boldsymbol{b}_r)</script><script type="math/tex; mode=display">\boldsymbol{z}_t=\sigma(W_{xz}\boldsymbol{x}_t+W_{hz}h_{t-1}+\boldsymbol{b}_z)</script><script type="math/tex; mode=display">\tilde{\boldsymbol{h}}_t=\tanh(W_{xh}\boldsymbol{x}_t+W_{hh}(\boldsymbol{r}_t\odot\boldsymbol{h}_{t-1})+\boldsymbol{b}_h)</script><script type="math/tex; mode=display">\boldsymbol{h}_t=(1-z_t)\boldsymbol{h}_{t-1}+\boldsymbol{z}_t\tilde{\boldsymbol{h}}_t</script><p>The most characteristic element of the GRU is the last equation, which interpolates between a candidate for the updated hidden state <script type="math/tex">\tilde{\boldsymbol{h}_t}</script> and the previous state <script type="math/tex">\boldsymbol{h}_{t-1}</script> (in the original tutorial, here is noted as <script type="math/tex">\tilde{\boldsymbol{h}}_{t-1}</script>, which might be a mistake). This interpolation is modulated by an <strong>update gate</strong> <script type="math/tex">\boldsymbol{z}_t</script>, where if the update gate is close to one, the GRU will use the new candidate hidden value, and if the update is close to zero, it will use the previous value. The candidate hidden state is similar to a standard RNN update but includes an additional modulation of the hidden state input by a <strong>reset gate</strong> <script type="math/tex">\boldsymbol{r}_t</script>. Compared to the LSTM, the GRU has slightly fewer parameters (it performs one less parameterized affine transform) and also does not have a separate concept of a “cell”. Thus, GRUs have been used by some to conserve memory or computation time.  </p><p>[The stacked RNNs , residual networks; online, batch and minibatch training will be temporarily left out (time limited). I will go over the the following chapters first and then return here.]</p><h2 id="Neural-Encoder-Decoder-Models"><a href="#Neural-Encoder-Decoder-Models" class="headerlink" title="Neural Encoder-Decoder Models"></a>Neural Encoder-Decoder Models</h2><h3 id="Linear-Encoding-Sequence"><a href="#Linear-Encoding-Sequence" class="headerlink" title="Linear Encoding Sequence"></a>Linear Encoding Sequence</h3><p>The basic idea of the <strong>encoder-decoder</strong> model is relatively simple: we have an RNN language model, but before starting calculation of the probabilities of <em>E</em>, we first calculate the initial state of the language model using another RNN over the source sentence <em>F</em>. The name “encoder-decoder” comes from the idea that the first neural network running over <em>F</em> “encodes” its information as a vector of real-valued numbers (the hidden state), then the second neural network used to predict <em>E</em> “decodes” this information into the target sentence.</p><p><img src="/images/nmt/encoder-decoder.jpg" alt></p><script type="math/tex; mode=display">\boldsymbol{m}_t^{(f)}=M_{\cdot,f_t}^{(f)}</script><script type="math/tex; mode=display">\boldsymbol{h}_t^{(f)}=\begin{cases}\text{RNN}^{(f)}(\boldsymbol{m}_t^{(f)},\boldsymbol{h}_{t-1}^{(f)})&t\ge1,\\0&\text{otherwise.}\end{cases}</script><script type="math/tex; mode=display">\boldsymbol{m}_t^{(e)}=M_{\cdot,e_{t-1}}^{(e)}</script><script type="math/tex; mode=display">\boldsymbol{h}_t^{(e)}=\begin{cases}\text{RNN}^{(e)}(\boldsymbol{m}_t^{(e)},\boldsymbol{h}_{t-1}^{(e)})&t\ge1,\\\boldsymbol{h}_{|F|}^{(f)}&\text{otherwise}.\end{cases}</script><script type="math/tex; mode=display">\boldsymbol{p}_t^{(e)}=\text{softmax}(W_{hs}\boldsymbol{h}_t^{(e)}+b_s)</script><p>In the first two lines, we look up the embedding <script type="math/tex">\boldsymbol{m}_t^{(f)}</script> and calculate the encoder hidden state <script type="math/tex">\boldsymbol{h}_t^{(f)}</script> for the <em>t</em>th word in the source sequence <em>F</em>. We start with am empty vector <script type="math/tex">\boldsymbol{h}_0^{(f)}=\boldsymbol{0}</script>, and by <script type="math/tex">\boldsymbol{h}_{|F|}^{(f)}</script>, the encoder has seen all the words in the source sentence. Thus, this hidden state should theoretically be able to encode all of the information in the source sentence.  </p><p>In the decoder phase, we predict the probability of word <script type="math/tex">e_t</script> at each time step. First, we similarly look up <script type="math/tex">\boldsymbol{m}_t^{(e)}</script>, but this time use the previous word <script type="math/tex">e_{t-1}</script>, as we must condition the probability of <script type="math/tex">e_t</script> on the previous word, not on itself. Then, we run the decoder to calculate <script type="math/tex">\boldsymbol{h}_t^{(e)}</script>, whose initial state <script type="math/tex">\boldsymbol{h}_0^{(e)}=\boldsymbol{h}_{|F|}^{(f)}</script>. Finally, we calculate the probability <script type="math/tex">\boldsymbol{p}_t^{(e)}</script> by using a softmax on the hidden state <script type="math/tex">\boldsymbol{h}_t^{(e)}</script>.</p><p>In general, given the probability model <script type="math/tex">P(E|F)</script>, we can generate output according to several criteria:</p><p><strong>Random Sampling</strong>: Randomly select an output <script type="math/tex">E</script> from the probability distribution <script type="math/tex">P(E|F)</script>. This is usually denoted <script type="math/tex">\hat{E}\sim P(E|F)</script>. Useful for a dialog system.</p><p><strong>1-best Search</strong>: Find the <script type="math/tex">E</script> that maximizes <script type="math/tex">P(E|F)</script>, denoted <script type="math/tex">\hat { E } = \underset { E } { \operatorname { argmax } } P ( E | F )</script>. Useful in machine translation.</p><p><strong>n-best Search</strong>: Find the <em>n</em> outputs with the highest probabilities according to <script type="math/tex">P(E|F)</script>.</p><h3 id="Other-Ways-of-Encoding-Sequences"><a href="#Other-Ways-of-Encoding-Sequences" class="headerlink" title="Other Ways of Encoding Sequences"></a>Other Ways of Encoding Sequences</h3><p>Reverse and bidirectional encoders, convolutional neural networks, tree-structured networks will be covered later.</p><h3 id="Ensembling-Multiple-Models"><a href="#Ensembling-Multiple-Models" class="headerlink" title="Ensembling Multiple Models"></a>Ensembling Multiple Models</h3><p><strong>Ensembling</strong> is widely used in encoder-decoders: the combination of the prediction of multiple independently trained models to improve the overall prediction results. </p><script type="math/tex; mode=display">P(e_t|F,e_1^{t-1})=\frac{1}{N}\sum_{i=1}^NP_i(e_t|F,e_1^{t-1})</script><h3 id="Problems"><a href="#Problems" class="headerlink" title="Problems"></a>Problems</h3><ul><li>Long-distance dependencies between words that need to be translated into each other</li><li>the encoder-decoder attempts to store information sentences of any arbitrary length in a hidden vector of fixed size</li></ul><h2 id="Attentional-Neural-Machine-Translation"><a href="#Attentional-Neural-Machine-Translation" class="headerlink" title="Attentional Neural Machine Translation"></a>Attentional Neural Machine Translation</h2><p>The basic idea of attention is that we keep around vectors for every word in the input sentence, and reference these vectors at each decoding step. Because the number of vectors available  to reference is equivalent to the number of words in the input sentence, long sentences will have more vectors than short sentences.  </p><p>First, we create a set of vectors that we will be using as this variably-lengthed representation. To do so, we calculate a vector for every word in the source sentence by running an RNN in both directions:</p><script type="math/tex; mode=display">\overrightarrow{\boldsymbol{h}}_{j}^{(f)} =\text{RNN}(\text{embed}( f _ { j } ) , \vec { \boldsymbol { h } } _ { j - 1 } ^ { ( f ) } )</script><script type="math/tex; mode=display">\overleftarrow{\boldsymbol{h}}_j^{(f)}=\text{RNN}(\text{embed}(f_j),\overleftarrow{\boldsymbol{h}}_{j+1}^{(f)})</script><p>Then, we concatenate the two vectors <script type="math/tex">\overrightarrow{\boldsymbol{h}}_j^{(f)}</script> and <script type="math/tex">\overleftarrow{\boldsymbol{h}}_j^{(f)}</script> into a bidirectional representation <script type="math/tex">\boldsymbol{h}_j^{(f)}</script></p><script type="math/tex; mode=display">\boldsymbol{h}_j^{(f)}=[\overleftarrow{\boldsymbol{h}}_j^{(f)};\overrightarrow{\boldsymbol{h}}_j^{(f)}]</script><p>We can further concatenate these vectors into a matrix:</p><script type="math/tex; mode=display">H^{(f)}=\text{concat_col}(\boldsymbol{h}_1^{(f)},...,\boldsymbol{h}_{|F|}^{(f)})</script><p>The key insight of attention is that we calculate a vector <script type="math/tex">\boldsymbol\alpha_t</script> that can be used to combine together the columns of <em>H</em> into a vector <script type="math/tex">\boldsymbol{c}_t</script></p><script type="math/tex; mode=display">\boldsymbol{c}_t=H^{(f)}\boldsymbol{\alpha}_t</script><p>the <script type="math/tex">\boldsymbol{\alpha}_t</script> is called the <strong>attentional vector</strong>, and is generally assumed to have elements that are between zero and one and add to one.  </p><p>The basic idea behind the attention vector is that it is telling us how much we are “focusing” on a particular source word at a particular time step. The larger the value in <script type="math/tex">\boldsymbol{\alpha}_t</script>, the more impact a word will have when predicting the next word in the output sentence.  </p><p>Then how to get the <script type="math/tex">\boldsymbol\alpha_t</script>? The answer lies in the <em>decoder</em> RNN, which we use to track our state while we are generating output. The decoder’s hidden state <script type="math/tex">\boldsymbol{h}_t^{(e)}</script> is a fixed-length continuous vector representing the previous target words <script type="math/tex">e_1^{t-1}</script>, initialized as <script type="math/tex">\boldsymbol{h}_0^{(e)}=\boldsymbol{h}_{|F|+1}^{(f)}</script>. This is used to calculate a context vector <script type="math/tex">\boldsymbol{c}_t</script> that is used to summarize the source attentional context used in choosing target word <script type="math/tex">e_t</script>, and initialized as <script type="math/tex">\boldsymbol{c}_t=0</script>.  </p><p>First, we update the hidden state to $\boldsymbol{h}_t^{(e)}$ based on the word representation and context vector from the previous target time step</p><script type="math/tex; mode=display">\boldsymbol{h}_t^{(e)}=\text{enc}([\text{embed}(e_{t-1});\boldsymbol{c}_{t-1}],\boldsymbol{h}_{t-1}^{(e)})</script><p>Based on this <script type="math/tex">\boldsymbol{h}_t^{(e)}</script>, we calculate an attention score $\boldsymbol{a}_t$, with each element equal to</p><script type="math/tex; mode=display">a_{t,j}=\text{attn_score}(\boldsymbol{h}_j^{(f)},\boldsymbol{h}_t^{(e)})</script><p>We then normalize this into the actual attention vector itself by taking a softmax over the scores:</p><script type="math/tex; mode=display">\boldsymbol\alpha_t=\text{softmax}(\boldsymbol{a}_t)</script><p>Then we can use this attention vector to weight the encoded representation <script type="math/tex">H^{(f)}</script> to create a context vector $\boldsymbol{c}_t$ for the current time step. </p><script type="math/tex; mode=display">\boldsymbol{c}_t=H^{(f)}\boldsymbol{\alpha}_t</script><p>We now have a context vector <script type="math/tex">\boldsymbol{c}_t</script> and hidden state <script type="math/tex">\boldsymbol{h}_t^{(e)}</script> for time step <em>t</em>, which we can pass down to downstream tasks.</p><p><img src="/images/nmt/attn.jpg" alt></p><h2 id="Questions"><a href="#Questions" class="headerlink" title="Questions"></a>Questions</h2><ul><li><p><em>If V is the size of the target vocabulary, how many are there for a sentence of length T?</em> (on page 4)</p><p>There are <script type="math/tex">V^T</script>.</p></li><li><p><em>How many parameters does an n-gram model with a particular n have?</em> (on page 6)</p><script type="math/tex; mode=display">V^n</script></li><li><p><em>What is this probability?</em> (on page 7)</p><script type="math/tex; mode=display">P=P(e_2=\text{am}|e_1=\text{i})\cdot P(e_3=\text{from}|e_2=\text{am})\cdot P(e_4=\text{utah}|e_3=\text{from}\cdot P(e_5=.|e_4=\text{utah}))\\=\frac{1}{2}\cdot1\cdot\frac{1}{2}\cdot1\\=\frac{1}{4}</script></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This post is mainly the reading note of the tutorial &lt;a href=&quot;https://arxiv.org/abs/1703.01619&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Neural Mac
      
    
    </summary>
    
      <category term="Natural Language Processing" scheme="http://haelchan.me/categories/Natural-Language-Processing/"/>
    
    
      <category term="neural machine translation" scheme="http://haelchan.me/tags/neural-machine-translation/"/>
    
      <category term="seq2seq" scheme="http://haelchan.me/tags/seq2seq/"/>
    
  </entry>
  
  <entry>
    <title>BERT - A new era of NLP</title>
    <link href="http://haelchan.me/2018/10/17/bert/"/>
    <id>http://haelchan.me/2018/10/17/bert/</id>
    <published>2018-10-17T15:45:54.000Z</published>
    <updated>2018-11-01T13:29:20.000Z</updated>
    
    <content type="html"><![CDATA[<p>Google AI Language just published their paper <a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noopener">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a> on arXiv. <a href="https://twitter.com/lmthang" target="_blank" rel="noopener">Thang Luong</a>, a research scientist on the Google Brain team, just called it “a new era of NLP”. It reaches SOTA on 11 tasks, even super-human performance on SWAG.<br>The lead author Jacob Devlin posted some comments on <a href="https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/" target="_blank" rel="noopener">Reddit</a>, which gives an simple explanation of the idea.   </p><p>Google-research just released their <a href="https://github.com/google-research/bert" target="_blank" rel="noopener">TensorFlow code and pre-trained models for BERT</a> on Halloween and received nearly 3k stars within 24 hours.  </p><p>I just glanced through the paper and I will go over it and write a detailed note. (Maybe after the graduation application).</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Google AI Language just published their paper &lt;a href=&quot;https://arxiv.org/abs/1810.04805&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;BERT: Pre-trainin
      
    
    </summary>
    
      <category term="Natural Language Processing" scheme="http://haelchan.me/categories/Natural-Language-Processing/"/>
    
    
      <category term="Transformer" scheme="http://haelchan.me/tags/Transformer/"/>
    
      <category term="reading note" scheme="http://haelchan.me/tags/reading-note/"/>
    
      <category term="pre-training" scheme="http://haelchan.me/tags/pre-training/"/>
    
  </entry>
  
  <entry>
    <title>Natural Language Processing with Python and NLTK</title>
    <link href="http://haelchan.me/2018/07/23/nlp-with-python-and-nltk/"/>
    <id>http://haelchan.me/2018/07/23/nlp-with-python-and-nltk/</id>
    <published>2018-07-23T08:20:56.000Z</published>
    <updated>2018-10-18T15:08:46.000Z</updated>
    
    <content type="html"><![CDATA[<p>This note is based on <a href="http://www.nltk.org/book/" target="_blank" rel="noopener">Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit</a>. I’ve uploaded the exercises solution to <a href="https://github.com/HaelChan/NLP-with-Python-Solutions" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Texts-and-words"><a href="#Texts-and-words" class="headerlink" title="Texts and words"></a>Texts and words</h2><p>Texts are represented in Python using lists. We can use indexing, slicing, and the <code>len()</code> function.</p><p>Some word comparison operators: <code>s.startswith(t)</code>, <code>s.endswith(t)</code>, <code>t in s</code>, <code>s.islower()</code>, <code>s.isupper()</code>, <code>s.isalpha()</code>, <code>s.isalnum()</code>, <code>s.isdigit()</code>, <code>s.istitle()</code>.</p><h3 id="Searching"><a href="#Searching" class="headerlink" title="Searching"></a>Searching</h3><p><code>text1.concordance(&quot;monstrous&quot;)</code><br>A concordance view shows us every occurrence of a given word, together with some context.<br>A concordance permits us to see words in context.</p><p><code>text1.similar(&quot;monstrous&quot;)</code><br>We can find out other words appear in a similar range of contexts, by appending the term <code>similar</code> to the name of the text, then inserting the relevant word in parentheses. (a bit like synonyms)</p><p><code>text2.common_contexts([&quot;monstrous&quot;, &quot;very&quot;])</code><br>The term <code>common_contexts</code> allows us to examine just the contexts that are shared by two or more words.</p><p><code>text4.dispersion_plot([&quot;citizens&quot;, &quot;democracy&quot;, &quot;freedom&quot;, &quot;duties&quot;, &quot;America&quot;])</code><br>We can determine the location of a word in the text: how many words from the beginning it appears. This positional information can be displayed using a dispersion plot.</p><p><strong>Fine-grained Selection of Words</strong><br>The mathematical set notation and corresponding Python expression.</p><script type="math/tex; mode=display">\{w|w\in V&P(w)\}</script><p><code>[w for w in V if p(w)]</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">V = set(text1)</span><br><span class="line">long_words = [w <span class="keyword">for</span> w <span class="keyword">in</span> V <span class="keyword">if</span> len(w) &gt; <span class="number">15</span>]</span><br></pre></td></tr></table></figure><p><strong>Collocations and Bigrams</strong><br>The bigram is written as <code>(&#39;than&#39;, &#39;said&#39;)</code> in Python.<br>A collocation is a sequence of words that occur together unusually often. Collocations are essentially just frequent bigrams, except that we want to pay more attention to the cases that involve rare words. In particular, we want to find bigrams that occur more often then we would expect based on the frequency of the individual words.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">list(bigrams([<span class="string">'more'</span>, <span class="string">'is'</span>, <span class="string">'said'</span>, <span class="string">'than'</span>, <span class="string">'done'</span>]))</span><br><span class="line">text4.collocations()</span><br><span class="line">``` </span><br><span class="line"></span><br><span class="line"><span class="comment">### Counting</span></span><br><span class="line">A **token** <span class="keyword">is</span> the technical name <span class="keyword">for</span> a sequence of characters that we want to trat <span class="keyword">as</span> a group.</span><br><span class="line">We will think of a text <span class="keyword">as</span> nothing more than a sequence of words <span class="keyword">and</span> punctuation. Thus we can use `len()` to compute the number of tokens.</span><br><span class="line">A **word type** <span class="keyword">is</span> the form <span class="keyword">or</span> spelling of the word independently of its specific occurrences <span class="keyword">in</span> a text - that <span class="keyword">is</span>, the word considered <span class="keyword">as</span> a unique item of vocabulary.</span><br><span class="line">Since our tokens include punctuation symbols <span class="keyword">as</span> well, we will generally call these unique items **types** instead of word types.</span><br><span class="line"></span><br><span class="line">```Python</span><br><span class="line">len(text1)                                                      <span class="comment"># the tokens of the text</span></span><br><span class="line">len(set(text1))                                                 <span class="comment"># the types of the text considering case</span></span><br><span class="line">len(set(word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> text1))                        <span class="comment"># without considering case</span></span><br><span class="line">len(set(word.lower() <span class="keyword">for</span> word <span class="keyword">in</span> text1 <span class="keyword">if</span> word.isalpha()))      <span class="comment"># eliminate numbers and punctuation</span></span><br></pre></td></tr></table></figure><p>A <strong>frequency distribution</strong> tells us the frequency of each vocabulary item in the text.<br>FreqDist can be treated as <code>dictionary</code> in Python, where the word(or word length, etc) is the key, and the occurrence is the corresponding value.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fdist1 = FreqDist(text1)                        <span class="comment"># invoke FreqDist(), pass the name of the text as an argument</span></span><br><span class="line">fdist1.most_common(<span class="number">50</span>)                          <span class="comment"># gives a list of the 50 most frequently occurring types in the text</span></span><br><span class="line">fdist1[<span class="string">'whale'</span>]                                 <span class="comment"># the frequency of the given word</span></span><br><span class="line">fdist1.hapaxes()                                <span class="comment"># find the words that occur only once</span></span><br><span class="line"></span><br><span class="line">fdist = FreqDist(len(w) <span class="keyword">for</span> w <span class="keyword">in</span> text1)         <span class="comment"># the length of words' frequency distribution</span></span><br><span class="line">fdist.most_common()                             <span class="comment"># if no argument is passed, just print all word lengths</span></span><br><span class="line">fdist.max()                                     <span class="comment"># print the most frequent word length</span></span><br><span class="line">fdist[<span class="number">3</span>]                                        <span class="comment"># print the word length 3's occurrence</span></span><br><span class="line">fdist.freq(<span class="number">3</span>)                                   <span class="comment"># print the frequency of word length 3</span></span><br></pre></td></tr></table></figure><p>functions defined for NLTK’s Frequency Distributions can be found <a href="http://www.nltk.org/book/ch01.html#tab-word-tests" target="_blank" rel="noopener">here</a></p><h2 id="Text-Corpora-and-Lexical-Resources"><a href="#Text-Corpora-and-Lexical-Resources" class="headerlink" title="Text Corpora and Lexical Resources"></a>Text Corpora and Lexical Resources</h2><h3 id="Accessing-Text-Corpora"><a href="#Accessing-Text-Corpora" class="headerlink" title="Accessing Text Corpora"></a>Accessing Text Corpora</h3><p>A text corpus is a large, structured collection of texts. Some text corpora are categorized, e.g., by genre or topic; sometimes the categories of a corpus overlap each other.<br><img src="/images/linguistic/text-corpus-structure.png" alt><br>The NLTK has many corpus in the package <code>nltk.corpus</code>. To perform the functions introduced before, we have to employ the following pair of statements:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">emma = nltk.Text(nltk.corpus.guterberg.words(<span class="string">'austen-emma.txt'</span>))        <span class="comment"># type cast, from StreamBackedCorpusView to Text</span></span><br><span class="line">emma.concordance(<span class="string">'surprise'</span>)</span><br></pre></td></tr></table></figure><p>A short program to display information about each text, by looping over all the values of <code>fileid</code> corresponding to the <code>gutenberg</code> file identifiers and then computing statistics for each text.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> fileid <span class="keyword">in</span> gutenberg.fileids():</span><br><span class="line">    num_chars = len(gutenberg.raw(fileid))                              <span class="comment"># raw() tells us how many letters occur in the text</span></span><br><span class="line">    num_words = len(gutenberg.words(fileid))                            <span class="comment"># words() divides the text into words</span></span><br><span class="line">    num_sents = len(gutenberg.sents(fileid))                            <span class="comment"># sents() divides the text up into its sentences, where each sentence is a list of words</span></span><br><span class="line">    num_vocab = len(set(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> gutenberg.words(fileid)))    <span class="comment"># count unique words</span></span><br><span class="line">    print(round(num_chars/num_words), round(num_words/num_sents), round(num_words/num_vocab), fileid)   <span class="comment"># use round() to round each number to the nearest integer</span></span><br><span class="line">    <span class="comment"># average word lenth, average sentence length, the number of times each vocabulary item appears in the text on average</span></span><br></pre></td></tr></table></figure><p><strong>Brown Corpus</strong><br>The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University. This corpus contains text from 500 sources, and the sources have been categorized by genre.(see <a href="http://www.nltk.org/book/ch02.html#tab-brown-sources" target="_blank" rel="noopener">here</a> for detail)<br>We can access the corpus as a list of words, or a list of sentences. We can optionally specify particular categories or files to read.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">brown.categories()</span><br><span class="line">brown.fileids()</span><br><span class="line">brown.words(categories=<span class="string">'news'</span>)</span><br><span class="line">brown.words(fileids=<span class="string">'cg22'</span>)</span><br><span class="line">brown.sents(categories=[<span class="string">'news'</span>, <span class="string">'editorial'</span>, <span class="string">'reviews'</span>])</span><br></pre></td></tr></table></figure><p>Use Brown Corpus to study <strong>stylistics</strong>: systematic differences between genres.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">news_text = brown.words(categories=<span class="string">'news'</span>)</span><br><span class="line">fdist = nltk.FreqDist(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> news_text)</span><br><span class="line">modals = [<span class="string">'can'</span>, <span class="string">'could'</span>, <span class="string">'may'</span>, <span class="string">'might'</span>, <span class="string">'must'</span>, <span class="string">'will'</span>]</span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span> modals:</span><br><span class="line">    print(m + <span class="string">':'</span>, fdist[m], end=<span class="string">' '</span>)</span><br><span class="line">print()                                                                         <span class="comment"># I add this line to modify the output</span></span><br><span class="line"></span><br><span class="line">cfd = nltk.ConditionalFreqDist(</span><br><span class="line">            (genre, word.lower())                                               <span class="comment"># to count the Uppercased words as well, or the statistics would be inconsistent</span></span><br><span class="line">            <span class="keyword">for</span> genre <span class="keyword">in</span> brown.categories()</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> brown.words(categories=genre))</span><br><span class="line">genres = [<span class="string">'news'</span>, <span class="string">'religion'</span>, <span class="string">'hobbies'</span>, <span class="string">'science_fiction'</span>, <span class="string">'romance'</span>, <span class="string">'humor'</span>]         <span class="comment"># genres = brown.categories()</span></span><br><span class="line">cfd.tabulate(conditions=genres, samples=modals)</span><br></pre></td></tr></table></figure><p>A conditional frequency distribution is a collection of frequency distributions, each one for a different “condition”. The condition will often be the category of the text.<br>A frequency distribution counts observable events, such as the appearance of words in a text. A conditional frequency distribution needs to pair each event with a condition. </p><p>NLTK’s Conditional Frequency Distributions: commonly-used methods and idioms for defining, accessing, and visualizing a conditional frequency distribution of counters.</p><div class="table-container"><table><thead><tr><th>Example</th><th>Description</th></tr></thead><tbody><tr><td>cfdist = ConditionalFreqDist(pairs)</td><td>create a conditional frequency distribution from a list of pairs</td></tr><tr><td>cfdist.conditions()</td><td>the conditions</td></tr><tr><td>cfdist[condition]</td><td>the frequency distribution for this condition</td></tr><tr><td>cfdist[condition][sample]</td><td>frequency for the given sample for this condition</td></tr><tr><td>cfdist.tabulate()</td><td>tabulate the conditional frequency distribution</td></tr><tr><td>cfdist.tabulate(samples, conditions)</td><td>tabulation limited to the specified samples and conditions</td></tr><tr><td>cfdist.plot()</td><td>graphical plot of the conditional frequency distribution</td></tr><tr><td>cfdist.plot(samples, conditions)</td><td>graphical plot limited to the specified samples and conditions</td></tr><tr><td>cfdist1 &lt; cfdist2</td><td>test if samples in cfdist1 occur less frequently than in cfdist2</td></tr></tbody></table></div><p><strong>Reuters Corpus</strong><br>The Reuters Corpus contains 10788 news documents totaling 1.3 million words. The documents have been classified into 90 topics and grouped into training and test sets.<br>Unlike the Brown Corpus, categories in the Reuters corpus overlap with each other, simply because a news story often covers multiple topics. We can ask for the topics covered by one or more documents, or for the documents included in one or more categories.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">reuters.categories(<span class="string">'training/9865'</span>)</span><br><span class="line">reuters.fileids(<span class="string">'barley'</span>)</span><br></pre></td></tr></table></figure><p><a href="http://www.nltk.org/book/ch02.html#tab-corpora" target="_blank" rel="noopener">Here</a> lists some of the Corpora and Corpus Samples Distributed with NLTK. For more information consult <a href="http://www.nltk.org/howto/" target="_blank" rel="noopener">NLTK HOWTOs</a>.</p><p>Basic Corpus Functionality defined in NLTK:</p><div class="table-container"><table><thead><tr><th>Example</th><th>Description</th></tr></thead><tbody><tr><td>fileids()</td><td>the files of the corpus</td></tr><tr><td>fileids([categories])</td><td>the files of the corpus corresponding to these categories</td></tr><tr><td>categories()</td><td>the categories of the corpus</td></tr><tr><td>categories([fileids])</td><td>the categories of the corpus corresponding to these files</td></tr><tr><td>raw()</td><td>the raw content of the corpus</td></tr><tr><td>raw(fileids=[f1,f2,f3])</td><td>the raw content of the specified files</td></tr><tr><td>raw(categories=[c1,c2])</td><td>the raw content of the specified categories</td></tr><tr><td>words()</td><td>the words of the whole corpus</td></tr><tr><td>words(fileids=[f1,f2,f3])</td><td>the words of the specified fileids</td></tr><tr><td>words(categories=[c1,c2])</td><td>the words of the specified categories</td></tr><tr><td>sents()</td><td>the sentences of the whole corpus</td></tr><tr><td>sents(fileids=[f1,f2,f3])</td><td>the sentences of the specified fileids</td></tr><tr><td>sents(categories=[c1,c2])</td><td>the sentences of the specified categories</td></tr><tr><td>abspath(fileid)</td><td>the location of the given file on disk</td></tr><tr><td>encoding(fileid)</td><td>the encoding of the file (if known)</td></tr><tr><td>open(fileid)</td><td>open a stream for reading the given corpus file</td></tr><tr><td>root</td><td>if the path to the root of locally installed corpus</td></tr><tr><td>readme()</td><td>the contents of the README file of the corpus</td></tr></tbody></table></div><h3 id="Lexical-Resources"><a href="#Lexical-Resources" class="headerlink" title="Lexical Resources"></a>Lexical Resources</h3><p>A lexicon, or lexical resource, is a collection of words and/or phrases along with associated such as part of speech and sense definition. Lexical resources are secondary to texts, and are usually created and enriched with the help of texts. For example, <code>vocab = sorted(set(my_text))</code> and <code>word_freq = FreqDist(my_text)</code> are both simple lexical resources.<br><img src="/images/linguistic/lexicon.png" alt><br><em>Lexicon Terminology: lexical entries for two lemmas having the same spelling (homonyms), providing part of speech and gloss information.</em></p><p><strong>Wordlist Corpora</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find unusual or mis-spelt words in a text corpus</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">unusual_words</span><span class="params">(text)</span>:</span></span><br><span class="line">    text_vocab = set(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> text <span class="keyword">if</span> w.isalpha())</span><br><span class="line">    english_vocab = set(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> nltk.corpus.words.words())</span><br><span class="line">    unusual = text_vocab - english_vocab</span><br><span class="line">    <span class="keyword">return</span> sorted(unusual)</span><br><span class="line"></span><br><span class="line">unusual_words(nltk.corpus.gutenberg.words(<span class="string">'austen-sense.txt'</span>))</span><br><span class="line">unusual_words(nltk.corpus.nps_chat.words())</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># word list for solving word puzzles</span></span><br><span class="line">puzzle_letters = nltk.FreqDist(<span class="string">'egivrvonl'</span>)             <span class="comment"># calculate frequency distribution for given word puzzle letters</span></span><br><span class="line">obligatory = <span class="string">'r'</span>                                        <span class="comment"># the center letter which must be included</span></span><br><span class="line">wordlist = nltk.corpus.words.words()</span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> wordlist <span class="keyword">if</span> len(w) &gt;= <span class="number">6</span></span><br><span class="line">                     <span class="keyword">and</span> obligatory <span class="keyword">in</span> w</span><br><span class="line">                     <span class="keyword">and</span> nltk.FreqDist(w) &lt;= puzzle_letters]    <span class="comment"># the comparison of frequency distribution of each letter</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># find names which appear in both male.txt and female.txt</span></span><br><span class="line">names = nltk.corpus.names                                       <span class="comment"># which contains 'male.txt' and 'female.txt'</span></span><br><span class="line">male_names = names.words(<span class="string">'male.txt'</span>)</span><br><span class="line">female_names = names.words(<span class="string">'female.txt'</span>)</span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> male_names <span class="keyword">if</span> w <span class="keyword">in</span> female_names]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># comparative wordlist - Swadesh wordlists</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> swadesh</span><br><span class="line">swedesh.fileids()</span><br><span class="line">fr2en = swadesh.entries([<span class="string">'fr'</span>, <span class="string">'en'</span>])</span><br><span class="line">de2en = swadesh.entries([<span class="string">'de'</span>, <span class="string">'en'</span>])</span><br><span class="line">es2en = swadesh.entries([<span class="string">'es'</span>, <span class="string">'en'</span>])</span><br><span class="line">translate = dict(fr2en)                         <span class="comment"># convert into a simple dictionary</span></span><br><span class="line">translate.update(dict(de2en))</span><br><span class="line">translate.update(dict(es2en))</span><br><span class="line">translate[<span class="string">'chien'</span>]                              <span class="comment"># 'dog'</span></span><br></pre></td></tr></table></figure><p><em>The CMU Pronouncing Dictionary, Toolbox are introduced in the book, I’ll just omit them in the note.</em></p><h3 id="WordNet"><a href="#WordNet" class="headerlink" title="WordNet"></a>WordNet</h3><p>WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155287 words and 117659 synonym sets.</p><p><strong>Synsets</strong><br>With the WordNet, we can find the word’s synonyms in <em>synsets</em> - “synonym set”, definitions and examples as well.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># synsets</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> wordnet <span class="keyword">as</span> wn</span><br><span class="line">wn.synsets(<span class="string">'motorcar'</span>)                          <span class="comment"># [Synset('car.n.01')]</span></span><br><span class="line">wn.synset(<span class="string">'car.n.01'</span>).lemma_names()             <span class="comment"># ['car', 'auto', 'automobile', 'machine', 'motorcar']</span></span><br><span class="line">wn.synset(<span class="string">'car.n.01'</span>).definition()</span><br><span class="line">wn.synset(<span class="string">'car.n.01'</span>).examples()</span><br></pre></td></tr></table></figure><p><strong>Hyponyms and hypernyms</strong><br>WordNet synsets correspond to abstract concepts, and they don’t always have corresponding words in English. These concepts are linked together in a hierarchy. (See hyponyms and in lexical relations.)<br>The corresponding methods are <code>hyponyms()</code> and <code>hypernyms()</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># hyponyms and hypernyms</span></span><br><span class="line">motorcar = wn.synset(<span class="string">'car.n.01'</span>)</span><br><span class="line">types_of_motorcar = motorcar.hyponyms()</span><br><span class="line">parent_of_motorcar = motorcar.hypernyms()</span><br><span class="line">paths = motorcar.hypernym_paths()</span><br></pre></td></tr></table></figure><p><strong>Some other lexical relations</strong><br>Another important way to navigate the WordNet network is from items to their components (meronyms), or to the things they are contained in (holonyms). There are three kinds of holonym-meronym relation: <code>member_meronyms()</code>, <code>part_meronyms()</code>, <code>substance_meronyms()</code>, <code>member_holonyms()</code>, <code>part_holonyms()</code>, <code>substance_holonyms()</code>.<br>There are also relationships between verbs. For example, the act of <em>walking</em> involves the act of <em>stepping</em>, so walking entails stepping. Some verbs have multiple entailments.(NLTK also includes VerbNet, a hierarhical verb lexicon linked to WordNet. It can be accessed with <code>nltk.corpus.verbnet</code>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># meronyms and holonyms</span></span><br><span class="line">tree = wn.synset(<span class="string">'tree.n.01'</span>)</span><br><span class="line">tree.part_meronyms()</span><br><span class="line">tree.substance_meronyms()</span><br><span class="line">tree.member_holonyms()</span><br><span class="line"></span><br><span class="line"><span class="comment"># entailments</span></span><br><span class="line">wn.synset(<span class="string">'walk.v.01'</span>).entailments()</span><br><span class="line">wn.synset(<span class="string">'eat.v.01'</span>).entailments()</span><br><span class="line">wn.synset(<span class="string">'tease.v.03'</span>).entailments()</span><br><span class="line"></span><br><span class="line"><span class="comment"># antonymy</span></span><br><span class="line">wn.lemma(<span class="string">'supply.n.02.supply'</span>).antonymys()</span><br><span class="line">wn.lemma(<span class="string">'rush.v.01.rush'</span>).antonymys()</span><br><span class="line">wn.lemma(<span class="string">'horizontal.a.01.horizontal'</span>).antonymys()</span><br><span class="line">wn.lemma(<span class="string">'staccato.r.01.staccato'</span>).antonymys()</span><br></pre></td></tr></table></figure><p><strong>Semantic Similarity</strong><br>Given a particular synset, we can traverse the WordNet network to find synsets with related meanings. Knowing which words are semantically related is useful for indexing a collection of texts, so that a search for a general term will match documents containing specific terms.<br>We can qualify the concept of generality(specific or general) by looking up the depth of the synset.<br><code>path_similarity</code> assigns a score in the range 0–1 based on the shortest path that connects the concepts in the hypernym hierarchy (-1 is returned in those cases where a path cannot be found). Comparing a synset with itself will return 1. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">right_whale = wn.synset(<span class="string">'right_whale.n.01'</span>)</span><br><span class="line">orca = wn.synset(<span class="string">'orca.n.01'</span>)</span><br><span class="line">minke = wn.synset(<span class="string">'minke_whale.n.01'</span>)</span><br><span class="line">tortoise = wn.synset(<span class="string">'tortoise.n.01'</span>)</span><br><span class="line">novel = wn.synset(<span class="string">'novel.n.01'</span>)</span><br><span class="line"></span><br><span class="line">right_whale.lowest_common_hypernyms(minke)</span><br><span class="line">right_whale.min_depth()</span><br><span class="line">right_whale.path_similarity(minke)</span><br></pre></td></tr></table></figure><h2 id="Processing-Raw-Text"><a href="#Processing-Raw-Text" class="headerlink" title="Processing Raw Text"></a>Processing Raw Text</h2><h3 id="Accessing-Text"><a href="#Accessing-Text" class="headerlink" title="Accessing Text"></a>Accessing Text</h3><p><strong>From Web</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># from electronic books</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> word_tokenize</span><br><span class="line">url = <span class="string">"http://www.gutenberg.org/files/2554/2554-0.txt"</span></span><br><span class="line">response = request.urlopen(url)</span><br><span class="line">raw = response.read().decode(<span class="string">'utf-8'</span>)</span><br><span class="line">raw = raw[raw.find(<span class="string">'PART I'</span>):raw.rfind(<span class="string">"End of Project Gutenberg’s Crime"</span>)]   <span class="comment"># extract useful information</span></span><br><span class="line">tokens = word_tokenize(raw)</span><br><span class="line">text = nltk.Text(tokens)</span><br><span class="line"></span><br><span class="line"><span class="comment"># from HTML is almost the same, except that to get text out of HTML</span></span><br><span class="line"><span class="comment"># we have to use a Python library called BeautifulSoup, </span></span><br><span class="line"><span class="comment"># available from http://www.crummy.com/software/BeautifulSoup/</span></span><br><span class="line"><span class="comment"># since there're many markup symbols, e.g., &lt;&lt; &gt;&gt; |</span></span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line">raw = BeautifulSoup(html).get_text()</span><br><span class="line">tokens = word_tokenize(raw)</span><br><span class="line"></span><br><span class="line"><span class="comment"># With the help of a Python library called the Universal Feed Parser available from </span></span><br><span class="line"><span class="comment"># https://pypi.python.org/pypi/feedparser, we can access the content of a blog</span></span><br><span class="line"><span class="keyword">import</span> feedparser</span><br><span class="line">llog = feedparser.parse(<span class="string">"http://languagelog.ldc.upenn.edu/nll/?feed=atom"</span>)</span><br></pre></td></tr></table></figure><p><strong>From local files</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">f = open(<span class="string">'document.txt'</span>)</span><br><span class="line">f.read()                                    <span class="comment"># read the contents of the entire file</span></span><br><span class="line"></span><br><span class="line">f = open(<span class="string">'document.txt'</span>, <span class="string">'rU'</span>)              <span class="comment"># after read() we have to reopen the file</span></span><br><span class="line">                                            <span class="comment"># this time 'r' means to open the file for reading(the default)</span></span><br><span class="line">                                            <span class="comment"># and 'U' stands for 'Universal', which lets us ignore the </span></span><br><span class="line">                                            <span class="comment"># different conventions used for marking newlines</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:                              <span class="comment"># read line by line</span></span><br><span class="line">    print(line.strip())</span><br></pre></td></tr></table></figure><p>ASCII text and HTML text are human readable formats. Text often comes in binary formats — like PDF and MSWord — that can only be opened using specialized software. Third-party libraries such as <code>pypdf</code> and <code>pywin32</code> provide access to these formats. Extracting text from multi-column documents is particularly challenging. For once-off conversion of a few documents, it is simpler to open the document with a suitable application, then save it as text to your local drive, and access it as described below. If the document is already on the web, you can enter its URL in Google’s search box. The search result often includes a link to an HTML version of the document, which you can save as text.</p><p><img src="/images/linguistic/pipeline.png" alt></p><h3 id="Regular-Expression"><a href="#Regular-Expression" class="headerlink" title="Regular Expression"></a>Regular Expression</h3><p>I’ve uploaded my summary of regular expression in the post <a href="http://haelchan.me/2018/05/23/regular-expression/">Regular Expression</a>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">wordlist = [w <span class="keyword">for</span> w <span class="keyword">in</span> nltk.corpus.words.words(<span class="string">'en'</span>) <span class="keyword">if</span> w.islower()]</span><br><span class="line">[w <span class="keyword">for</span> w <span class="keyword">in</span> wordlist <span class="keyword">if</span> re.search(<span class="string">'ed$'</span>, w)]</span><br></pre></td></tr></table></figure><p>NLTK provides a regular expression tokenizer: <code>nltk.regexp_tokenize()</code>.</p><h2 id="Structured-Program"><a href="#Structured-Program" class="headerlink" title="Structured Program"></a>Structured Program</h2><p>The basic part is discussed in my <a href="http://haelchan.me/2018/02/07/python-learning/">Python learning</a> note. And in this section, I just record some unfamiliar knowledge.</p><p>Assignment always copies the value of an expression, but a value is not always what you might expect it to be. In particular, the “value” of a structured object such as a list is actually just a <em>reference</em> to the object.<br>Python provides two ways to check that a pair of items are the same. The <code>is</code> operator tests for object identity.</p><p>A list is typically a sequence of objects all having the same type, of arbitrary length. We often use lists to hold sequences of words. In contrast, a tuple is typically a collection of objects of <em>different types</em>, of <em>fixed length</em>. We often use a tuple to hold a <strong>record</strong>, a collection of different <strong>fields</strong> relating to some entity.</p><p><strong>Generator Expression:</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>max([w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(text)]) [<span class="number">1</span>]</span><br><span class="line"><span class="string">'word'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>max(w.lower() <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(text)) [<span class="number">2</span>]</span><br><span class="line"><span class="string">'word'</span></span><br></pre></td></tr></table></figure><p>The second line uses a <strong>generator expression</strong>. This is more than a notational convenience: in many language processing situations, generator expressions will be more efficient. In 1, storage for the list object must be allocated before the value of <code>max()</code> is computed. If the text is very large, this could be slow. In 2, the data is streamed to the calling function. Since the calling function simply has to find the maximum value - the word which comes latest in lexicographic sort order - it can process the stream of data without having to store anything more than the maximum value seen so far.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search1</span><span class="params">(substring, words)</span>:</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> substring <span class="keyword">in</span> word:</span><br><span class="line">            result.append(word)</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">search2</span><span class="params">(substring, words)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="keyword">if</span> substring <span class="keyword">in</span> word:</span><br><span class="line">            <span class="keyword">yield</span> word</span><br></pre></td></tr></table></figure><p><strong>Function</strong><br>It is not necessary to have any parameters.<br>A function usually communicates its results back to the calling program via the <code>return</code> statement.<br>A Python function is not required to have a return statement. Some functions do their work as a side effect, printing a result, modifying a file, or updating the contents of a parameter to the function (such functions are called “procedures” in some other programming languages).</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">my_sort1</span><span class="params">(mylist)</span>:</span>      <span class="comment"># good: modifies its argument, no return value</span></span><br><span class="line"><span class="meta">... </span>    mylist.sort()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">my_sort2</span><span class="params">(mylist)</span>:</span>      <span class="comment"># good: doesn't touch its argument, returns value</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> sorted(mylist)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">my_sort3</span><span class="params">(mylist)</span>:</span>      <span class="comment"># bad: modifies its argument and also returns it</span></span><br><span class="line"><span class="meta">... </span>    mylist.sort()</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> mylist</span><br></pre></td></tr></table></figure><p>When you refer to an existing name from within the body of a function, the Python interpreter first tries to resolve the name with respect to the names that are local to the function. If nothing is found, the interpreter checks if it is a global name within the module. Finally, if that does not succeed, the interpreter checks if the name is a Python built-in. This is the so-called <strong>LGB rule</strong> of name resolution: local, then global, then built-in.</p><p>Python also lets us pass a function as an argument to another function. Now we can abstract out the operation, and apply a <em>different operation</em> on the <em>same data</em>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sent = [<span class="string">'Take'</span>, <span class="string">'care'</span>, <span class="string">'of'</span>, <span class="string">'the'</span>, <span class="string">'sense'</span>, <span class="string">','</span>, <span class="string">'and'</span>, <span class="string">'the'</span>,</span><br><span class="line"><span class="meta">... </span>        <span class="string">'sounds'</span>, <span class="string">'will'</span>, <span class="string">'take'</span>, <span class="string">'care'</span>, <span class="string">'of'</span>, <span class="string">'themselves'</span>, <span class="string">'.'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">extract_property</span><span class="params">(prop)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> [prop(word) <span class="keyword">for</span> word <span class="keyword">in</span> sent]</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>extract_property(len)</span><br><span class="line">[<span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">last_letter</span><span class="params">(word)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> word[<span class="number">-1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>extract_property(last_letter)</span><br><span class="line">[<span class="string">'e'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'e'</span>, <span class="string">'e'</span>, <span class="string">','</span>, <span class="string">'d'</span>, <span class="string">'e'</span>, <span class="string">'s'</span>, <span class="string">'l'</span>, <span class="string">'e'</span>, <span class="string">'e'</span>, <span class="string">'f'</span>, <span class="string">'s'</span>, <span class="string">'.'</span>]</span><br></pre></td></tr></table></figure><p>Python’s default arguments are evaluated once when the function is defined, not each time the function is called (like it is in say, Ruby). This means that if you use a mutable default argument and mutate it, you will and have mutated that object for all future calls to the function as well.</p><p><strong>Space-Time Tradeoffs</strong><br>We can test the effeciency using the <code>timeit</code> module. The <code>Timer</code> class has two parameters, a statement which is executed multiple times, and setup code that is executed once at the beginning.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> timeit <span class="keyword">import</span> Timer</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vocab_size = <span class="number">100000</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>setup_list = <span class="string">"import random; vocab = range(%d)"</span> % vocab_size [<span class="number">1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>setup_set = <span class="string">"import random; vocab = set(range(%d))"</span> % vocab_size [<span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>statement = <span class="string">"random.randint(0, %d) in vocab"</span> % (vocab_size * <span class="number">2</span>) [<span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(Timer(statement, setup_list).timeit(<span class="number">1000</span>))</span><br><span class="line"><span class="number">2.78092288971</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(Timer(statement, setup_set).timeit(<span class="number">1000</span>))</span><br><span class="line"><span class="number">0.0037260055542</span></span><br></pre></td></tr></table></figure><p><strong>Dynamic Programming</strong><br>Dynamic programming is a general technique for designing algorithms which is widely used in natural language processing. Dynamic programming is used when a problem contains overlapping sub-problems. Instead of computing solutions to these sub-problems repeatedly, we simply store them in a lookup table.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka1</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">""</span>]</span><br><span class="line">    <span class="keyword">elif</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">"S"</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka1(n<span class="number">-1</span>)]</span><br><span class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka1(n<span class="number">-2</span>)]</span><br><span class="line">        <span class="keyword">return</span> s + l</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka2</span><span class="params">(n)</span>:</span></span><br><span class="line">    lookup = [[<span class="string">""</span>], [<span class="string">"S"</span>]]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> lookup[i+<span class="number">1</span>]]</span><br><span class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> lookup[i]]</span><br><span class="line">        lookup.append(s + l)</span><br><span class="line">    <span class="keyword">return</span> lookup[n]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka3</span><span class="params">(n, lookup=&#123;<span class="number">0</span>:[<span class="string">""</span>], <span class="number">1</span>:[<span class="string">"S"</span>]&#125;)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n <span class="keyword">not</span> <span class="keyword">in</span> lookup:</span><br><span class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka3(n<span class="number">-1</span>)]</span><br><span class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka3(n<span class="number">-2</span>)]</span><br><span class="line">        lookup[n] = s + l</span><br><span class="line">    <span class="keyword">return</span> lookup[n]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> memoize</span><br><span class="line"><span class="meta">@memoize</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">virahanka4</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">""</span>]</span><br><span class="line">    <span class="keyword">elif</span> n == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> [<span class="string">"S"</span>]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        s = [<span class="string">"S"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka4(n<span class="number">-1</span>)]</span><br><span class="line">        l = [<span class="string">"L"</span> + prosody <span class="keyword">for</span> prosody <span class="keyword">in</span> virahanka4(n<span class="number">-2</span>)]</span><br><span class="line">        <span class="keyword">return</span> s + l</span><br></pre></td></tr></table></figure><p>Four Ways to Compute Sanskrit Meter: (i) recursive; (ii) bottom-up dynamic programming; (iii) top-down dynamic programming; and (iv) built-in memoization.</p><h2 id="Categorizing-and-Tagging-Words"><a href="#Categorizing-and-Tagging-Words" class="headerlink" title="Categorizing and Tagging Words"></a>Categorizing and Tagging Words</h2><p>Universal Part-of-Speech Tagset<br>| Tag | Meaning | English Examples |<br>| —- | —- | —- |<br>| ADJ | adjective | new, good |<br>| ADP | adposition | on, of |<br>| ADV | adverb | really, already |<br>| CONJ | conjunction | and, or |<br>| DET | determiner, article | the, some |<br>| NOUN | noun | year, home |<br>| NUM | numeral | twenty-four, fourth |<br>| PRT | particle | at, on |<br>| PRON | pronoun | he, their |<br>| VERB | verb | is, say |<br>| . | punctuation marks | .,;! |<br>| X | other | gr8, univeristy |</p><p>Some related functions:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># nltk.pos_tag()</span></span><br><span class="line">text = word_tokenize(<span class="string">"They refuse to permit us to obtain the refuse permit"</span>)</span><br><span class="line">nltk.pos_tag(text)</span><br><span class="line"></span><br><span class="line"><span class="comment"># nltk.tag.str2tuple()</span></span><br><span class="line">tagged_token = nltk.tag.str2tuple(<span class="string">'fly/NN'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># nltk.corpus.tagged_words()</span></span><br><span class="line">nltk.corpus.brown.tagged_words()</span><br><span class="line">nltk.corpus.brown.tagged_words(tagset=<span class="string">'universal'</span>)</span><br></pre></td></tr></table></figure><h3 id="Automatic-Tagging"><a href="#Automatic-Tagging" class="headerlink" title="Automatic Tagging"></a>Automatic Tagging</h3><p>The Default Tagger<br>Default taggers assign their tag to every single word, even words that have never been encountered before.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">default_tagger = nltk.DefaultTagger(<span class="string">'NN'</span>)</span><br><span class="line">default_tagger.tag(tokens)</span><br></pre></td></tr></table></figure><p>The Regular Expression Tagger<br>The regular expression tagger assigns tags to tokens on the basis of matching patterns.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">patterns = [</span><br><span class="line">    (<span class="string">r'.*s$'</span>, <span class="string">'NNS'</span>)</span><br><span class="line">    (<span class="string">r'.*ing$'</span>, <span class="string">'VBG'</span>)</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>The Lookup Tagger<br>Find the most frequent words and store their most likely tag. Then use this information as the model for a “lookup tagger”(an NLTK <code>UnigramTagger</code>).<br>For those words not among the most frequent words, it’s okay to assign the default tag of <code>NN</code>. In other words, we want to use the lookup table first, and if it is unable to assign a tag, then use the default tagger, a process known as backoff (5). </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">fd = nltk.FreqDist(brown.words(categories=<span class="string">'news'</span>))</span><br><span class="line">cfd = nltk.ConditionalFreqDist(brown.tagged_words(categories=<span class="string">'news'</span>))</span><br><span class="line">most_freq_words = fd.most_common(<span class="number">100</span>)</span><br><span class="line">likely_tags = dict((word, cfd[word].max()) <span class="keyword">for</span> (word, _) <span class="keyword">in</span> most_freq_words)</span><br><span class="line">baseline_tagger = nltk.UnigramTagger(model=likely_tags,</span><br><span class="line">                                     backoff=nltk.DefaultTagger(<span class="string">'NN'</span>))</span><br></pre></td></tr></table></figure><h3 id="N-Gram-Tagging"><a href="#N-Gram-Tagging" class="headerlink" title="N-Gram Tagging"></a>N-Gram Tagging</h3><p>Unigram Tagging<br>Unigram taggers are based on a simple statistical algorithm: for each token, assign the tag that is most likely for that particular token.<br>An n-gram tagger is a generalization of a unigram tagger whose context is the current word together with the part-of-speech tags of the <em>n-1</em> preceding tokens.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">unigram_tagger = nltk.UnigramTagger(train_sents)</span><br><span class="line">bigram_tagger = nltk.BigramTagger(train_sents)</span><br></pre></td></tr></table></figure><p>Note:<br>n-gram taggers should not consider context that crosses a sentence boundary. Accordingly, NLTK taggers are designed to work with lists of sentences, where each sentence is a list of words. At the start of a sentence, t_{n-1} and preceding tags are set to <code>None</code>.</p><p>As <em>n</em> gets larger, the specificity of the contexts increases, as does the chance that the data we wish to tag contains contexts that were not present in the training data. This is known as the <em>sparse data</em> problem, and is quite pervasive in NLP. As a consequence, there is a trade-off between the accuracy and the coverage of our results (and this is related to the precision/recall trade-off in information retrieval).<br>One way to address the trade-off between accuracy and coverage is to use the more accurate algorithms when we can, but to fall back on algorithms with wider coverage when necessary. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Try tagging the token with the bigram tagger.</span></span><br><span class="line"><span class="comment"># 2. If the bigram tagger is unable to find a tag for the token, try the unigram tagger.</span></span><br><span class="line"><span class="comment"># 3. If the unigram tagger is also unable to find a tag, use a default tagger.</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t0 = nltk.DefaultTagger(<span class="string">'NN'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t1 = nltk.UnigramTagger(train_sents, backoff=t0)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t2 = nltk.BigramTagger(train_sents, backoff=t1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We can further specify that a tagger needs to see more than one instance of a context in order to retain it.</span></span><br><span class="line">nltk.BigramTagger(sents, cutoff=<span class="number">2</span>, backoff=t1)</span><br><span class="line"><span class="comment"># it will discard contexts that have only been seen once or twice.</span></span><br></pre></td></tr></table></figure><h2 id="Classification"><a href="#Classification" class="headerlink" title="Classification"></a>Classification</h2><p>Classification is the task of choosing the correct class label for a given input. In basic classification tasks, each input is considered in isolation from all other inputs, and the set of labels is defined in advance.<br>E.g., deciding whether an email is spam or not; deciding what the topic of a news article is, from a fixed list of topic areas such as “sports,” “technology,” and “politics”; deciding whether a given occurrence of the word bank is used to refer to a river bank, a financial institution, the act of tilting to the side, or the act of depositing something in a financial institution.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The example of gender identification</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># the function of extracting possible features</span></span><br><span class="line"><span class="comment"># e.g., the name's last/first letter, the length of the name,</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gender_features</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;<span class="string">'last_letter'</span>: word[<span class="number">-1</span>]&#125;</span><br><span class="line">    <span class="comment"># return &#123;'first_letter': word[0]&#125;</span></span><br><span class="line">    <span class="comment"># return &#123;'word_length': len(word)&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get the name data</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> names</span><br><span class="line">labeled_names = ([(name, <span class="string">'male'</span>) <span class="keyword">for</span> name <span class="keyword">in</span> names.words(<span class="string">'male.txt'</span>)] + </span><br><span class="line">                 [(name, <span class="string">'female'</span>) <span class="keyword">for</span> name <span class="keyword">in</span> names.words(<span class="string">'female.txt'</span>)])</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">random.shuffle(labeled_names)</span><br><span class="line"></span><br><span class="line"><span class="comment"># seperate the training set and test set</span></span><br><span class="line">featuresets = [(gender_features(n), gender) <span class="keyword">for</span> (n, gender) <span class="keyword">in</span> labeled_names]</span><br><span class="line"><span class="comment"># train_set, test_set = featuresets[500:], featuresets[:500]</span></span><br><span class="line"><span class="comment"># nltk.classify.apply_features returns an object that acts like a list but does not store all the feature sets in memory</span></span><br><span class="line"><span class="keyword">from</span> nltk.classify <span class="keyword">import</span> apply_features</span><br><span class="line">train_set = apply_features(gender_features, labeled_names[<span class="number">500</span>:])</span><br><span class="line">test_set = apply_features(gender_features, labeled_names[:<span class="number">500</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># use Naive Bayes classifier and see the accuracy</span></span><br><span class="line">classifier = nltk.NaiveBayesClassifier.train(train_set)</span><br><span class="line">nltk.classify.accuracy(classifier, test_set)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accuracy:</span></span><br><span class="line"><span class="comment"># last letter: 0.774</span></span><br><span class="line"><span class="comment"># first letter: 0.622</span></span><br><span class="line"><span class="comment"># name length: 0.618</span></span><br></pre></td></tr></table></figure><p>The training set is used to train the model, and the dev-test set is used to perform error analysis. The test set serves in our final evaluation of the system. Just as common machine learning does.</p><hr><p>The following content seems to focus on some methods provided by NLTK. And to learn the principles like decision tree, which is not covered in Andrew Ng’s course, I’d like to turn to Hands-on Machine Learning with Scikit-Learn and TensorFlow rather than this book. And I’ll write a new post recording notes on that book:D<br>(Well, I’ll come back to continue updating this post when it’s necessary to.)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;This note is based on &lt;a href=&quot;http://www.nltk.org/book/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Natural Language Processing with Python - Analyz
      
    
    </summary>
    
      <category term="Natural Language Processing" scheme="http://haelchan.me/categories/Natural-Language-Processing/"/>
    
    
      <category term="Python" scheme="http://haelchan.me/tags/Python/"/>
    
      <category term="NLTK" scheme="http://haelchan.me/tags/NLTK/"/>
    
  </entry>
  
  <entry>
    <title>Overview of Linguistics - Reading Notes of The Study of Language(Second edition)</title>
    <link href="http://haelchan.me/2018/07/16/reading-note-of-the-study-of-language-2e/"/>
    <id>http://haelchan.me/2018/07/16/reading-note-of-the-study-of-language-2e/</id>
    <published>2018-07-17T03:03:01.000Z</published>
    <updated>2018-10-18T15:08:35.000Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/images/linguistic/outline.png" alt><br>I got to know the book <em>The Study of Language</em> on social network(Qzone) and luckily borrowed it from my schoolmate. Before diving into the field of natural language processing, I’d like to get an roughly understanding of linguistics, and this book is a good choice. In this post I’ll take notes on the The Study of Language second edition(though it has sixth edition up to date).</p><h2 id="The-origins-of-language"><a href="#The-origins-of-language" class="headerlink" title="The origins of language"></a>The origins of language</h2><h3 id="Speculations"><a href="#Speculations" class="headerlink" title="Speculations"></a>Speculations</h3><p>Well, we simply do not know how language originated. Some speculations about the origins of language: <strong>the divine source</strong>, <strong>the natural-sound source</strong> and <strong>the oral-gesture source</strong>.<br>In a different view, <strong>glossogenetics</strong> and <strong>physiological adapation</strong> shows that human teeth(upright), lips(intricate muscle interlacing), larynx(with pharynx) and brain(lateralized) provide possibilities of language.</p><h3 id="Major-functions-of-language-use"><a href="#Major-functions-of-language-use" class="headerlink" title="Major functions of language use"></a>Major functions of language use</h3><p><strong>Interactional function</strong>: to do with how humans use language to interact with each other, socially or emotionally; how they indicate friendliness, co-operation or hostility, or annoyance, pain, or pleasure.<br><strong>Transactional function</strong>: use linguistic abilities to communicate knowledge, skills and information.</p><h2 id="The-development-of-writing"><a href="#The-development-of-writing" class="headerlink" title="The development of writing"></a>The development of writing</h2><h3 id="Pictograms-and-ideograms"><a href="#Pictograms-and-ideograms" class="headerlink" title="Pictograms and ideograms"></a>Pictograms and ideograms</h3><p><strong>Pictogram</strong>: picture-writing<br><strong>Ideograms</strong>: idea-writing<br>The distinction between pictograms and ideograms is essentially a difference in the relationship between the symbol and the entity it represents. The more ‘picture-like’ forms are pictograms, the more abstract, derived forms are ideograms. A key property of both pictograms and ideograms is that they do not represent words or sounds in a particular language.</p><h3 id="Logograms"><a href="#Logograms" class="headerlink" title="Logograms"></a>Logograms</h3><p>When symbols come to be used to represent words in a language, they are described as examples of word-writing, or <strong>logograms</strong>.<br><strong>Cuneiform</strong> writing: normally referred to when the expression “the earliest known writing system” is used.<br><strong>Characters</strong>(Chinese writing): the longest continuous history of use as a writing system.</p><h3 id="Syllabic-writing-and-alphabetic-writing"><a href="#Syllabic-writing-and-alphabetic-writing" class="headerlink" title="Syllabic writing and alphabetic writing"></a>Syllabic writing and alphabetic writing</h3><p>To avoid substantial memory load, some principled method is required to go from symbols which represent words(i.e. a logographic system) to a set of symbols which represent sounds(i.e. a phonographic system).<br>When a writing system employs a set of symbols which represent the pronunciations of syllabic writing. (There are no purely syllabic writing systems in use today.)<br><strong>Alphabetic writing</strong>: the symbols can be used to represent single sound types in a language. An alphabet is essentially a set of written symbols which each represent a single type of sound.</p><h2 id="The-properties-of-language"><a href="#The-properties-of-language" class="headerlink" title="The properties of language"></a>The properties of language</h2><h3 id="Communicative-versus-informative"><a href="#Communicative-versus-informative" class="headerlink" title="Communicative versus informative"></a>Communicative versus informative</h3><p><strong>Communicative</strong>: intentionally communicating something<br><strong>Informative</strong>: unintentionally sent signals</p><h3 id="Unique-properties"><a href="#Unique-properties" class="headerlink" title="Unique properties"></a>Unique properties</h3><p><strong>Displacement</strong>: It allows the users of language to talk about things and events not present in the immediate environment. It enables us to talk about things and places whose existence we cannot even be sure of.<br><strong>Arbitrariness</strong>: A property of linguistic signs is their arbitrary relationship with the objects they are used to indicate. They do not, in any way, ‘fit’ the objects they denote.<br><strong>Productivity</strong>(creativity/open-endedness): The potential number of utterances in any human language is infinite.<br><strong>Culture transmission</strong>: Humans are not born with the ability to produce utterances in a specific language.<br><strong>Discreteness</strong>: Each sound in the language is treated as discrete.<br><strong>Duality</strong>: Language is organized at two levels or layers simultaneously. At one level, we have distinct sounds, and at another level, we have distinct meanings.</p><h3 id="Other-properties"><a href="#Other-properties" class="headerlink" title="Other properties"></a>Other properties</h3><p>The use of the <strong>vocal-auditory channel</strong>: Human linguistic communication is typically generated via the vocal organs and perceived via the ears.<br><strong>Reciprocity</strong>: Any speaker/sender of a linguistic signal can also be a listener/receiver.<br><strong>Specialization</strong>: Linguistic signals do not normally serve any other type of purpose, such as breathing or feeding.<br><strong>Non-directionality</strong>: Linguistic signals can be picked up by anyone within hearing, even unseen.<br><strong>Rapid fade</strong>: Linguistic signals are produced and disappear quickly.<br>Most of these are properties of the spoken language, but not of the written language.</p><h2 id="The-sounds-of-language"><a href="#The-sounds-of-language" class="headerlink" title="The sounds of language"></a>The sounds of language</h2><p><strong>Phonetics</strong>: the general study of the characteristics of speech sounds<br><strong>Articulatory phonetics</strong>: the study of how speech sounds are made, or ‘articulated’<br><strong>Acoustic phonetics</strong>: deals with the physical properties of speech as sound waves ‘in the air’<br><strong>Auditory(Perceptual) phonetics</strong>: deals with the perception, via the ear, of speech sounds<br><strong>Forensic phonetics</strong>: has applications in legal cases involving speaker identification and the analysis of recorded utterances</p><h3 id="Voiced-and-voiceless"><a href="#Voiced-and-voiceless" class="headerlink" title="Voiced and voiceless"></a>Voiced and voiceless</h3><p>When the vocal cords are spread apart, the air from the lungs passes between them unimpeded. Sounds produced in this way are described as <strong>voiceless</strong>.<br>When the vocal cords are drawn together, the air from the lungs repeatedly pushes them apart as it passes through, creating a vibration effect. Sounds produced in this way are described as <strong>voiced</strong>.</p><h3 id="Place-of-articulation"><a href="#Place-of-articulation" class="headerlink" title="Place of articulation"></a>Place of articulation</h3><p><strong>Bilabials</strong><br>These are sounds formed using both upper and lower lips.<br>Includes: <script type="math/tex">\text{[b] [p] [m] [w]}</script></p><p><strong>Labiodentals</strong><br>These are sounds formed with the upper teeth and the lower lip.<br>Includes: <script type="math/tex">\text{[f] [v]}</script></p><p><strong>Dentals</strong><br>These sounds are formed with the tongue tip behind the upper front teeth.<br>Includes: <script type="math/tex">[\theta] \text{[eth]}</script> (Sorry can’t type the phonetic symbol directly. The phonetic of <em>th</em> in <em>the,there,then</em>)</p><p><strong>Alveolars</strong><br>These are sounds formed with the front part of the tongue on the alveolar ridge.<br>Includes: <script type="math/tex">\text{[t] [d] [s] [z] [n], [l] [r](at the beginning of words)}</script></p><p><strong>Alveo-palatals</strong><br>These are sounds formed with the tongue at the very front of the palate, near the alveolar ridge.<br>Includes: <script type="math/tex">\text{[sWedge] [cWedge] [zWedge] [jWedge], [y](often described as a palatal)}</script></p><p><strong>Velars</strong><br>These are sounds formed with the back of the tongue against the velum.<br>Includes: <script type="math/tex">\text{[k] [g] [ng]}</script></p><p><strong>Glottals</strong><br>This sound is produced without the active use of the tongue and other parts of the mouth.<br>Includes: <script type="math/tex">\text{[h]}</script></p><h3 id="Manner-of-articulation"><a href="#Manner-of-articulation" class="headerlink" title="Manner of articulation"></a>Manner of articulation</h3><p><strong>Stops</strong><br>Consonant sound resulting from a blocking or stopping effect on the airstream.<br>Includes: <script type="math/tex">\text{[p][b][t][d][k][g]}</script></p><p><strong>Fricatives</strong><br>As the air is pushed through, a type of friction is produced.<br>Includes: <script type="math/tex">\text{[f] [v] [s] [z]}\ [\theta] \text{[eth] [sWedge] [zWedge]}</script></p><p><strong>Affricates</strong><br>Combine a brief stopping of the airstream with an obstructed release which causes some friction.<br>Includes: <script type="math/tex">\text{[cWedge] [jWedge]}</script></p><p><strong>Nasals</strong><br>The velum is lowered and the airstream is allowed to flow out through the nose.<br>Includes: <script type="math/tex">\text{[m] [n] [ng]}</script></p><p><strong>Approximants</strong><br>The articulation of each is strongly influenced by the following vowel sound.<br>Includes: <script type="math/tex">\text{[w] [y] [l] [r] [h]}</script></p><p><em>The contents of vowel and the sound patterns are omitted</em></p><h2 id="Words-and-word-formation-process"><a href="#Words-and-word-formation-process" class="headerlink" title="Words and word-formation process"></a>Words and word-formation process</h2><p><strong>Coinage</strong><br>The invention of totally new terms(The most typical sources are invented trade names for one company’s product which become general terms for any version of that product).<br><em>aspirin, nylon, zipper</em> </p><p><strong>Borrowing</strong><br>Take over of words from other languages.<br>A special type of borrowing is described as load-translation, or calque. In this process, there is a direct translation of the elements of a word into the borrowing language.<br><em>alcohol, boss, piano</em></p><p><strong>Compounding</strong><br>There is a joining of two separate words to produce a single form.<br><em>bookcase, fingerprint, wallpaper</em></p><p><strong>Blending</strong><br>Blending is typically accomplished by taking only the beginning of one word and joining it to the end of the other word.<br><em>smog(smoke+fog), bit(binary+digit), brunch(breakfast+lunch)</em></p><p><strong>Clipping</strong><br>This occurs when a word of more than one syllable is reduced to a shorter form, often in casual speech.<br><em>fax(facsimile), gas(gasoline), ad(advertisement)</em></p><p><strong>Backformation</strong><br>A word of one type(usually a noun) is reduced to form another word of a different type(usually a verb).<br><em>televise(television), donate(donation), opt(option)</em></p><p><strong>Conversion</strong><br>A change in the function of a word.<br><em>paper(noun-&gt;verb), guess(verb-&gt;noun), empty(adjective-&gt;verb)</em></p><p><strong>Acronyms</strong><br>New words formed from the initial letters of a set of other words.<br><em>CD(compact disk), radar(radio detecting and ranging), ATM(automatic tell machine)</em></p><p><strong>Derivation</strong><br>Accomplished by means of a large number of affixes which are not usually given separate listings in dictionaries.<br><em>prefix</em>: added to the beginning of the word <em>un-</em><br><em>suffix</em>: added to the end of the word <em>-ish</em><br><em>infix</em>: incorporated inside another word <em>unfuckingbelievable</em></p><h2 id="Morphology"><a href="#Morphology" class="headerlink" title="Morphology"></a>Morphology</h2><p><strong>Morphology</strong>, which literally means the ‘study of forms’, was originally used in biology, but, since the middle of the nineteenth century, has also been used to describe that type of investigation which analyzes all those <strong>morphemes</strong> which are used in a language.<br>The definition of a morpheme is “a minimal unit of meaning or grammatical function”.</p><ul><li>morphemes<ul><li>free<ul><li>lexical</li><li>functional</li></ul></li><li>bound<ul><li>derivational</li><li>inflectional</li></ul></li></ul></li></ul><h3 id="Free-and-bound-morphemes"><a href="#Free-and-bound-morphemes" class="headerlink" title="Free and bound morphemes"></a>Free and bound morphemes</h3><h4 id="Free-morphemes"><a href="#Free-morphemes" class="headerlink" title="Free morphemes"></a>Free morphemes</h4><p>Morphemes which can stand by themselves as single words.</p><p><strong>Lexical morphemes</strong>: a set of ordinary nouns, adjectives and verbs which we think of as the words which carry the ‘content’ of messages we convey.<br><em>boy, man, house, tiger, sad, long, yellow, sincere, open, look, follow, break</em><br>‘Open’ class of words(we can add new lexical morphemes to the language rather easily).</p><p><strong>Functional morphemes</strong>: a set consists largely of the functional words in the language such as conjunctions, prepositions, articles and pronouns.<br><em>and, but, when, because, on, near, above, in, the, that, it</em><br>‘Closed’ class of words(we almost never add new functional morphemes to the language).</p><h4 id="Bound-morphemes"><a href="#Bound-morphemes" class="headerlink" title="Bound morphemes"></a>Bound morphemes</h4><p>Morphemes which cannot normally stand alone, but are typically attached to another form.</p><p><strong>Derivational morphemes</strong>: used to makek new words in the language and are often used to make words of a different grammatical category from the stem(when affixes are used with bound morphemes, the basic word-form involved is technically known as the stem).<br>(<em>-ness, -ful, -less, -ish, -ly, re-, pre-, ex-, dis-, co-, un-</em>)</p><p><strong>Inflectional morphemes</strong>: to indicate aspects of the grammatical function of a word.<br>Noun+ <em>-‘s(possessive), -s(plural)</em><br>Verb+ <em>-s(3rd person present singular), -ing(present participle), -ed(past tense), -en(past participle)</em><br>Adjective+ <em>-est(superlative), -er(comparative)</em></p><p>An inflectional morpheme never changes the grammatical category of a word. A derivational morpheme can change the grammatical category of a word.</p><h2 id="Phrases-and-sentences-grammar"><a href="#Phrases-and-sentences-grammar" class="headerlink" title="Phrases and sentences: grammar"></a>Phrases and sentences: grammar</h2><p>We need a way of describing the structure of phrases and sentences which will account for all of the grammatical sequences and rule out all ungrammatical sequences. Providing such an account involves us in the study of <strong>grammar</strong>.</p><p>The part of speech<br><strong>Nouns</strong> are words used to refer to people, objects, creatures, places, qualities, phenomena and abstract ideas as if they were all ‘things’.<br><strong>Adjectives</strong> are words used, typically with nouns, to provide more information about the ‘things’ referred to. <em>(happy, large, cute)</em><br><strong>Verbs</strong> are words used to refer to various kinds of actions<em>(run, jump)</em> and states<em>(be, seem)</em> involving the ‘things’ in events.<br><strong>Adverbs</strong> are words used to provide more information about the actions and events<em>(slowly, suddenly)</em>. Some adverb<em>(really, very)</em> are also used twith adjectives to modify the information about ‘things’.<br><strong>Prepositions</strong> are words<em>(at, in, on, near, with, without)</em> used with nouns in phrases providing information about time, place and other connections involving actions and things.<br><strong>Pronouns</strong> are words<em>(me, they, he, himself, this, it)</em> used in place of noun phrases, typically referring to things already known.<br><strong>Conjunctions</strong> are words<em>(and, but, although, if)</em> used to connect, and indicate relationships between events and things.</p><h3 id="Traditional-grammar"><a href="#Traditional-grammar" class="headerlink" title="Traditional grammar"></a>Traditional grammar</h3><p>In addition to the terms used for the parts of speech, traditional grammatical analysis also gave us a number of other categories, including ‘number’, ‘person’, ‘tense’, ‘voice’ and ‘gender’.<br><strong>Number</strong> is whether the noun in singular or plural.<br><strong>Person</strong> covers the distinctions of first person(involving the speaker), second person(involving the hearer) and third person(involving any others).<br><strong>Tense</strong>: present tense, past tense, future tense.<br><strong>Voice</strong>: active voice, passive voice<br><strong>Gender</strong>: describe the relationship in terms of natural gender, mainly derived from a biological distinction between male and female. (Grammatical gender is common but may not be as appropriate in describing English)</p><h3 id="The-prescriptive-approach"><a href="#The-prescriptive-approach" class="headerlink" title="The prescriptive approach"></a>The prescriptive approach</h3><p>The view of grammar as a set of rules for the ‘proper’ user of a language is still to be found today and may be best characterized as the prescriptive approach.<br>Some familar examples of prescriptive rules for English sentences:</p><ol><li>You must not split an infinitive.</li><li>You must not end a sentence with a preposition.</li></ol><h3 id="The-descriptive-approach"><a href="#The-descriptive-approach" class="headerlink" title="The descriptive approach"></a>The descriptive approach</h3><p>Analysts collect samples of the language they are interested in and attempt to describe the regular structures of the language as it is used, not according to some view of how it should be used. This is called the descriptive approach and it is the basis of most modern attempts to characterize the structure of different languages.<br><strong>Structural analysis</strong><br>One type of descriptive approach is called structural analysis and its main concern is to investigate the distribution of forms(e.g., morphemes) in a language. The method employed involves the use of ‘test-frams’ which can be sentences with empty slots in them.<br><strong>Immediate constituent analysis</strong><br>An approach with the same descriptive aims is called immediate constituent analysis. The technique employed in this approach is designed to show how small constituents(or components) in sentences go together to form larger constituents.<br>The analysis of the constituent structure of the sentence can be represented in different types of diagrams. (Simple diagram, labeled and bracketed sentences, tree diagrams, discussed in the following chapter in detail)</p><h2 id="Syntax"><a href="#Syntax" class="headerlink" title="Syntax"></a>Syntax</h2><p>The word ‘syntax’ came originally from Greek and literally meant ‘a setting out together’ or ‘arrangement’. </p><p><strong>Generative grammar</strong><br>There have been attempts to produce a particular type of grammar which would have a very explicit system of rules specifying what combinations of basic elements would result in well-formed sentences since the 1950s.<br>Given an algebraic expression <script type="math/tex">3x+2y</script>, the simple algebraic expression can generate an endless set of values, by following the simple rules of arithmetic. The endless set of such results is ‘generated’ by the operation of the explicitly formalized rules. If the sentences of a language can be seen as a comparable set, then there must be a set of explicit rules which yield those sentences. Such a set of explicit rules is a generative grammar.</p><p><strong>Some properties of the grammar</strong></p><ul><li>The grammar will generate all the well-formed syntactic structures(e.g. sentences) of the language and fail to generate any ill-formed structures.</li><li>The grammar will have a finite(i.e. limited) number of rules, but will be capable of generating an infinite number of well-formed structures.</li><li>Property of recursion: the capacity to be applied more than once in generating a structure.</li><li>The grammar will have to capture the fact that a sentence can have another sentence inside it, or a phrase can have another phrase of the same type inside it.</li><li>The grammar should be capable of revealing how superficially didstinct sentences are closely related.</li><li>The grammar should be capable of revealing how some superficially similar sentences are in fact distinct.</li></ul><h3 id="Symbols-and-abbreviations-in-syntactic-description"><a href="#Symbols-and-abbreviations-in-syntactic-description" class="headerlink" title="Symbols and abbreviations in syntactic description"></a>Symbols and abbreviations in syntactic description</h3><p>S       sentence<br>N       noun<br>Pro     pronoun<br>PN      proper noun<br>V       verb<br>Adj     adjective<br>Art     article<br>Adv     adverb<br>Prep    preposition<br>NP      noun phrase<br>VP      verb phrase<br>PP      prepositional phrase<br>*      ungrammatical sequence<br>-&gt;      consists of<br>()      optional constituent<br>{}      one and only one of these constituents must be selected</p><p>(May be a bit different with the symbols used in the post <a href="http://haelchan.me/2018/03/31/NLP-note/">COMS W4705 Natural Language Processing Note</a>, but it doesn’t matter. And the tree diagram is introduced in that post as well).</p><h2 id="Semantics"><a href="#Semantics" class="headerlink" title="Semantics"></a>Semantics</h2><p>Semantics is the study of the meaning of the words, phrases and sentences. Linguistic semantics deals with the conventional meaning conveyed by the use of words and sentences of a language.</p><h3 id="Semantic-features"><a href="#Semantic-features" class="headerlink" title="Semantic features"></a>Semantic features</h3><p>Analyze meaning in terms of semantic features. Features such as +<em>animate</em>, -<em>animate</em>; +<em>human</em>, -<em>human</em>, for example, can be treated as the basic features involved in differentiaiting the meanings of each word in the language from every other word.<br>This approach gives us the ability to predict what nouns would make sentence semantically odd.<br>However, for many words in a language it may not be so easy to come up with neat components of meaning. Part of the problem seems to be that the approach involves a view of words in a language as some sort of ‘containers’, carrying meaning-components.</p><h3 id="Semantic-roles"><a href="#Semantic-roles" class="headerlink" title="Semantic roles"></a>Semantic roles</h3><p>Instead of thinking of the words as ‘containers’ of meaning, we can look at the ‘roles’ they fulfill within the situation described by a sentence.</p><p><strong>agent</strong>: the entity that performs the action<br><strong>theme</strong>: the entity that is involved in or affected by the action<br><strong>instrument</strong>: if an agent uses another entity in performing an action, that other entity fills the role of instrument<br><strong>experiencer</strong>: when a noun phrase designates an entity as the person who has a feeling, a perception or a state<br><strong>location</strong>: where an entity is<br><strong>source</strong>: where an entity moves from<br><strong>goal</strong>: where an entity moves to</p><h3 id="Lexical-relations"><a href="#Lexical-relations" class="headerlink" title="Lexical relations"></a>Lexical relations</h3><p>Characterize the meaning of a word not in terms of its component features, but in terms of its relationship to other words. This procedure has also been used in the semantic description of languages and is treated as the analysis of lexical relations.</p><p><strong>Synonymy</strong><br>Two or more forms with very closely related meanings, which are often, but not always, intersubstituatable in sentences.</p><p><strong>Antonymy</strong><br>Two forms with opposite meanings<br><em>gradable antonyms</em> such as the pair big-small, can be used in comparative constructions like bigger than-smaller than. Also, the negative of one member of the gradable pair does not necessarily imply the other.<br><em>non-gradable antonyms</em> also called ‘complementatry pairs’, comparative constructions are not normally used, and the negative of one member does imply the other.</p><p><strong>Hyponymy</strong><br>When the meaning of one form is included in the meaning of another, the relationship is described as hyponymy. (The meaning of <em>animal</em> is ‘included’ in the meaning of <em>dog</em>. Or, <em>dog</em> is a hyponym of <em>animal</em>.)<br>When we consider hyponymous relations, we are essentially looking at the meaning of words in some type of hierarchical relationship.<br>From the hierarchical diagram, we can say that two or more terms which share the same superordinate(higher-up) term are co-hyponymss.<br>The relation of hyponymy captures the idea of ‘is a kind of’.</p><p><strong>Prototype</strong><br>The concept of a prototype helps explain the meaning of certain words not in terms of component features, but in terms of resemblance to the clearest exemplar. (For many American English speakers, the prototype of ‘bird’ is the robin.)</p><p><strong>Homophony</strong><br>Two or more different (written) forms have the same pronunciation.(<em>bare-bear, meat-meet</em>)</p><p><strong>Homonymy</strong><br>One form(written and spoken) has two or more unrelated meanings.(<em>bank: of a river; financial instituion, race: contest of speed; ethnic group</em>)</p><p><strong>Polysemy</strong><br>One form(written or spoken) has multiple meanings which are all related by extension.(<em>head, foot</em>)</p><p>(Some other lexical relations like <em>meronyms</em> and <em>holonyms</em> are introduced in this <a href="http://haelchan.me/2018/07/23/nlp-with-python-and-nltk/">post</a>)</p><h3 id="Metonymy"><a href="#Metonymy" class="headerlink" title="Metonymy"></a>Metonymy</h3><p>Relationship between words based simply on a close connection in everyday experience. That close connection can be based on a container-contents relation(<em>bottle-coke; can-juice</em>), a whole-part relation(<em>car-wheel; hourse-roof</em>) or a representative-symbol relationship(<em>king-crown; the President-the White House</em>).<br>Many examples of metonymy are highly conventionalized and easy to interpret. However, many others depend on an ability to infer what the speaker has in mind.</p><h3 id="Collocation"><a href="#Collocation" class="headerlink" title="Collocation"></a>Collocation</h3><p>One way we seem to organize our knowledge of words is simply in terms of collocation, or frequently occurring together.<br>(<em>butter-bread, needle-thread, salt-pepper</em>)</p><h2 id="Pragmatics"><a href="#Pragmatics" class="headerlink" title="Pragmatics"></a>Pragmatics</h2><p>When we read or hear pieces of language, we normally try to understand not only what the words mean, but what the writer or speaker of those words intended to convey. The study of ‘intended speaker meaning’ is called pragmatics.</p><h3 id="Contexts"><a href="#Contexts" class="headerlink" title="Contexts"></a>Contexts</h3><p><strong>Linguistic context</strong>(<strong>co-text</strong>)<br>The co-text of a word is the set of other words used in the same phrase or sentence. This surrounding co-text has a strong effect on what we think the word means.</p><p><strong>Physical context</strong><br>Our understanding of much of what we read and hear is tied to the physical context, particularly the time and place, in which we encounter linguistic expressions.</p><h3 id="Deixis"><a href="#Deixis" class="headerlink" title="Deixis"></a>Deixis</h3><p>There are some words in the language that cannot be interpreted at all unless the physical context, especially the physical context of the speaker, is known. Expressions, which depend for their interpretation on the immediate physical context in which they were uttered, are very obvious examples of bits of language which we can only understand in terms of speaker’s intended meaning. These are technically known as <strong>deictic expressions</strong>.<br><strong>Person deixis</strong>: used to point to a person(<em>me, you, him, them</em>)<br><strong>Place deixis</strong>: (<em>here, there, yonder</em>)<br><strong>Time deixis</strong>: (<em>now, then, tonight, last week</em>)</p><h3 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h3><p>An act by which a speaker(or writer) uses language to enable a listener(or reader) to identify something.<br>We can use names associated with things to refer to people and names of people to refer to things. The key process here is called inference. An inference is any additional information used by the listener to connect what is said to whawt must be meant.</p><h3 id="Anaphora"><a href="#Anaphora" class="headerlink" title="Anaphora"></a>Anaphora</h3><p>When we establish a referent and subsequently refer to the same object, we have a particular kind of referential relationship. The second referring expression is an example of anaphora and the first mention is called the antecedent.<br>Anaphora can be defined as subsequent reference to an already introduced entity. Mostly we use anaphora in texts to maintain reference.</p><h3 id="Presupposition"><a href="#Presupposition" class="headerlink" title="Presupposition"></a>Presupposition</h3><p>What a speaker assumes is true or is known by the hearer can be described as a presupposition.<br><strong>Constancy under negation</strong>: although two sentences have opposite meanings, the underlying presupposition remains true in both.</p><h3 id="Speech-acts"><a href="#Speech-acts" class="headerlink" title="Speech acts"></a>Speech acts</h3><p>In very general terms, we can usually recognize the type of ‘act’ performed by a speaker in uttering a sentence. The use of the term speech act covers ‘actions’ such as ‘requesting’, ‘commanding’, ‘questioning’ and ‘informing’.</p><div class="table-container"><table><thead><tr><th>Forms</th><th>Functions</th></tr></thead><tbody><tr><td>Interrogative</td><td>Question</td></tr><tr><td>Imperative</td><td>Command(request)</td></tr><tr><td>Declarative</td><td>Statement</td></tr></tbody></table></div><p><strong>Direct speech act</strong>: the forms in the set above is used to perform the corresponding function<br><strong>Indirect speech act</strong>: whenever one of the forms in the set above is used to perform a function other than the one listed beside it</p><h3 id="Politeness"><a href="#Politeness" class="headerlink" title="Politeness"></a>Politeness</h3><p>Politeness is showing awareness of another person’s face(<em>Face</em> is public self-image. This is the emotional and social sense of self that every person has and expects everyone else to recognize).</p><p><strong>face-threatening act</strong>: say something that represents a threat to another person’s self-image (use a direct speech act to order someone to do something)<br><strong>face-saving act</strong>: say something that lessens the possible threat to another’s face (use an indirect speech act instead)</p><p><strong>negative face</strong>: the need to be independent and to have freedom from imposition<br><strong>positive face</strong>: the need to be connected, to belong, to be a member of the group</p><h2 id="Discourse-analysis"><a href="#Discourse-analysis" class="headerlink" title="Discourse analysis"></a>Discourse analysis</h2><p>When we ask how it is that we, as language users, make sense of what we read in texts, understand what speakers mean despite what they say, recognize connected as opposed to jumbled or incoherent discourse, and successfully take part in that complex activity called conversation, we are undertaking what is known as discourse analysis.</p><h3 id="Cohesion"><a href="#Cohesion" class="headerlink" title="Cohesion"></a>Cohesion</h3><p>Texts must have a certain structure which depends on factors quite different from those required in the structure of a single sentence. Some of those factors are described in terms of cohesion, or the ties and connections which exist within texts.<br>Analysis of cohesive links within a text gives us some insight into how writers structure what they want to say, and may be crucial factors in our judgments on whether something is well-written or not.</p><h3 id="Coherence"><a href="#Coherence" class="headerlink" title="Coherence"></a>Coherence</h3><p>There must be some factor which leads us to distinguish connected texts which make sense from those which do not. This factor is usually described as coherence.<br>The key to the concept of coherence is not something which exists in the language, but something which exists in people. It is people who ‘make sense’ of what they read and hear.</p><h3 id="The-co-operative-principle"><a href="#The-co-operative-principle" class="headerlink" title="The co-operative principle"></a>The co-operative principle</h3><p>An underlying assumption in most conversational exchanges seems to be that the participants are co-operating with each other.</p><p>Four maxim<br><strong>Quantity</strong>: Make your contribution as informative as is required, but not more, or less, than is required<br><strong>Quality</strong>: Do not say that which you believe to be false or for which you lack evidence<br><strong>Relation</strong>: Be relevant<br><strong>Manner</strong>: Be clear, brief and orderly</p><h3 id="Background-knowledge"><a href="#Background-knowledge" class="headerlink" title="Background knowledge"></a>Background knowledge</h3><p>We actually create what the text is about, based on our expectations of what normally happens. In attempting to describe this phenomenon, many researchers use the concept of a ‘schema’.<br>A <strong>schema</strong> is a general term for a conventional knowledge structure which exists in memory. We have many schemata which are used in the interpretation of what we experience and what we hear or read about.<br>One particular kind of shcema is a ‘script’. A <strong>script</strong> is essentially a dynamic schema, in which a series of conventional actions takes place.</p><h2 id="Neurolinguistics"><a href="#Neurolinguistics" class="headerlink" title="Neurolinguistics"></a>Neurolinguistics</h2><h3 id="Parts-of-the-brain"><a href="#Parts-of-the-brain" class="headerlink" title="Parts of the brain"></a>Parts of the brain</h3><p><img src="/images/linguistic/brain.jpg" alt></p><p><strong>Broca’s area</strong><br>Paul Broca, a French surgeon, reported in the 1860s that damage to this specific part of the brain was related to extreme difficulty in producing speech. It was noted that damage to the corresponding area on the right hemisphere had no such effect. This finding was first used to argue that language ability must be located in the left hemisphere and since then has been taken as more specifically illustrating that Broca’s area is crucially involved in the production of speech.</p><p><strong>Wernicke’s area</strong><br>Carl Wernicke was a German doctor who, in the 1870s, reported that damage to this part of the brain was found among patients who had speech comprehension difficulties. This finding confirmed the left-hemisphere location of language ability and led to the view that Wernicke’s area is part of the brain crucially involved in the understanding of speech.</p><p><strong>The motor cortex</strong><br>The motor cortex generally controls movement of the muscles. Close to Broca’s area is the part of the motor cortex that controls the articulatory muscles of the face, jaw, tongue and larynx. Evidence that this area is involved in the actual physical articulation of speech comes from the work, reported in the 1950s, of two neurosurgeons, Penfield and Roberts.</p><p><strong>The arcuate fasciculus</strong><br>The arcuate fasciculus is a bundle of nerve fibers. This was also one of Wernicke’s discoveries and forms a crucial connection between Wernicke’s area and Broca’s area.</p><p>The word is heard and comprehended via Wernicke’s area. This signal is then transferred via the arcuate fasciculus to Broca’s area where preparations are made to produce it. A signal is then sent to the motor cortex to physically articulated the word.(A massively oversimplified version of what may actually take place.)</p><h3 id="Tongue-tips-and-slips"><a href="#Tongue-tips-and-slips" class="headerlink" title="Tongue tips and slips"></a>Tongue tips and slips</h3><p><strong>Tip-of-the-tongue</strong><br>There is the tip-of-the-tongue phenomenon in which you feel that some word is just eluding you, that you know the word, but it just won’t come to the surface.<br><strong>Malapropisms</strong><br>The experience which occurs with uncommon terms or names suggests that our ‘word-storage’ may be partially organized on the basis of some phonological information and that some words in that ‘store’ are more easily retrieved than others. When we make mistakes in this retrieval process, there are often strong phonological similarities between the target word and the mistake. Mistakes of this type are sometimes referred to as Malapropisms.<br><strong>Slip-of-the-tongue</strong><br>A slip-of-the-tongue often results in tangled expressions or word reversals. This type of slip is also known as a <strong>Spoonerism</strong>.<br><strong>Tip-of-the-lung</strong><br>Tips-of-the-lung are often simply the result of a sound being carried over from one word to the next, or a sound used in one word in anticipation of its occurrence in the next word.<br><strong>Slip-of-the-ear</strong><br>Slips-of-the-ear can result in misinterpretaion when hearing. </p><h3 id="Aphasia"><a href="#Aphasia" class="headerlink" title="Aphasia"></a>Aphasia</h3><p>Aphasia is defined as an impairment of language function due to localized cerebral damage which leads to difficulty in understanding and/or producing linguistic forms.<br><strong>Broca’s aphasia</strong><br>The type of serious language disorder known as Broca’s aphasia(also called ‘motor aphasia’) is characterized by a substantially reduced amount of speech, distorted articulation and slow, often effortful speech. What is said often consists almost entirely of lexical morphemes(e.g. nouns and verbs). The frequent omission of functional morphemes(e.g. articles, prepositions, inflections) has led to the characterization of this type of aphasia as agrammatic. The grammatical markers are missing.<br>In Broca’s aphasia, comprehension is typically much better than production.</p><p><strong>Wernicke’s aphasia</strong><br>The type of language disorder which results in difficulties in auditory comprehension is sometimes called ‘sensory aphasia’, but is more commonly known as Wernicke’s aphasia. Someone suffering from this disorder can actually produce very fluent speech which is, however, often difficult to make sense of.<br>Difficulty in finding the correct words(sometimes referred to as anomia) is also very common and circumlocution may be used.</p><p><strong>Conduction aphasia</strong><br>The type of aphasia is identified with damage to the arcuate fasciculus and is called conduction aphasia. Individuals suffering from this disorder typically do not have articulation problems. They are fluent, but may have disrupted rhythm because of pauses and hesitations. Comprehension of spoken words is normally good. However, the task of repeating a word phrase(spoken by someone else) will create major difficulty. What is heard and understood cannot be transferred to the speech production area.</p><h3 id="Dichotic-listening"><a href="#Dichotic-listening" class="headerlink" title="Dichotic listening"></a>Dichotic listening</h3><p>An experimental technique which has demonstrated that, for the majority of subjects tested, the language functions must be located in the left hemisphere is called the dichotic listening test. This is a technique which uses the generally established fact that anything experienced on the right-hand side of the body is processed in the left hemisphere of the brain and anything on the left side is processed in the right hemisphere.<br>An experiment is possible in which a subject sits with a set of earphones on and is given two different sound signals simultaneously, one through each earphone. When asked to say what was heard, the subject more often correctly identifies the sound which came via the right ear. This has come to be known as the right ear advantage.</p><p>The explanation of this process proposes that a language signal received through the left ear is first sent to the right hemisphere and then has to be sent over to the left hemisphere(language center) for processing. This nondirect route will take longer than a linguistic signal which is received through the right ear and goes directly to the left hemisphere. First signal to get processed wins.</p><h2 id="Language-history-and-change"><a href="#Language-history-and-change" class="headerlink" title="Language history and change"></a>Language history and change</h2><h3 id="Philology"><a href="#Philology" class="headerlink" title="Philology"></a>Philology</h3><p>the historical study of languages<br><strong>Cognates</strong>: within groups of related languages, we often find close similarities in particular sets of terms. A cognate of a word in one language is a word in another language which has a similar form and is, or was, used with a similar meaning.<br><strong>Comparative reconstruction</strong>: the aim of this procedure is to reconstruct what must have been the original, or ‘proto’ form in the common ancestral language. It’s a bit like trying to work out what the greatgrandmother must have been like on the basis of common features possessed by the set of granddaughters.<br>The <strong>majority principle</strong>: if, in a cognate set, three forms begin with a [p] sound and one form begins with a [b] sound, then our best guess is that the majority have retained the original sound(i.e. [p]), and the minority has changed a little through time.<br>The <strong>most natural development principle</strong>: based on the fact that certain types of sound-change are very common, whereas others are extremely  unlikely.</p><h3 id="Language-change"><a href="#Language-change" class="headerlink" title="Language change"></a>Language change</h3><p><strong>Sound changes</strong><br>metathesis: involves a reversal in position of two adjoining sounds<br>epenthesis: involves the addition of a sound to the middle of a word<br>prothesis: involves the addition of a sound to the beginning of a word</p><p><strong>Lexical changes</strong><br>broadening<br>narrowing</p><h2 id="Sociolinguistics"><a href="#Sociolinguistics" class="headerlink" title="Sociolinguistics"></a>Sociolinguistics</h2><p>In general terms, sociolinguistics deals with the inter-relationships between language and society. It has strong connections to anthropology, through the investigation of language and culture, and to sociology, through the crucial role that language plays in the organization of social groups and institutions. It is also tied to social psychology, particularly with regard to how attitudes and perceptions are expressed and how in-group and out-group behaviors are identified.</p><h3 id="Social-dialects"><a href="#Social-dialects" class="headerlink" title="Social dialects"></a>Social dialects</h3><p>Varieties of language used by groups defined according to class, education, age, sex, and a number of other social parameters.<br>Factors include: social class and education; age and gender; ethnic background; idiolect; style, register and jargon, diglossia.</p><h3 id="Language-and-culture"><a href="#Language-and-culture" class="headerlink" title="Language and culture"></a>Language and culture</h3><p><strong>Linguistic determinism</strong>: language determines thought<br><strong>The Sapir-Whorf hypothesis</strong>: we dissect nature along lines laid down by our native languages.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;img src=&quot;/images/linguistic/outline.png&quot; alt&gt;&lt;br&gt;I got to know the book &lt;em&gt;The Study of Language&lt;/em&gt; on social network(Qzone) and luck
      
    
    </summary>
    
    
      <category term="reading note" scheme="http://haelchan.me/tags/reading-note/"/>
    
      <category term="linguistics" scheme="http://haelchan.me/tags/linguistics/"/>
    
  </entry>
  
  <entry>
    <title>Retrospect &amp; Prospect</title>
    <link href="http://haelchan.me/2018/07/16/retrospect-prospect/"/>
    <id>http://haelchan.me/2018/07/16/retrospect-prospect/</id>
    <published>2018-07-16T07:09:19.000Z</published>
    <updated>2018-10-18T14:16:05.000Z</updated>
    
    <content type="html"><![CDATA[<p>7月4日，7月15日。<br>应该是持续时间最长的考试周吧。3门数学，3门CS相关，以及GRE。以学期均绩4.41，GRE332（152+170）的成绩，算是给大三画上了一个圆满的句号吧。</p><p>杭州很大，杭州也挺小。<br>紫金港，玉泉，杭州大厦。不同于前几个学期的囿于某一校区内，这学期的课程不仅在周一至周五要往返玉泉紫金港，还有周日在杭州大厦的新东方GRE课程。曾经觉得偌大无比的杭州城，终于渐渐熟悉了其中的一角。公交卡、校车票、滴滴出行的轮番交替，勾勒出紫金港玉泉之间的交错纵横。<br>原来，有三个属于玉泉的公交站：浙大玉泉校区，玉古路求是路口，古荡；以及好巧，有三个属于紫金港的公交站：紫荆花路余杭塘路口，浙大紫金港校区，章桥头。<br>原来，不同的出行方式也有不同的风格：“己”字形的89路公交路线，“卅”字形的校车路线，以及“乛”字形的出租车路线。或急或缓，或简或繁。</p><p>一学期很长，一学期也转瞬即逝。<br>虽然这学期的课表已尽量安排地比较空，不过周一至周五的校内课程、周日的GRE课程，以及课外杂七杂八的一些知识内容，依然将每一周、每一天填补地满满当当。在玉泉紫金港往返着，在清明节劳动节端午节瞎忙着，在考试周连续熬夜着，至关重要的大三下也过去了。<br>（嗯……就是想感慨下时间过得很快。）</p><p>我不够优秀，我值得更好。<br>在三四月大厂春招时，我也抱着尝试的心态投了阿里网易等机器学习相关的实习岗，凭借着浙大的光环有幸得到笔试/面试机会，然而终因实力不济在首轮面试便纷纷折戟。毕竟自己因学业原因没有为面试准备，不过潜意识里也还是害怕着吧，害怕精心准备后依然迎来的是失败的结局。<br>为了丰富自己的科研经历，亦为了能翘掉无聊的小学期，略读了CMU的几位LTI教授的论文后便试着发了邮件申请暑期科研，但是，一位教授暑假旅游，一位教授不收暑研，剩下的均石沉大海杳无音信。不过至少，暑假我可以待在丁老师的实验室安心学习钻研呐。<br>TOEFL 96，GRE 332。都是接近于裸考。虽然分数勉强看得过去，不过还是希望能再努力加把劲，以取得更理想的成绩吧。</p><p><em>I don’t need love. I wanna find love.</em><br><em>Finished? To be continued.</em></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;7月4日，7月15日。&lt;br&gt;应该是持续时间最长的考试周吧。3门数学，3门CS相关，以及GRE。以学期均绩4.41，GRE332（152+170）的成绩，算是给大三画上了一个圆满的句号吧。&lt;/p&gt;
&lt;p&gt;杭州很大，杭州也挺小。&lt;br&gt;紫金港，玉泉，杭州大厦。不同于前几个学期
      
    
    </summary>
    
      <category term="Essay" scheme="http://haelchan.me/categories/Essay/"/>
    
    
  </entry>
  
  <entry>
    <title>Regular Expression</title>
    <link href="http://haelchan.me/2018/05/22/regular-expression/"/>
    <id>http://haelchan.me/2018/05/22/regular-expression/</id>
    <published>2018-05-23T05:31:18.000Z</published>
    <updated>2019-02-09T15:37:27.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction-to-Regular-Expression"><a href="#Introduction-to-Regular-Expression" class="headerlink" title="Introduction to Regular Expression"></a>Introduction to Regular Expression</h2><p>A regular expression(RegEx) is an algebraic notation for characterizing a set of strings. They are particularly useful for searching in texts, when we have a pattern to search for and a corpus of texts to search through. A regular expression search function will search through the corpus, returning all texts that match the pattern. The corpus can be a single document or a collection.<br>(In some case, regular expressions are shown by delimiting by slashes <code>/</code>. But slashes are not part of the regular expression.)<br>In this post, I’d just use <code>expression</code> (without quotes) to denote regular expressions, and <code>&#39;expression&#39;</code> to denote the patterns matched.</p><p>Regular expressions can contain both special and ordinary characters. Most ordinary characters, like <code>A</code>, <code>a</code>, or <code>0</code>, are the simplest regular expressions; they simply match themselves. E.g., <code>hello</code> matches <code>&#39;hello&#39;, world</code>.<br>Regular expressions are <strong>case sensitive</strong>. Therefore, the lower case <code>h</code> is distinct from upper case <code>H</code>.</p><h3 id="Character-set"><a href="#Character-set" class="headerlink" title="Character set"></a>Character set</h3><p><code>[]</code><br>Used to indicate a set of characters. </p><ul><li>Characters can be listed individually, e.g., <code>[amk]</code> will match <code>&#39;a&#39;</code>,<code>&#39;m&#39;</code>, or <code>&#39;k&#39;</code>.</li><li>Ranges of characters can be indicated by giving two characters and separating them by a <code>-</code>. E.g., <code>[A-Z]</code> matches an upper case letter, <code>[0-9]</code> matches a single digit. If <code>-</code> is escaped(<code>\-</code>) or if it’s placed as the first or last character(e.g., <code>[a-]</code> or <code>[-a]</code>), it will match a literal <code>&#39;-&#39;</code>.</li><li>Special characters lose their special meaning inside sets. For example, <code>[(+*)]</code> will match any of the literal characters <code>&#39;(&#39;</code>, <code>&#39;+&#39;</code>, <code>&#39;*&#39;</code>, or <code>&#39;)&#39;</code>.</li><li>Character classes such as <code>\w</code> or <code>\S</code> are also accepted inside a set.</li><li>Characters that are not within a range can be matched by <em>complementing</em> the set. If the first character of the set is <code>^</code>, all characters that are <em>not</em> in the set will be matched. This is only true when the caret is the first symbol after the open square brace. If it occurs anywhere else, it usually stands for a caret and has no special meaning.</li><li>To match a literal <code>&#39;]&#39;</code> inside a set, precede it with a backslash, or place it at the beginning of the set.</li></ul><h3 id="Special-sequences"><a href="#Special-sequences" class="headerlink" title="Special sequences"></a>Special sequences</h3><p>Some of the special sequences beginning with <code>\</code> represent predefined sets of characters that are often useful, such as the set of digits, the set of letters, or the set of anything that isn’t whitespace.</p><p><code>\d</code><br>Matches any decimal digit; this is equivalent to <code>[0-9]</code>.</p><p><code>\D</code><br>Matches any non-digit character; this is equivalent to <code>[^0-9]</code>.</p><p><code>\s</code><br>Matches any whitespace character; this is equivalent to the class <code>[ \t\n\r\f\v]</code>.</p><p><code>\S</code><br>Matches any non-whitespace character; this is equivalent to the class <code>[^ \t\n\r\f\v]</code>.</p><p><code>\w</code><br>Matches any alphanumeric character and <strong>underscore</strong>(easily overlooked); this is equivalent to the class [a-zA-Z0-9_].</p><p><code>\W</code><br>Matches any non-alphanumeric character; this is equivalent to the class <code>[^a-zA-Z0-9_]</code>.</p><h3 id="Wildcard-expression"><a href="#Wildcard-expression" class="headerlink" title="Wildcard expression"></a>Wildcard expression</h3><p>The period/dot <code>.</code> is a special character that matches any single character(except a newline). <code>.</code> is often used where “any character” is to be matched.</p><h3 id="Anchors"><a href="#Anchors" class="headerlink" title="Anchors"></a>Anchors</h3><p><strong>Anchors</strong> are special characters that anchor regular expressions to particular places in a string. </p><p><code>^</code><br>The caret <code>^</code> matches the start of a line.<br>Thus, the caret <code>^</code> has three uses: to match the start of a line(<code>^</code>), to indicate a negation inside of square brackets(<code>[^]</code>), and just to mean a caret(<code>\^</code> or <code>[.^]</code>).</p><p><code>$</code><br>The dollar sign <code>$</code> matches the end of a line. </p><p>There are also two other anchors: <code>\b</code> matches a word boundary, and <code>\B</code> matches a non-boundary. More technically, a “word” for the purposes of a regular expression is defined as any sequence of digits, underscores, or letters.</p><h3 id="Quantifiers"><a href="#Quantifiers" class="headerlink" title="Quantifiers"></a>Quantifiers</h3><p><code>?</code><br>The question mark <code>?</code> means “zero or one instances of the previous character”.<br><code>ab?</code> will match either <code>&#39;a&#39;</code> or <code>&#39;ab&#39;</code>.</p><p><code>(?i)</code> makes regular expression case insensitive.</p><p><code>*</code><br>Commonly called the <strong>Kleene *</strong>. The Kleene star means “zero or more occurrences of the immediately previous character or regular expression”.</p><p><code>+</code><br>The <strong>Kleene +</strong> means “one or more of the previous character”.</p><p>The <code>?</code>, <code>*</code> and <code>+</code> qualifiers are all <em>greedy</em>, they match as much text as possible. There are also ways to enforce <em>non-greedy</em> matching, using another meaning of the <code>?</code> qualifier(here <code>?</code> means <em>lazy</em>: cause it to match as few characters as possible): <code>*?</code>, <code>+?</code>, <code>??</code>.</p><p><code>{}</code><br><code>{n}</code>: Exactly n repeats where n is a non-negative integer<br><code>{n,}</code>: At least n repeats<br><code>{,n}</code>: No more than n repeats<br><code>{m,n}</code>: At least m and no more than n repeats</p><h3 id="Alternation"><a href="#Alternation" class="headerlink" title="Alternation"></a>Alternation</h3><p>The <strong>disjunction</strong> operator, also called the <strong>pipe</strong> symbol <code>|</code> acts like a boolean OR. It matches the expression before or after the <code>|</code>.<br>In some sense, <code>|</code> is never greedy. As the target string is scanned, REs separated by <code>|</code> are tried from left to right. When one pattern completely matches, that branch is accepted. This means that once A matches, B will not be tested further, even if it would produce a longer overall match.</p><h3 id="Groups"><a href="#Groups" class="headerlink" title="Groups"></a>Groups</h3><p><code>()</code><br>Enclosing a pattern in parentheses makes it act like a single character for the purposes of neighboring operators like the pipe <code>|</code> and the Kleene <code>*</code>.  </p><h3 id="Capture-group"><a href="#Capture-group" class="headerlink" title="Capture group"></a>Capture group</h3><p>The use of parentheses to store a pattern in memory is called a <strong>capture group</strong>. Every time a capture group is used (i.e., parentheses surround a pattern), the resulting match is stored in a numbered <strong>register</strong>. Use backslash with number like <code>\1</code> to refer to those registers. Here the <code>\1</code> will be replaced by whatever string matched the first item in parentheses.<br>Parentheses thus have a double function in regular expressions; they are used to group terms for specifying the order in which operators should apply, and they are used to capture something in a register. Occasionally we might want to use parentheses for grouping, but don’t want to capture the resulting pattern in a register. In that case we use a non-capturing group, which is specified by putting the commands<br><code>?:</code> after the open paren, in the form <code>(?: pattern )</code>.</p><h3 id="Precedence"><a href="#Precedence" class="headerlink" title="Precedence"></a>Precedence</h3><p>This idea that one operator may take precedence over another, requiring us to sometimes use parentheses to specify what we mean, is formalized by the operator precedence hierarchy for regular expressions.<br>| Kind | Operators |<br>| —- | —- |<br>| Parenthesis | () |<br>| Counters | * + ? {} |<br>| Sequences and anchors | the ^my end$ |<br>| Disjunction | | |</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">re.findall(<span class="string">r'^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$'</span>, <span class="string">'processing'</span>)     <span class="comment"># ['ing']</span></span><br><span class="line">re.findall(<span class="string">r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$'</span>, <span class="string">'processing'</span>)   <span class="comment"># ['processing']</span></span><br></pre></td></tr></table></figure><h3 id="Lookahead-assertions"><a href="#Lookahead-assertions" class="headerlink" title="Lookahead assertions"></a>Lookahead assertions</h3><p>There will be time when we need to predict the future: look ahead in the text to see if some pattern matches, but not advance the match cursor, so that we can deal with the pattern if it oocurs.<br><strong>Positive lookahead</strong>: <code>(?= pattern)</code><br><strong>Negative lookahead</strong>: <code>(?! pattern)</code></p><h2 id="Regular-Expression-in-Python"><a href="#Regular-Expression-in-Python" class="headerlink" title="Regular Expression in Python"></a>Regular Expression in Python</h2><h3 id="Raw-string"><a href="#Raw-string" class="headerlink" title="Raw string"></a>Raw string</h3><p>To the Python interpreter, a regular expression is just like any other string. If the string contains a backslash followed by particular characters, it will interpret these specially. For example <code>\b</code> would be interpreted as the backspace character. In general, when using regular expressions containing backslash, we should instruct the interpreter not to look inside the string at all, but simply to pass it directly to the <code>re</code> library for processing. We do this by prefixing the string with the letter <code>r</code>, to idicate that it is a <strong>raw string</strong>. </p><h3 id="Multiple-delimiters-using-regular-expression"><a href="#Multiple-delimiters-using-regular-expression" class="headerlink" title="Multiple delimiters using regular expression"></a>Multiple delimiters using regular expression</h3><p>It seems that the original Python’s <code>split()</code> function only support one separator. However, it would be convenient to use regular expression to support multiple separators.<br><code>re.split(r&#39;\W+&#39;, original_string)</code><br>It will return the list of separated strings. But it gives us empty strings at the start and the end.<br>We can use <code>re.findall(r&#39;\w+&#39;, original_string)</code> to get the same tokens, but without the empty strings.</p><p><code>re.findall(r&quot;\w+(?:[-&#39;]\w+)*|&#39;|[-.(]+|\S\w*&quot;, raw)</code><br>This way helps to deal with the words like <code>it&#39;s</code> and <code>ward-hearted</code>.</p><p><code>re.findall()</code> is also useful to extract information when dealing with tagged corpora, and currently I am doing a research with the help of it.</p><h3 id="Markdown-inline-LaTeX"><a href="#Markdown-inline-LaTeX" class="headerlink" title="Markdown inline LaTeX"></a>Markdown inline LaTeX</h3><p>The markdown editors supports inline <script type="math/tex">\LaTeX</script> well. However, to support LaTeX on Hexo, it’s necessary to  use mathjax and add inline code surrounding the dollor sign. Now it’s time to use regular expression to modify the format.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pattern = <span class="string">r' \$([\w\\=|&#123;&#125;()^\*-]+)\$'</span></span><br><span class="line">repl = <span class="string">r' `\$\1\$`'</span></span><br><span class="line">re.sub(pattern, repl, originText)</span><br></pre></td></tr></table></figure><p>(Well, I just use Sublime Text’s Replace function to make the modification.)</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p>Speech and Language Processing (3rd ed. draft), Dan Jurafsky and James H. Martin”:<br><a href="https://web.stanford.edu/~jurafsky/slp3/2.pdf" target="_blank" rel="noopener">Regular Expressions, Text Normalization, Edit Distance</a>  </p><p>Python Documentation:<br><a href="https://docs.python.org/3/library/re.html" target="_blank" rel="noopener">re - Regular expression operations</a><br><a href="https://docs.python.org/3/howto/regex.html#regex-howto" target="_blank" rel="noopener">Regular Expression HOWTO</a>  </p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Introduction-to-Regular-Expression&quot;&gt;&lt;a href=&quot;#Introduction-to-Regular-Expression&quot; class=&quot;headerlink&quot; title=&quot;Introduction to Regular 
      
    
    </summary>
    
      <category term="Natural Language Processing" scheme="http://haelchan.me/categories/Natural-Language-Processing/"/>
    
    
      <category term="Python" scheme="http://haelchan.me/tags/Python/"/>
    
      <category term="regular expression" scheme="http://haelchan.me/tags/regular-expression/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow Learning</title>
    <link href="http://haelchan.me/2018/04/27/tensorflow-learning/"/>
    <id>http://haelchan.me/2018/04/27/tensorflow-learning/</id>
    <published>2018-04-27T12:39:33.000Z</published>
    <updated>2018-10-18T15:05:35.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><h3 id="Install"><a href="#Install" class="headerlink" title="Install"></a>Install</h3><p><a href="https://www.tensorflow.org/install/" target="_blank" rel="noopener">Official Install Guidance</a></p><p>As for me, my environment is:</p><blockquote><p>macOS Sierra 10.12.6<br>Python 3.6.3<br>pip3</p></blockquote><p>The official command uses <code>pip install --upgrade virtualenv</code>. However, my MacBook Air doesn’t have <code>pip</code> and just can’t install <code>pip</code> using <code>sudo easy_install pip</code>. The error message is<br><code>Download error on https://pypi.python.org/simple/pip/: [SSL: TLSV1_ALERT_PROTOCOL_VERSION] tlsv1 alert protocol version (_ssl.c:590) -- Some packages may not be found!</code></p><p>There seems something wrong with <code>openssl</code>. But after updating <code>openssl</code> with <code>homebrew</code>, it still doesn’t work. Thankfully, I can install with <code>pip3</code> instead.</p><p>So the full commands are as follows:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">pip3 install --upgrade virtualenv</span><br><span class="line">virtualenv --system-site-packages -p python3</span><br><span class="line">cd tensorflow</span><br><span class="line">easy_install -U pip3</span><br><span class="line">source ./bin/activate</span><br><span class="line">pip3 install --upgrade tensorflow</span><br></pre></td></tr></table></figure><h4 id="CS20"><a href="#CS20" class="headerlink" title="CS20"></a>CS20</h4><p>To learn TensorFlow, I’m following Stanford’s course <a href="http://web.stanford.edu/class/cs20si/syllabus.html" target="_blank" rel="noopener">CS20: TensorFlow for Deep Learning Research</a>. So I’ve also installed TensorFlow 1.4.1 with the <a href="https://github.com/chiphuyen/stanford-tensorflow-tutorials/blob/master/setup/setup_instruction.md" target="_blank" rel="noopener">setup instruction</a>.<br>There seems something wrong when importing tensorflow. The error message:<br><code>/usr/local/Cellar/python/3.6.3/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module &#39;tensorflow.python.framework.fast_tensor_util&#39; does not match runtime version 3.6return f(*args, **kwds)</code></p><p>The solution is found <a href="https://github.com/lakshayg/tensorflow-build" target="_blank" rel="noopener">here</a>. Download the binary resource and use the command <code>pip install --ignore-installed --upgrade tensorflow-1.4.0-cp36-cp36m-macosx_10_12_x86_64.whl</code> (may be different on different on different machines).</p><h3 id="Activation-and-Deactivation"><a href="#Activation-and-Deactivation" class="headerlink" title="Activation and Deactivation"></a>Activation and Deactivation</h3><p>Activate the Virtualenv each time when using TensorFlow in a new shell.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd targetDirectory</span><br><span class="line">source ./bin/activate</span><br></pre></td></tr></table></figure><p>Change the path to Virtualenv environment, invoke the activation command and then the prompt will transform to the following to indicate that TensorFlow environment is active:<br>(<em>targetDirectory</em>)<code>$</code></p><p>When it is done, deactivate the environment by issuing the following command:<br>(<em>targetDirectory</em>)<code>$ deactivate</code></p><h3 id="Graphs-and-Sessions"><a href="#Graphs-and-Sessions" class="headerlink" title="Graphs and Sessions"></a>Graphs and Sessions</h3><h4 id="Graphs"><a href="#Graphs" class="headerlink" title="Graphs"></a>Graphs</h4><p>TensorFlow separates definition of computations from their execution.<br><strong>Phase</strong>:</p><ol><li>assemble a graph</li><li>use a session to execute operations in the graph</li></ol><p>(this might change in the future with eager mode)</p><p><strong>Tensor</strong><br>A tensor is an n-dimensional array.<br>0-d tensor: scalar (number)<br>1-d tensor: vector<br>2-d tensor: matrix<br>and so on…</p><p>Data Flow Graphs<br><img src="/images/tf/dataFlowGraph.jpg" alt><br>Nodes: operators, variables, and constants<br>Edges: tensors</p><p>Tensors are data.<br>TensorFlow = tensor + flow = data + flow</p><h4 id="Session"><a href="#Session" class="headerlink" title="Session"></a>Session</h4><p>Then how to get the value of a?<br>Create a session, assign it to variable sess so we can call it later.<br>Within the session, evaluate the graph to fetch the value of a.</p><p>Two ways:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># commands</span></span><br><span class="line">sess.close()</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># commands</span></span><br></pre></td></tr></table></figure><p>A session object encapsulates the environment in which Operation objects are executed, and Tensor objects are evaluated.<br>Session will also allocate memory to store the current values of variables.</p><p><strong>Why graphs</strong></p><ol><li>Save computation. Only run subgraphs that lead to the values you want to fetch</li><li>Break computation into small, differential pieces to facilitate auto-differentiation</li><li>Facilitate distributed computation, spread the work across multiple CPUs, GPUs, TPUs, or other devices</li><li>Many common machine learning models are taught and visualized as directed graphs</li></ol><h3 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h3><blockquote><p>The computations you’ll use TensorFlow for - like training a massive deep neural network - can be complex and confusing. To make it easier to understand, debug, and optimize TensorFlow programs, we’ve included a suite of visualization tools called TensorBoard.</p></blockquote><p>When a user perform certain operations in a TensorBoard-activated TensorFlow program, these operations are exported to an event log file. TensorBoard is able to convert these event files to visualizations that can give insight into a model’s graph and its runtime behavior. Learning to use TensorBoard early and often will make working with TensorFlow that much more enjoyable and productive.</p><p>To visualize the program with TensorBoard, we need to write log files of the program. To write event files, we first need to create a writer for those logs, using the code <code>writer = tf.summary. FileWriter ([logdir], [graph])</code><br>[graph] is the graph of the program that we’re working on. Either call it using <code>tf.get_default_graph()</code>, which returns the default graph of the program, or through <code>sess.graph</code>, which returns the graph the session is handling. The latter requires that a session is created.<br>[logdir] is the folder where we want to store those log files.</p><p>Note: if running the code several times, there will be multiple event files in [logdir]. TF will show only the latest graph and display the warning of multiple event files. To get rid of the warning, just delete the event files that is useless.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">a = tf.constant(<span class="number">2</span>)</span><br><span class="line">b = tf.constant(<span class="number">3</span>)</span><br><span class="line">x = tf.add(a, b)</span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">'./graphs'</span>, tf.get_default_graph())</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># writer = tf.summary.FileWriter('./graphs', sess.graph)</span></span><br><span class="line">    print(sess.run(x))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 programName.py</span><br><span class="line">tensorboard --logdir=&quot;./graphs&quot; --port 6006</span><br></pre></td></tr></table></figure><h2 id="Operations"><a href="#Operations" class="headerlink" title="Operations"></a>Operations</h2><h3 id="Constants"><a href="#Constants" class="headerlink" title="Constants"></a>Constants</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># constant of 1d tensor (vector)</span></span><br><span class="line">tf.constant(value, dtype=<span class="keyword">None</span>, shape=<span class="keyword">None</span>, name=<span class="string">'Const'</span>, verify_shape=<span class="keyword">False</span>)</span><br></pre></td></tr></table></figure><p><code>a = tf.constant([2, 2], name=&#39;a&#39;)</code></p><p><strong>Tensors filled with a specific value</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a tensor of shape and all elements are zeros</span></span><br><span class="line">tf.zeros(shape, dtype=tf.float32, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>Similar to numpy.zeros<br><code>tf.zeros([2, 3], tf.int32)</code> ==&gt; [[0, 0, 0], [0, 0, 0]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a tensor of shape and type (unless type is specified) as the input_tensor but all elements are zeros</span></span><br><span class="line">tf.zeros_like(input_tensor, dtype=<span class="keyword">None</span>, name=<span class="keyword">None</span>, optimize=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Similar to numpy.zeros_like<br><code>tf.zeros_like(input_tensor) # input_tensor = [[0, 1], [2, 3], [4, 5]]</code> ==&gt; [[0, 0], [0, 0], [0, 0]]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a tensor of shape and all elements are ones</span></span><br><span class="line">tf.ones(shape, dtype=tf.float32, name=<span class="keyword">None</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create a tensor of shape and type (unless type is specified) as the input_tensor but all elements are ones</span></span><br><span class="line">tf.ones_like(input_tensor, dtype=<span class="keyword">None</span>, name=<span class="keyword">None</span>, optimize=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure><p>Similar to numpy.ones, numpy.ones_like</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a tensor filled with a scalar value</span></span><br><span class="line">tf.fill(dims, value, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>Similar to numpy.full<br><code>tf.fill([2, 3], 8)</code> ==&gt; [[8, 8, 8], [8, 8, 8]]</p><p><strong>Constants as sequences</strong><br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a sequence of  num  evenly-spaced values are generated beginning at  start . If  num  &gt; 1, the values in the sequence increase by (stop - start) / (num - 1), so that the last one is exactly  stop .</span></span><br><span class="line"><span class="comment"># comparable to but slightly different from numpy.linspace</span></span><br><span class="line">tf.lin_space(start, stop, num, name=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure></p><p><code>tf.lin_space(10.0, 13.0, 4)</code> ==&gt; [10. 11. 12. 13.]</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create a sequence of numbers that begins at start and extends by increments of delta up to but not including limit</span></span><br><span class="line"><span class="comment"># slight different from range in Python</span></span><br><span class="line">tf.range(start, limit=<span class="keyword">None</span>, delta=<span class="number">1</span>, dtype=<span class="keyword">None</span>, name=<span class="string">'range'</span>)</span><br></pre></td></tr></table></figure><p><code>tf.range(3, 18, 3)</code> ==&gt; [3 6 9 12 15]<br><code>tf.range(5)</code> ==&gt; [0 1 2 3 4]</p><p><strong>Randomly Generated Constants</strong><br>tf.random_normal<br>tf.truncated_normal<br>tf.random_uniform<br>tf.random_shuffle<br>tf.random_crop<br>tf.multinomial<br>tf.random_gamma</p><p>tf.set_random_seed(seed)</p><h3 id="Basic-operations"><a href="#Basic-operations" class="headerlink" title="Basic operations"></a>Basic operations</h3><p><strong>Element-wise mathematical operations</strong><br>Add, Sub, Mul, Div, Exp, Log, Greater, Less, Equal, …<br>Well, there’re 7 different div operations in TensorFlow, all doing more or less the same thing: <code>tf.div(), tf. divide(), tf.truediv(), tf.floordiv(), tf.realdiv(), tf.truncatediv(), tf.floor_div()</code></p><p><strong>Array operations</strong><br>Concat, Slice, Split, Constant, Rank, Shape, Shuffle, …</p><p><strong>Matrix operations</strong><br>MatMul, MatrixInverse, MatrixDeterminant, …</p><p><strong>Stateful operations</strong><br>Variable, Assign, AssignAdd, …</p><p><strong>Neural network building blocks</strong><br>SoftMax, Sigmoid, ReLU, Convolution2D, MaxPool, …</p><p><strong>Checkpointing operations</strong><br>Save, Restore</p><p><strong>Queue and synchronization operations</strong><br>Enqueue, Dequeue, MutexAcquire, MutexRelease, …</p><p><strong>Control flow operations</strong><br>Merge, Switch, Enter, Leave, NextIteration</p><h3 id="Data-types"><a href="#Data-types" class="headerlink" title="Data types"></a>Data types</h3><p>TensorFlow takes Python natives types: boolean, numeric(int, float), strings</p><p>scalars are treated like 0-d tensors<br>1-d arrays are treated like 1-d tensors<br>2-d arrays are treated like 2-d tensors</p><p><img src="/images/tf/dataType.jpg" alt></p><p>TensorFlow integrates seamlessly with NumPy<br>Can pass numpy types to TensorFlow ops</p><p>Use TF DType when possible:<br>Python native types: TensorFlow has to infer Python type<br>NumPy arrays: NumPy is not GPU compatible</p><h3 id="Variable"><a href="#Variable" class="headerlink" title="Variable"></a>Variable</h3><p>Constants are stored in the graph definition. This makes loading graphs expensive when constants are big.<br>Therefore, only use constants for primitive types. Use variables or readers for more data that requires more memory.</p><h4 id="Creating-variables"><a href="#Creating-variables" class="headerlink" title="Creating variables"></a>Creating variables</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">tf.get_variable(</span><br><span class="line">    name,</span><br><span class="line">    shape = <span class="keyword">None</span>, </span><br><span class="line">    dtype = <span class="keyword">None</span>, </span><br><span class="line">    initializer = <span class="keyword">None</span>, </span><br><span class="line">    regularizer = <span class="keyword">None</span>, </span><br><span class="line">    trainable = <span class="keyword">True</span>, </span><br><span class="line">    collections = <span class="keyword">None</span>, </span><br><span class="line">    caching_device = <span class="keyword">None</span>, </span><br><span class="line">    partitioner = <span class="keyword">None</span>, </span><br><span class="line">    validate_shape = <span class="keyword">True</span>, </span><br><span class="line">    use_resource = <span class="keyword">None</span>, </span><br><span class="line">    custom_getter = <span class="keyword">None</span>, </span><br><span class="line">    constraint = <span class="keyword">None</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure><p>With <code>tf.get_variable</code>, we can provide variable’s internal name, shape, type, and initializer to give the variable its initial value.</p><p>The old way to create a variable is simply call <code>tf.Variable(&lt;initial-value&gt;, name=&lt;optional-name&gt;)</code>.(Note that it’s written <code>tf.constant</code> with lowercase ‘c’ but <code>tf.Variable</code> with uppercase ‘V’. It’s because <code>tf.constant</code> is an op, while <code>tf.Variable</code> is a class with multiple ops.) However, this old way is discouraged and TensorFlow recommends that we use the wrapper <code>tf.get_variable</code>, which allows for easy variable sharing. </p><p><strong>Some initializer</strong><br><code>tf.zeros_initializer()</code><br><code>tf.ones_initializer()</code><br><code>tf.random_normal_initializer()</code><br><code>tf.random_uniform_initializer()</code></p><h4 id="Initialization"><a href="#Initialization" class="headerlink" title="Initialization"></a>Initialization</h4><p>We have to initialize a variable before using it. (If you try to evaluate the variables before initializing them you’ll run into FailedPreconditionError: Attempting to use uninitialized value.)</p><p>The easiest way is initializing all variables at once:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess: </span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br></pre></td></tr></table></figure><p>Initialize only a subset of variables:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(tf.variables_initializer([a, b]))</span><br></pre></td></tr></table></figure><p>Initialize a single variable:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(W.initializer)</span><br></pre></td></tr></table></figure><h4 id="Assignment"><a href="#Assignment" class="headerlink" title="Assignment"></a>Assignment</h4><p>Eval: get a variable’s value.<br><code>print(W.eval())  # Similar to print(sess.run(W))</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(<span class="number">10</span>) </span><br><span class="line">W.assign(<span class="number">100</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(W.initializer) </span><br><span class="line">    print(W.eval())   <span class="comment"># &gt;&gt; 10</span></span><br></pre></td></tr></table></figure><p>Why W is 10 but not 100? In fact, <code>W.assign(100)</code> creates an assign op. That op needs to be executed in a session to take effect.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">W = tf.Variable(<span class="number">10</span>)</span><br><span class="line">assign_op = W.assign(<span class="number">100</span>)</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(assign_op)</span><br><span class="line">    print(W.eval()) <span class="comment"># &gt;&gt; 100</span></span><br></pre></td></tr></table></figure><p>Note that we don’t have to initialize W in this case, because assign() does it for us. In fact, the initializer op is an assign op that assigns the variable’s initial value to the variable itself.</p><p>For simple incrementing and decrementing of variables, TensorFlow includes the <code>tf.Variable.assign_add()</code> and <code>tf.Variable.assign_sub()</code> methods. Unlike <code>tf.Variable.assign()</code>, <code>tf.Variable.assign_add()</code> and <code>tf.Variable.assign_sub()</code> don’t initialize your variables for you because these ops depend on the initial values of the variable.</p><p>Each session maintains its own copy of variables.</p><h4 id="Control-Dependencies"><a href="#Control-Dependencies" class="headerlink" title="Control Dependencies"></a>Control Dependencies</h4><p>Sometimes, we have two or more independent ops and we’d like to specify which ops should be run first. In this case, we use <code>tf.Graph.control_dependencies([control_inputs])</code>.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># your graph g have 5 ops: a, b, c, d, e</span></span><br><span class="line">g = tf.get_default_graph()</span><br><span class="line"><span class="keyword">with</span> g.control_dependencies([a, b, c]):</span><br><span class="line">    <span class="comment"># 'd' and 'e' will only run after 'a', 'b', and 'c' have executed.</span></span><br><span class="line">    d = ...</span><br><span class="line">    e = ...</span><br></pre></td></tr></table></figure><h3 id="Placeholders"><a href="#Placeholders" class="headerlink" title="Placeholders"></a>Placeholders</h3><p>We can assemble the graphs first without knowing the values needed for computation.<br>(Just think about defining the function of x,y without knowing the values of x,y. E.g., <script type="math/tex">f(x,y)=2x+y</script><br>With the graph assembled, we, or our clients, can later supply their own data when they need to execute the computation.<br>To define a place holder:<br><code>tf.placeholder(dtype, shape=None, name=None)</code></p><p>We can feed as many data points to the placeholder as we want by iterating through the data set and feed in the value one at a time.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="keyword">for</span> a_value <span class="keyword">in</span> list_of_a_values:</span><br><span class="line">        print(sess.run(c, &#123;a: a_value&#125;))</span><br></pre></td></tr></table></figure><p>We can feed_dict any feedable tensor. Placeholder is just a way to indicate that something must be fed. Use <code>tf.Graph.is_feedable(tensor)</code> to check if a tensor is feedable or not.<br>feed_dict can be extremely useful to test models. When you have a large graph and just want to test out certain parts, you can provide dummy values so TensorFlow won’t waste time doing unnecessary computations.</p><h4 id="Placeholder-and-tf-data"><a href="#Placeholder-and-tf-data" class="headerlink" title="Placeholder and tf.data"></a>Placeholder and tf.data</h4><p>Pros and Cons of placeholder:<br>Pro: put the data processing outside TensorFlow, making it easy to do in Python<br>Cons: users often end up processing their data in a single thread and creating data bottleneck that slows execution down</p><p><strong>tf.data</strong></p><p><code>tf.data.Dataset.from_tensor_slices((features, labels))</code><br><code>tf.data.Dataset.from_generator(gen, output_types, output_shapes</code></p><p>For prototyping, feed dict can be faster and easier to write(pythonic)<br>tf.data is tricky to use when you have complicated preprocessing or multiple data sources<br>NLP data is normally just a sequence of integers. In this case, transferring the data over to GPU is pretty quick, so the speedup of tf.data isn’t that large</p><h3 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h3><p>How does TensorFlow know what variables to update?</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=<span class="number">0.01</span>).minimizer(loss)</span><br><span class="line">_, l = sess.run([optimizer, loss], feed_dict=&#123;X:x, Y:y&#125;)</span><br></pre></td></tr></table></figure><p>By default, the optimizer trains all the trainable variables its objective function depends on. If there are variables that you do not want to train, you can set the keyword <code>trainable=False</code> when declaring a variable.</p><hr><p><strong>Solution for LAZY LOADING</strong></p><ol><li>Separate definition of ops from computing/running ops</li><li>Use Python property to ensure function is also loaded once the first time it is called</li></ol><h2 id="Linear-and-Logistic-Regression"><a href="#Linear-and-Logistic-Regression" class="headerlink" title="Linear and Logistic Regression"></a>Linear and Logistic Regression</h2><h3 id="Linear-Regression"><a href="#Linear-Regression" class="headerlink" title="Linear Regression"></a>Linear Regression</h3><p>Given World Development Indicators dataset, X is birth rate, Y is life expectancy. Find a linear relationship between X and Y to predict Y from X.</p><p>Phase 1: Assemble the graph</p><ol><li>Read in data</li><li>Create placeholders for inputs and labels</li><li>Create weight and bias</li><li>Inference <code>Y_predicted = w * X + b</code></li><li>Specify loss function</li><li>Create optimizer</li></ol><p>Phase 2: Train the model</p><ol><li>Initialize variables</li><li>Run optimizer</li></ol><p>Write log files using a FileWriter</p><p>See it on TensorBoard</p><hr><p><strong>Huber loss</strong><br>One way to deal with outliers is to use Huber loss.<br>If the difference between the predicted value and the real value is small, square it<br>If it’s large, take its absolute value</p><script type="math/tex; mode=display">L_\delta(y,f(x))=\begin{cases}\frac{1}{2}(y-f(x))^2\ \ \ \text{for }|y-f(x)|\le\delta\\\delta|y-f(x)|-\frac{1}{2}\delta^2\ \ \text{ otherwise}\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">huber_loss</span><span class="params">(labels, predictions, delta=<span class="number">14.0</span>)</span>:</span></span><br><span class="line">    residual = tf.abs(labels - predictions)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f1</span><span class="params">()</span>:</span> <span class="keyword">return</span> <span class="number">0.5</span> * tf.square(residual)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">f2</span><span class="params">()</span>:</span> <span class="keyword">return</span> delta * residual - <span class="number">0.5</span> * tf.square(delta)</span><br><span class="line">    <span class="keyword">return</span> tf.cond(residual &lt; delta, f1, f2)</span><br></pre></td></tr></table></figure><h3 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h3><p>X: image of a handwritten digit<br>Y: the digit value<br>Recognize the digit in the image</p><p>Phase 1: Assemble the graph</p><ol><li>Read in data</li><li>Create datasets and iterator</li><li>Create weights and biases</li><li>Build model to predict Y</li><li>Specify loss function</li><li>Create optimizer</li></ol><p>Phase 2: Train the model</p><ol><li>Initialize variables</li><li>Run optimizer op</li></ol><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">""" Starter code for simple logistic regression model for MNIST</span></span><br><span class="line"><span class="string">with tf.data module</span></span><br><span class="line"><span class="string">MNIST dataset: yann.lecun.com/exdb/mnist/</span></span><br><span class="line"><span class="string">Created by Chip Huyen (chiphuyen@cs.stanford.edu)</span></span><br><span class="line"><span class="string">CS20: "TensorFlow for Deep Learning Research"</span></span><br><span class="line"><span class="string">cs20.stanford.edu</span></span><br><span class="line"><span class="string">Lecture 03</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">'TF_CPP_MIN_LOG_LEVEL'</span>]=<span class="string">'2'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> utils</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define paramaters for the model</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line">n_epochs = <span class="number">30</span></span><br><span class="line">n_train = <span class="number">60000</span></span><br><span class="line">n_test = <span class="number">10000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: Read in data</span></span><br><span class="line">mnist_folder = <span class="string">'data/mnist'</span></span><br><span class="line">utils.download_mnist(mnist_folder)</span><br><span class="line">train, val, test = utils.read_mnist(mnist_folder, flatten=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: Create datasets and iterator</span></span><br><span class="line"><span class="comment"># create training Dataset and batch it</span></span><br><span class="line">train_data = tf.data.Dataset.from_tensor_slices(train)</span><br><span class="line">train_data = train_data.shuffle(<span class="number">10000</span>) <span class="comment"># if you want to shuffle your data</span></span><br><span class="line">train_data = train_data.batch(batch_size)</span><br><span class="line"></span><br><span class="line"><span class="comment"># create testing Dataset and batch it</span></span><br><span class="line">test_data = tf.data.Dataset.from_tensor_slices(test)</span><br><span class="line">test_data = test_data.batch(batch_size)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># create one iterator and initialize it with different datasets</span></span><br><span class="line">iterator = tf.data.Iterator.from_structure(train_data.output_types, </span><br><span class="line">                                           train_data.output_shapes)</span><br><span class="line">img, label = iterator.get_next()</span><br><span class="line"></span><br><span class="line">train_init = iterator.make_initializer(train_data)<span class="comment"># initializer for train_data</span></span><br><span class="line">test_init = iterator.make_initializer(test_data)<span class="comment"># initializer for train_data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: create weights and bias</span></span><br><span class="line"><span class="comment"># w is initialized to random variables with mean of 0, stddev of 0.01</span></span><br><span class="line"><span class="comment"># b is initialized to 0</span></span><br><span class="line"><span class="comment"># shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)</span></span><br><span class="line"><span class="comment"># shape of b depends on Y</span></span><br><span class="line"></span><br><span class="line">w = tf.get_variable(<span class="string">'weight'</span>, shape=(img.shape[<span class="number">1</span>], label.shape[<span class="number">1</span>]), initializer = tf.random_normal_initializer(mean=<span class="number">0.0</span>, stddev=<span class="number">0.01</span>))</span><br><span class="line">b = tf.get_variable(<span class="string">'bias'</span>, shape=(<span class="number">1</span>, label.shape[<span class="number">1</span>]), initializer = tf.zeros_initializer())</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 4: build model</span></span><br><span class="line"><span class="comment"># the model that returns the logits.</span></span><br><span class="line"><span class="comment"># this logits will be later passed through softmax layer</span></span><br><span class="line">logits = tf.matmul(img, w) + b</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 5: define loss function</span></span><br><span class="line"><span class="comment"># use cross entropy of softmax of logits as the loss function</span></span><br><span class="line">entropy = tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits)</span><br><span class="line">loss = tf.reduce_mean(entropy)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 6: define optimizer</span></span><br><span class="line"><span class="comment"># using Adamn Optimizer with pre-defined learning rate to minimize loss</span></span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 7: calculate accuracy with test set</span></span><br><span class="line">preds = tf.nn.softmax(logits)</span><br><span class="line">correct_preds = tf.equal(tf.argmax(preds, <span class="number">1</span>), tf.argmax(label, <span class="number">1</span>))</span><br><span class="line">accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))</span><br><span class="line"></span><br><span class="line">writer = tf.summary.FileWriter(<span class="string">'./graphs/logreg'</span>, tf.get_default_graph())</span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">   </span><br><span class="line">    start_time = time.time()</span><br><span class="line">    sess.run(tf.global_variables_initializer())</span><br><span class="line"></span><br><span class="line">    <span class="comment"># train the model n_epochs times</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n_epochs): </span><br><span class="line">        sess.run(train_init)<span class="comment"># drawing samples from train_data</span></span><br><span class="line">        total_loss = <span class="number">0</span></span><br><span class="line">        n_batches = <span class="number">0</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">                _, l = sess.run([optimizer, loss])</span><br><span class="line">                total_loss += l</span><br><span class="line">                n_batches += <span class="number">1</span></span><br><span class="line">        <span class="keyword">except</span> tf.errors.OutOfRangeError:</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        print(<span class="string">'Average loss epoch &#123;0&#125;: &#123;1&#125;'</span>.format(i, total_loss/n_batches))</span><br><span class="line">    print(<span class="string">'Total time: &#123;0&#125; seconds'</span>.format(time.time() - start_time))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># test the model</span></span><br><span class="line">    sess.run(test_init)<span class="comment"># drawing samples from test_data</span></span><br><span class="line">    total_correct_preds = <span class="number">0</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">            accuracy_batch = sess.run(accuracy)</span><br><span class="line">            total_correct_preds += accuracy_batch</span><br><span class="line">    <span class="keyword">except</span> tf.errors.OutOfRangeError:</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    print(<span class="string">'Accuracy &#123;0&#125;'</span>.format(total_correct_preds/n_test))</span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><h2 id="Eager-execution"><a href="#Eager-execution" class="headerlink" title="Eager execution"></a>Eager execution</h2><p><strong>Pros and Cons of Graph:</strong></p><p>PRO:</p><p><em>Optimizable</em><br>· automatic buffer reuse<br>· constant folding<br>· inter-op parallelism<br>· automatic trade-off between compute and memory</p><p><em>Deployable</em><br>· the Graph is an intermediate representation for models</p><p><em>Rewritable</em><br>· experiment with automatic device placement or quantization</p><p>CON:</p><p><em>Difficult to debug</em><br>· errors are reported long after graph construction<br>· execution cannot by debugged with <code>pdb</code> or print statements</p><p><em>Un-Pythonic</em><br>· writing a TensorFlow program is an exercise in metaprogramming<br>· control flow(e.g., <code>tf.while_loop</code>) differs from Python<br>· can’t easily mix graph construction with custom data structures</p><hr><blockquote><p>A NumPy-like library for numerical computation with support for GPU acceleration and automatic differentiation, and a flexible platform for machine learning research and experimentation.</p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> tensorflow.contrib.eager <span class="keyword">as</span> tfe</span><br><span class="line">tfe.enable_eager_execution()</span><br></pre></td></tr></table></figure><p>Key advantages of eager execution</p><ul><li>Compatible with Python debugging tools<ul><li><code>pdb.set_trace()</code> to heart content</li></ul></li><li>Provides immediate error reporting</li><li>Permits use of Python data structures<ul><li>e.g., for structured input</li></ul></li><li>Enables easy, Pythonic control flow<ul><li><code>if</code> statements, <code>for</code> loops, recursion</li></ul></li></ul><p>Since TensorFlow 2.0 is coming (<a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/bgug1G6a89A" target="_blank" rel="noopener">a preview version of TensorFlow 2.0 later this year</a>) and <a href="https://www.tensorflow.org/community/roadmap" target="_blank" rel="noopener">eager execution is a central feature of 2.0</a>. I’ll update more after the release of TensorFlow 2.0. Looking forward to it.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;h3 id=&quot;Install&quot;&gt;&lt;a href=&quot;#Install&quot; class=&quot;header
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://haelchan.me/categories/Deep-Learning/"/>
    
    
      <category term="learning note" scheme="http://haelchan.me/tags/learning-note/"/>
    
      <category term="TensorFlow" scheme="http://haelchan.me/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>Learning note of two papers about composition model</title>
    <link href="http://haelchan.me/2018/04/08/composition-model/"/>
    <id>http://haelchan.me/2018/04/08/composition-model/</id>
    <published>2018-04-08T15:46:59.000Z</published>
    <updated>2018-10-18T15:07:43.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Composition-in-Distributional-Models-of-Semantics"><a href="#Composition-in-Distributional-Models-of-Semantics" class="headerlink" title="Composition in Distributional Models of Semantics"></a>Composition in Distributional Models of Semantics</h1><h2 id="Semantic-representation"><a href="#Semantic-representation" class="headerlink" title="Semantic representation"></a>Semantic representation</h2><h3 id="Semantic-Networks"><a href="#Semantic-Networks" class="headerlink" title="Semantic Networks"></a>Semantic Networks</h3><p>Semantic networks represent concepts as nodes in a graph. Edges in the graph denote semantic relationships between concepts(e.g., <em>DOG</em> IS-A <em>MAMMAL</em>, <em>DOG</em> HAS <em>TAIL</em>) and word meaning is expressed by the number of type of connections to other words. In this framework, word similarity is a function of path length-semantically related words are expected to have shorter paths between them. Semantic networks constitute a somewhat idealized representation that abstracts away from real-word usage-they are traditionally hand coded by modelers who a priori decide which relationships are most relevant in representing meaning.<br>More recent work creates a semantic network from word association norms (Nelson, McEvoy, &amp; Schreiber, 1999); however, these can only represent a small fraction of the vocabulary of an adult speaker.</p><h3 id="Feature-based-Models"><a href="#Feature-based-Models" class="headerlink" title="Feature-based Models"></a>Feature-based Models</h3><p>Feature-based model has the idea that word meaning can be described in terms of feature lists. Theories tend to differ with respect to their definition of features. In many cases, the features are obtained by asking native speakers to generate attributes they consider important in describing the meaning of a word. This allows the representation of each word by a distribution of numerical values over the feature set.<br>Admittedly, norming studies have the potential of revealing which dimensions of meaning are psychologically salient. However, a number of difficulties arise when working with such data. For example, the number and types of attributes generated can vary substantially as a function of the amount of time devoted to each word. There are many degrees of freedom in the way that responses are coded and analyzed. And multiple subjects are required to create a representation for each word, which in practice limits elicitation studies to a small-size lexicon.</p><h3 id="Semantic-Spaces"><a href="#Semantic-Spaces" class="headerlink" title="Semantic Spaces"></a>Semantic Spaces</h3><p>It has been driven by the assumption that word meaning can be learned from the linguistic environment. Words that are similar in meaning tend to occur in contexts of similar words. Semantic space models capture meaning <em>quantitatively</em> in terms of simple co-occurrence statistics. Words are represented as vectors in a high-dimensional space, where each component corresponds to some co-occurring contextual element. The latter can be words themselves, larger linguistic units such as paragraphs or documents, or even more complex linguistic representations such as n-grams and the argument slots of predicates.<br>The advantage of taking such a geometric approach is that the similarity of word meanings can be easily quantifies by measuring their distance in the vector space, or the cosine of the angle between them.</p><h4 id="Some-semantic-space-models"><a href="#Some-semantic-space-models" class="headerlink" title="Some semantic space models"></a>Some semantic space models</h4><p><strong>Hyperspace Analog to Language model(HAL)</strong><br>Represents each word by a vector where each element of the vector corresponds to a weighted co-occurrence value of that word with some other word.</p><p><strong>Latent Semantic Analysis(LSA)</strong><br>Derives a high-dimensional semantic space for words while using co-occurrence information between words and the passages they occur in. LSA constructs a word-document co-occurrence matrix from a large document collection.</p><p><strong>Probabilistic topic models</strong><br>Offers an alternative to semantic spaces based on the assumption that words observed in a corpus manifest some latent structure linked to topics. Words are represented as a probability distribution over a set of topics(corresponding to coarse-grained senses). Each topic is a probability distribution over words, and the content of the topic is reflected in the words to which it assigns high probability. Topic models are <em>generative</em>, they specify a probabilistic procedure by which documents can be generated. Thus, to make a new document, one first chooses a distribution over topics. Then for each word in that document, one chooses a topic at random according to this distribution and selects a word from that topic. Under this framework, the problem of meaning representation is expressed as one of statistical inference: Given some data-words in a corpus-infer the latent structure from which it was generated.</p><h4 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h4><ul><li>semantic priming</li><li>discourse comprehension</li><li>word categorization</li><li>judgments of essay quality</li><li>synonymy tests</li><li>association</li></ul><h2 id="Composition"><a href="#Composition" class="headerlink" title="Composition"></a>Composition</h2><p>It is well known that linguistic structures are <em>compositional</em>(simpler elements are combined to form more complex ones).<br>Morphemes are combined into words, words into phrases, and phrases into sentences. It is also reasonable to assume that the meaning of sentences is composed of the meanings of individual words or phrases.<br>Compositionality allows languages to construct complex meanings from combinations of simpler elements. This property is often captured in the following principle: The meaning of a whole is a function of the meaning of the parts. Therefore, whatever approach we take to modeling semantics, representing the meanings of complex structures will involve modeling the way in which meanings combine.</p><h3 id="Preset"><a href="#Preset" class="headerlink" title="Preset"></a>Preset</h3><p>In this article, the authors attempt to bridge the gap in the literature by developing models of semantic composition that can represent the meaning of word combinations as opposed to individual words. Our models are narrower in scope compared with those developed in earlier connectionist work. Our vectors represent words; they are high-dimensional but relatively structured, and every component corresponds to a predefined context in which the words are found. The author take it as a defining property of the vectors they consider that the values of their components are derived from event frequencies such as the number of times a given word appears in a given context. Having this in mind, they present a general framework for vector-based composition that allows us to consider different classes of models. Specifically, they formulate composition as a function of two vectors and introduce models based on addition and multiplication. They also investigate how the choice of the underlying semantic representation interacts with the choice of composition function by comparing a spatial model that represents words as vectors in a high-dimensional space against a probabilistic model that represents words as topic distributions. They assess the performance of these models directly on a similarity task. They elicit similarity ratings for pairs of adjective–noun, noun–noun, and verb–object constructions and examine the strength of the relationship between similarity ratings and the predictions of their models.</p><h3 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h3><script type="math/tex; mode=display">\begin{equation}\mathbf{p}=f(\mathbf{u,v})\end{equation}</script><p>Express the composition of two constituents, <script type="math/tex">\mathbf{u}</script> and <script type="math/tex">\mathbf{v}</script>, in terms of a function acting on those constituents.</p><script type="math/tex; mode=display">\begin{equation}\mathbf{p}=f(\mathbf{u,v},R)\end{equation}</script><p>A further refinement of the above principle <strong>taking the role of syntax into account</strong>: The meaning of a whole is a function of the meaning of the parts and of the way they are syntactically combined. They thus modify the composition function in Eq. 1 to account for the fact that there is a syntactic relation <em>R</em> between constituents <script type="math/tex">\mathbf{u}</script> and <script type="math/tex">\mathbf{v}</script>.<br>Even this formulation may not be fully adequate. The meaning of the whole is greater than the meaning of the parts. The implication here is that language users are bringing more to the problem of constructing complex meanings than simply the meaning of the parts and their syntactic relations. This additional information includes both knowledge about the language itself and also knowledge about the real world. </p><script type="math/tex; mode=display">\begin{equation}\mathbf{p}=f(\mathbf{u,v},R,K)\end{equation}</script><p>A full understanding of the compositional process involves an account of how novel interpretations are integrated with existing knowledge. The composition function needs to be augmented to include an additional argument, <em>K</em>, representing any knowledge utilized by the compositional process.</p><p>Compositionality is a matter of degree rather than a binary notion. Linguistic structures range from fully compositional(e.g., <em>black hair</em>), to partly compositional syntactically fixed expressions,(e.g., <em>take advantage</em>), in which the constituents can still be assigned separate meanings, and noncompositional idioms(e.g., <em>kick the bucket</em>) or multiword expressions(e.g., <em>by and large</em>), whose meaning cannot be distributed across their constituents.</p><h4 id="Logic-based-view"><a href="#Logic-based-view" class="headerlink" title="Logic-based view"></a>Logic-based view</h4><p>Within symbolic logic, compositionality is accounted for elegantly by assuming a tight correspondence between syntactic expressions and semantic form. In this tradition, the meaning of a phrase or sentence is its truth conditions which are expressed in terms of truth relative to a model. In classical Montague grammar, for each syntactic category there is a uniform semantic type(e.g., sentences express propositions; nouns and adjectives express properties of entities; verbs express properties of events). Most lexical meanings are left unanalyzed and treated as <em>primitive</em>.<br>Noun is represented by logical symbol. Verb is represented by a function from entities to propositions, expressed in <a href="https://en.wikipedia.org/wiki/Lambda_calculus" target="_blank" rel="noopener">lambda calculus</a>.(Well I’m not familiar with lambda calculus yet, but the idea is similar to predicate logic in discrete mathematics.)<br>For example, the proper noun <em>John</em> is represented by the logical symbol <em>JOHN</em> denoting a specific entity, and the verb <em>wrote</em> is represented as <em>λx.WROTE(x)</em>. Applying this function to the entity <em>JOHN</em> yields the logical formula <em>WROTE(JOHN)</em> as a representation of the sentence <em>John wrote</em>. It is worth noting that the entity and predicate within this formula are represented symbolically, and that the connection between a symbol and its meaning is an arbitrary matter of convention.</p><p><strong>Advantage</strong><br>Allows composition to be carried out syntactically.<br>The laws of deductive logic in particular can be defined as syntactic processes which act irrespective of the meanings of the symbols involved.</p><p><strong>Disadvantage</strong><br>Abstracting away from the actual meanings may not be fully adequate for modeling semantic composition.<br>For example, <script type="math/tex">GOOD(JOHN)\wedge LAWYER(JOHN)</script> doesn’t mean that <em>John is a good lawyer</em>.<br>Modeling semantic composition means modeling the way in which meanings combine, and this requires that words have representations which are richer than single, arbitrary symbols.</p><h4 id="Connectionism"><a href="#Connectionism" class="headerlink" title="Connectionism"></a>Connectionism</h4><p>The key premise here is that knowledge is represented not as discrete symbols that enter into symbolic expressions, but as patterns of activation distributed over many processing elements. These representations are distributed in the sense that any single concept is represented as a pattern, that is, vector, of activation over many elements(nodes or units) that are typically assumed to correspond to neurons or small collections of neurons.</p><p><strong>Tensor product</strong><br>The tensor product <script type="math/tex">\mathbf{u}\otimes\mathbf{v}</script> is a matrix whose components are all the possible products <script type="math/tex">u_iv_j</script> of the components of vectors <strong>u</strong> and <strong>v</strong>.<br><img src="/images/composition/tensorProduct.jpg" alt><br>The tensor product has dimensionality m×n, which grows exponentially in size as more constituents are composed.</p><p><strong>Holographic reduced representation</strong><br>The tensor product is projected onto the space of the original vectors, thus avoiding any dimensionality increase.<br>The projection is defined in terms of <em>circular convolution</em>, which compresses the tensor product of two vectors. The compression is achieved by summing along the transdiagonal(?) elements of the tensor product. Noisy versions of the original vectors can be recovered by means of <em>circular correlation</em>, which is the approximate inverse of circular convolution. The success of circular correlation crucially depends on the components of the <em>n</em>-dimensional vectors <strong>u</strong> and <strong>v</strong> being real numbers and randomly distributed with mean 0 and variance 1/n.</p><p><strong>Binary spatter codes</strong><br>Binary spatter codes are a particularly simple form of holographic reduced representation. Typically, these vectors are random bit strings or binary <em>N</em> vectors (e.g., <em>N</em>=10000). Compositional representations are synthesized from parts or chunks. Chunks are combined by binding, which is the same as taking the exclusive or(XOR) of two vectors. Here, only the transdiagonal elements of the tensor product of two vectors are kept and the rest are discarded.</p><p>Both spatter codes and holographic reduced representations can be implemented efficiently and the dimensionality of the resulting vector does not change.<br>The downside is that operations like circular convolution are a form of lossy compression that introduces noise into the representation. To retrieve the original vectors from their bindings, a <em>clean-up memory</em> process is usually employed where the noisy vector is compared with all component vectors in order to find the closest one.</p><h4 id="Semantic-Space"><a href="#Semantic-Space" class="headerlink" title="Semantic Space"></a>Semantic Space</h4><p><strong>Premise</strong>: Words occurring within similar contexts are semantically similar.<br>Semantic space models extract from a corpus a set of counts representing the occurrences of a target word <em>t</em> in the specific context <em>c</em> of choice and then map these counts into the components of a vector in some space.<br>Semantic space models resemble the representations used in the connectionist literature. Words are represented as vectors and their meaning is distributed across many dimensions. Crucially, the vector components are neither binary nor randomly distributed(compared with holographic reduced representation and binary spatter code mentioned above). They correspond to co-occurrence counts, and it is assumed that differences in meaning arise from differences in the distribution of these counts across contexts.</p><h2 id="Composition-models"><a href="#Composition-models" class="headerlink" title="Composition models"></a>Composition models</h2><p><strong>Aim</strong>: construct vector representations for phrases and sentences.<br><strong>Note</strong>: the problem of combining semantic vectors to make a representation of a multiword phrase is different to the problem of how to incorporate information <em>about</em> multiword contexts into a distributional representation for a single target word.</p><p>Define <strong>p</strong>, the composition of vectors <strong>u</strong> and <strong>v</strong>, representing a pair of words which stand in some syntactic relation <em>R</em>, given some background knowledge <em>K</em> as: <script type="math/tex">\mathbf{p}=f(\mathbf{u},\mathbf{v},R,K)</script>.<br>To begin with, just ignore <em>K</em> so as to explore what can be achieved in the absence of any background or world knowledge.</p><p><strong>Assumption</strong><br>Constituents are represented by vectors which subsequently combine in some way to produce a new vector.<br><strong>p</strong> lies in the same space as <strong>u</strong> and (~?)<strong>v</strong>. This essentially means that all syntactic types have the same dimensionality.<br>The restriction renders the composition problem computationally feasible. </p><p>A hypothetical semantic space for <em>practical</em> and <em>difficulty</em></p><div class="table-container"><table><thead><tr><th></th><th>music</th><th>solution</th><th>economy</th><th>craft</th><th>reasonable</th></tr></thead><tbody><tr><td>practical</td><td>0</td><td>6</td><td>2</td><td>10</td><td>4</td></tr><tr><td>difficulty</td><td>1</td><td>8</td><td>4</td><td>4</td><td>0</td></tr></tbody></table></div><h3 id="Additive-composition-models"><a href="#Additive-composition-models" class="headerlink" title="Additive composition models"></a>Additive composition models</h3><h4 id="Simplest-addition"><a href="#Simplest-addition" class="headerlink" title="Simplest addition"></a>Simplest addition</h4><script type="math/tex; mode=display">\mathbf{p}=\mathbf{u}+\mathbf{v}</script><script type="math/tex; mode=display">\mathbf{practical}+\mathbf{difficulty}=\begin{bmatrix}1&14&6&14&4\end{bmatrix}</script><p>This model assumes that composition is a symmetric function of the constituents; in other words, the order of constituents essentially makes no difference.<br>Might be reasonable for certain structures, a list perhaps.</p><h4 id="Addition-with-neighbors"><a href="#Addition-with-neighbors" class="headerlink" title="Addition with neighbors"></a>Addition with neighbors</h4><p>A sum of predicate, argument, and a number of neighbors of the predicate:</p><script type="math/tex; mode=display">\mathbf{p}=\mathbf{u}+\mathbf{v}+\sum_in_i</script><p>Model the composition of a predicate with its argument in a manner that distinguishes the role of these constituents, making use of the lexicon of semantic representations to identify the features of each constituent relevant to their combination.<br>Considerable latitude is allowed in selecting the appropriate neighbors. Kintsch(2001) considers only the <em>m</em> most similar neighbors to the predicate, from which he subsequently selects <em>k</em>, those most similar to its argument.</p><p>E.g., in the composition of <em>practical</em> with <em>difficulty</em>, the chosen neighbor is <em>problem</em>, with <script type="math/tex">\mathbf{problem}=\begin{bmatrix}2&15&7&9&1\end{bmatrix}</script>,</p><script type="math/tex; mode=display">\mathbf{practical}+\mathbf{difficulty}+\mathbf{problem}=\begin{bmatrix}3&29&13&23&5\end{bmatrix}</script><p>In this process, the selection of relevant neighbors for the predicate plays a role similar to the integration of a representation with existing background knowledge in the original construction–integration model. Here, background knowledge takes the form of the lexicon from which the neighbors are drawn.</p><h4 id="Weighted-summation"><a href="#Weighted-summation" class="headerlink" title="Weighted summation"></a>Weighted summation</h4><p>Weight the constituents differentially in the summation.</p><script type="math/tex; mode=display">\mathbf{p}=\alpha\mathbf{v}+\beta\mathbf{u}</script><p>This makes the composition function asymmetric in <strong>u</strong> and <strong>v</strong> allowing their distinct syntactic roles to be recognized.<br>E.g., set α to 0.4 and β to 0.6.</p><script type="math/tex; mode=display">0.4\times\mathbf{practical}+0.6\times\mathbf{difficulty}=\begin{bmatrix}0.6&7.2&3.2&6.4&1.6\end{bmatrix}</script><p>(there’s some calculation mistake in the paper)</p><p><strong>Extrem form</strong></p><script type="math/tex; mode=display">\mathbf{p}=\mathbf{v}</script><p>One of the vectors(<strong>u</strong>) contributes nothing at all to the combination. It can serve a simple baseline against which to compare more sophisticated models.</p><h3 id="Multiplicative-function"><a href="#Multiplicative-function" class="headerlink" title="Multiplicative function"></a>Multiplicative function</h3><h4 id="Simple"><a href="#Simple" class="headerlink" title="Simple"></a>Simple</h4><script type="math/tex; mode=display">\mathbf{p}=\mathbf{u}\odot\mathbf{v}</script><p>where the symbol <script type="math/tex">\odot</script> represents multiplication of the corresponding components: <script type="math/tex">p_i=u_i\cdot v_i</script></p><script type="math/tex; mode=display">\mathbf{practical}\odot\mathbf{difficulty}=\begin{bmatrix}0&48&8&40&0\end{bmatrix}</script><p>It is still a symmetric function and thus does not take word order or syntax into account.</p><h4 id="Tensor-product"><a href="#Tensor-product" class="headerlink" title="Tensor product"></a>Tensor product</h4><script type="math/tex; mode=display">\mathbf{p}=\mathbf{u}\otimes\mathbf{v}</script><p>where the symbol <script type="math/tex">\otimes</script> stands for the operation of taking all pairwise products of the components of <strong>u</strong> and <strong>v</strong>: <script type="math/tex">p_{i,j}=u_i\cdot v_j</script></p><script type="math/tex; mode=display">\mathbf{practical}\otimes\mathbf{difficulty}=\begin{bmatrix}0&0&0&0&0\\6&48&24&24&0\\2&16&8&8&0\\10&80&40&40&0\\4&32&16&16&0\end{bmatrix}</script><h4 id="Circular-Convolution"><a href="#Circular-Convolution" class="headerlink" title="Circular Convolution"></a>Circular Convolution</h4><script type="math/tex; mode=display">\mathbf{p}=\mathbf{u}\circledast\mathbf{v}</script><p>where the symbol <script type="math/tex">\circledast</script> stands for a compression of the tensor product based on summing along its transdiagonal elements: <script type="math/tex">p_i=\sum_ju_j\cdot v_{(i-j)}</script><br>Subscripts are interpreted modulo <em>n</em> which gives the operation its circular nature.</p><script type="math/tex; mode=display">\mathbf{practical}\circledast\mathbf{difficulty}=\begin{bmatrix}80&62&66&50&116\end{bmatrix}</script><p>(the result in the paper is reversed, which seems wrong)</p><p>Temporarily not understand why multiplicative functions only affect magnitude but not direction, while additive models can have a considerable effect on both the magnitude and direction. And cosine similarity is itself insensitive to the magnitudes of vectors.</p><h3 id="Matrix-idea"><a href="#Matrix-idea" class="headerlink" title="Matrix idea"></a>Matrix idea</h3><h4 id="Basic"><a href="#Basic" class="headerlink" title="Basic"></a>Basic</h4><p>To see how the vector <strong>u</strong> can be thought of as something that modifies <strong>v</strong>, consider the partial product of <strong>C</strong> with <strong>u</strong>, producing a matrix which is called <strong>U</strong>.</p><script type="math/tex; mode=display">\mathbf{p=Cuv=Uv}</script><p>Here, the composition function can be thought of as the action of a matrix, <strong>U</strong>, representing one constituent, on a vector <strong>v</strong>, representing the other constituent. Since the authors’ decision to use vectors, they just make use of the  insight. Map a constituent vector, <strong>u</strong>, onto a matrix, <strong>U</strong>, while representing all words with vectors.</p><script type="math/tex; mode=display">U_{ij}=0,U_{ii}=u_i</script><p><strong>U</strong>‘s off-diagonal elements are zero and <strong>U</strong>‘s diagonal elements are equal to the components of <strong>u</strong>.<br>The action of this matrix on <strong>v</strong> is a type of dilation, in that it stretches and squeezes <strong>v</strong> in various directions. Specifically, <strong>v</strong> is scaled by a factor of <script type="math/tex">u_i</script> along the <em>i</em>th basis.<br>The drawback of this process is that its results are independent on the basis used.</p><h4 id="Parallel-and-Orthogonal-Decomposition"><a href="#Parallel-and-Orthogonal-Decomposition" class="headerlink" title="Parallel and Orthogonal Decomposition"></a>Parallel and Orthogonal Decomposition</h4><p>Ideally, we would like to have a basis-independent composition, that is, one which is based solely on the geometry of <strong>u</strong> and <strong>v</strong>. One way to achieve basis independence is by dilating <strong>v</strong> along the direction of <strong>u</strong>, rather than along the basis directions. Just decompose <strong>v</strong> into a component parallel to <strong>u</strong> and a component orthogonal to <strong>u</strong>, and then stretch the parallel component to modulate <strong>v</strong> to be more like <strong>u</strong>.<br><img src="/images/composition/decomposition.jpg" alt></p><script type="math/tex; mode=display">\mathbf{x=\frac{u\cdot v}{u\cdot u}u}</script><script type="math/tex; mode=display">\mathbf{y=v-x=v-\frac{u\cdot v}{u\cdot u}u}</script><p>By dilating <strong>x</strong> by a factor λ, while leaving <strong>y</strong> unchanged, we get a modified vector <strong>v</strong>‘, which has been stretched to emphasize the contribution of <strong>u</strong>:</p><script type="math/tex; mode=display">\mathbf{v'=\lambda x+y=\lambda\frac{u\cdot v}{u\cdot u}u+v-\frac{u\cdot v}{u\cdot u}u=(\lambda-1)\frac{u\cdot v}{u\cdot u}u+v}</script><p>Multiplying through by <script type="math/tex">\mathbf{u\cdot u}</script> makes the expression easier to work with(since the cosine similarity function is insensitive to the magnitudes of vectors)</p><script type="math/tex; mode=display">\mathbf{p=(u\cdot u)v+(\lambda-1)(u\cdot v)u}</script><p>From the given example, <script type="math/tex">\mathbf{practical\times practical}=156</script> and <script type="math/tex">\mathbf{practical\times difficulty}=96</script>. Assuming λ=2 and we can get</p><script type="math/tex; mode=display">156\times\mathbf{difficulty}+96\times\mathbf{practical}=\begin{bmatrix}156&1824&816&1584&384\end{bmatrix}</script><p>(Again there’s some mistakes in the paper, it confused the coefficient uu and uv.)</p><h2 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h2><h3 id="Code-implementation"><a href="#Code-implementation" class="headerlink" title="Code implementation"></a>Code implementation</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># The simplest addition</span></span><br><span class="line"><span class="comment"># p = u + v</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">additive</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        u: row vector</span></span><br><span class="line"><span class="string">        v: row vector</span></span><br><span class="line"><span class="string">        the size of u is the same as that of v</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.add(u, v)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Addition with neighbors</span></span><br><span class="line"><span class="comment"># p = u + v + n</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kintsch</span><span class="params">(u, v, n)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        u: row vector</span></span><br><span class="line"><span class="string">        v: row vector</span></span><br><span class="line"><span class="string">        n: list of row vectors</span></span><br><span class="line"><span class="string">        the size of u is the same as that of v, as well as the elements of n</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    result = u + v</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(n)):</span><br><span class="line">        result += n[i]</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line"><span class="comment"># Multiplication of the corresponding components(element-wise)</span></span><br><span class="line"><span class="comment"># p = u * v</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiplicative</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        u: row vector</span></span><br><span class="line"><span class="string">        v: row vector</span></span><br><span class="line"><span class="string">        the size of u is the same as that of v</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> u * v</span><br><span class="line"></span><br><span class="line"><span class="comment"># Tensor product</span></span><br><span class="line"><span class="comment"># p = u \otimes v</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">tensor</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        u: row vector</span></span><br><span class="line"><span class="string">        v: row vector</span></span><br><span class="line"><span class="string">        the size of u is the same as that of v</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> np.dot(u.T, v)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Circular convolution</span></span><br><span class="line"><span class="comment"># p = u \circledast v</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">circular</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        u: row vector</span></span><br><span class="line"><span class="string">        v: row vector</span></span><br><span class="line"><span class="string">        the size of u is the same as that of v</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># --------------------------------------</span></span><br><span class="line">    <span class="comment">#   Reference:</span></span><br><span class="line">    <span class="comment">#   Kh40tiK at StackOverflow</span></span><br><span class="line">    <span class="comment">#   https://stackoverflow.com/questions/35474078/python-1d-array-circular-convolution</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.real(np.fft.ifft(np.fft.fft(u) * np.fft.fft(v)))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># --------------------------------------</span></span><br><span class="line">    <span class="comment"># implementation with the definition of circular convolution</span></span><br><span class="line"></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    result = np.zeros(u.shape)</span></span><br><span class="line"><span class="string">    for i in range(result.shape[1]):</span></span><br><span class="line"><span class="string">        for j in range(u.shape[1]):</span></span><br><span class="line"><span class="string">            if i - j &lt; 0:</span></span><br><span class="line"><span class="string">                result[0][i] += u[0][j] * v[0][i - j + u.shape[1]]</span></span><br><span class="line"><span class="string">            else:</span></span><br><span class="line"><span class="string">                result[0][i] += u[0][j] * v[0][i - j]</span></span><br><span class="line"><span class="string">    return result</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Weighted additive</span></span><br><span class="line"><span class="comment"># p = alpha * u + beta * v</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weighted</span><span class="params">(u, v, alpha, beta)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        u: row vector</span></span><br><span class="line"><span class="string">        v: row vector</span></span><br><span class="line"><span class="string">        alpha: weighted parameter of u</span></span><br><span class="line"><span class="string">        beta: weighted parameter of v</span></span><br><span class="line"><span class="string">        the size of u is the same as that of v</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> alpha * u + beta * v</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dilation</span></span><br><span class="line"><span class="comment"># p=(u \cdot u) v + (lambda - 1) (u \cdot v) u</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dilation</span><span class="params">(u, v, lamda)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        u: row vector</span></span><br><span class="line"><span class="string">        v: row vector</span></span><br><span class="line"><span class="string">        lamda: dilation factor</span></span><br><span class="line"><span class="string">        the size of u as the same as that of v</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    uu = np.sum(u * u)</span><br><span class="line">    print(uu)</span><br><span class="line">    uv = np.sum(u * v)</span><br><span class="line">    print(uv)</span><br><span class="line">    <span class="keyword">return</span> uu * v + (lamda - <span class="number">1</span>) * uv * u</span><br><span class="line"></span><br><span class="line"><span class="comment"># Head only, ignoring the effect of u</span></span><br><span class="line"><span class="comment"># p = v</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">headOnly</span><span class="params">(u, v)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        u: row vector</span></span><br><span class="line"><span class="string">        v: row vector</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line"><span class="comment"># Semantic space for PRACTICAL and DIFFICULTY according to the paper</span></span><br><span class="line">practical = np.array([<span class="number">0</span>, <span class="number">6</span>, <span class="number">2</span>, <span class="number">10</span>, <span class="number">4</span>])</span><br><span class="line">difficulty = np.array([<span class="number">1</span>, <span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Reshape the "rank-1 array" to vector(1-dimension matrix)</span></span><br><span class="line">practical = practical.reshape([<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">difficulty = difficulty.reshape([<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">problem = np.array([<span class="number">2</span>, <span class="number">15</span>, <span class="number">7</span>, <span class="number">9</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">print(additive(practical, difficulty))</span><br><span class="line">print(kintsch(practical, difficulty, [problem]))</span><br><span class="line">print(multiplicative(practical, difficulty))</span><br><span class="line">print(tensor(practical, difficulty))</span><br><span class="line">print(weighted(practical, difficulty, <span class="number">0.4</span>, <span class="number">0.6</span>))</span><br><span class="line">print(circular(practical, difficulty))</span><br><span class="line">print(dilation(practical, difficulty, <span class="number">2</span>))</span><br></pre></td></tr></table></figure><h1 id="Comparison-Study-on-Critical-Components-in-Compositional-Model-for-Phrase-Representation"><a href="#Comparison-Study-on-Critical-Components-in-Compositional-Model-for-Phrase-Representation" class="headerlink" title="Comparison Study on Critical Components in Compositional Model for Phrase Representation"></a>Comparison Study on Critical Components in Compositional Model for Phrase Representation</h1><h2 id="Word-Representation"><a href="#Word-Representation" class="headerlink" title="Word Representation"></a>Word Representation</h2><p><strong>Count model</strong><br>The count model learns word vectors by calculating the co-occurrence frequency of each word with features. </p><p><strong>Predict model</strong><br>The predict model learns word vectors by maximizing the probability of the contexts in which the word is observed in the corpus.<br>The Skip-Gram model and CBOW model included in word2vec tool are most widely used for generating word vectors.</p><h3 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h3><p>A method described by many works. I’m just planning to write another post about Word2Vec in details.</p><h3 id="Retrofitting-Method"><a href="#Retrofitting-Method" class="headerlink" title="Retrofitting Method"></a>Retrofitting Method</h3><p>There’re some mistakes in introducing the retrofitting method in this paper(e.g., the neighbor word vector should be <script type="math/tex">q_j</script> instead of <script type="math/tex">q_i</script> which is confusing.) , so I just find the original paper describing retrofitting method: <a href="https://www.cs.cmu.edu/~hovy/papers/15HLT-retrofitting-word-vectors.pdf" target="_blank" rel="noopener">Retrofitting Word Vectors to Semantic Lexicons</a>.</p><p>Let <script type="math/tex">V=\{w_1,...,w_n\}</script> be a <strong>vocabulary</strong>, i.e., the set of word types, (<em>well, I don’t know why it’s the set of word types…</em>) and <script type="math/tex">\Omega</script> be an <strong>ontology</strong> that encodes semantic relations between words in <script type="math/tex">V</script>. <script type="math/tex">\Omega</script> is an undirected graph <script type="math/tex">(V,E)</script> with one vertex for each word type and edges <script type="math/tex">(w_i,w_j)\in E\subseteq V\times V</script> indicating a semantic relationship of interest.<br>The matrix <script type="math/tex">\hat{Q}</script> will be the collection of vector representation <script type="math/tex">\hat{q_i}\in\mathbb{R}^d</script>, for each <script type="math/tex">w_i\in V</script>, learned using a standard data-driven technique, where <script type="math/tex">d</script> is the length of the word vectors. The objective is to learn the matrix <script type="math/tex">Q=(q_1,...,q_n)</script> such that the columns are both close (under a distance metric) to their counterparts in <script type="math/tex">\hat{Q}</script> <em>and</em> to adjacent vertices in <script type="math/tex">\Omega</script>.<br>The distance between a pair of vectors is defined to be the Euclidean distance. Since we want the inferred word vector to be close to the observed value <script type="math/tex">\hat{q_i}</script> and close to its neighbors <script type="math/tex">q_j,\forall j</script> such that <script type="math/tex">(i,j)\in E</script>, the objective to be minimized becomes:</p><script type="math/tex; mode=display">\Psi(Q)=\sum_{i=1}^n[\alpha_i||q_i-\hat{q_i}||^2+\sum_{(i,j)\in E}\beta_{i,j}||q_i-q_j||^2]</script><p>where <script type="math/tex">\alpha</script> and <script type="math/tex">\beta</script> values control the relative strengths of associations.</p><h3 id="Word-Paraphrasing"><a href="#Word-Paraphrasing" class="headerlink" title="Word Paraphrasing"></a>Word Paraphrasing</h3><p>Related paper: <a href="https://arxiv.org/pdf/1506.03487.pdf" target="_blank" rel="noopener">From Paraphrase Database to Compositional Paraphrase Model and Back</a><br>Train word vectors with a contrastive max-margin objective function. Specifically, the training data consisting of a set X of word paraphrase pairs <script type="math/tex">(x_1,x_2)</script>, while <script type="math/tex">(t_1,t_2)</script> are negative examples that are the most similar word pairs to <script type="math/tex">(x_1,x_2)</script> in a mini-batch during optimization. The objective function is given as follows:</p><script type="math/tex; mode=display">\min{W_w}\frac{1}{|X|}(\sum_{(x_1,x_2)\in X}\max{(0,1-W_w^{x_1}\cdot W_w^{x_2}+W_w^{x_1}\cdot W_w^{t_1})+\max{(0,1-W_w^{x_1}\cdot W_w^{x_2}+W_w^{x_2}\cdot W_w^{t_2})}+\lambda||W_{w_{initial}}-W_w||^2}</script><p>where <script type="math/tex">\lambda</script> is the regularization parameter, <script type="math/tex">|X|</script> is the length of training paraphrase pairs, <script type="math/tex">W_w</script> is the target word vector matrix, and <script type="math/tex">W_{w_{initial}}</script> is the initial word vector matrix.</p><hr><h2 id="Training-data"><a href="#Training-data" class="headerlink" title="Training data"></a>Training data</h2><p>For bigram phrase similarity task, there are two general types of training data in existing work.</p><p><strong>Pseudo-word training data</strong><br>Consists of tuples in the form {adjective1 noun1, adjective1-noun1}. E.g., {older man, older-man}.<br>Pseudo-word training data have been widely used in composition models.</p><p><strong>Pair training data</strong><br>Consists of tuples in the form {adjective1 noun1, adjective2 noun2}. E.g., {older man, elderly woman}.</p><p>For multi-word phrase, pair training data is the first choice because it is hard to learn the representation of pseudo-words with multiple words. So this paper use pair training data for multi-word phrase experiments.</p><h2 id="Composition-Function"><a href="#Composition-Function" class="headerlink" title="Composition Function"></a>Composition Function</h2><p>With a phrase <em>p</em> consisting of two words <script type="math/tex">w^{(1)}</script> and <script type="math/tex">w^{(2)}</script>, we can establish equation as follows:</p><script type="math/tex; mode=display">f(\vec w^{(1)},\vec w^{(2)}=\vec{p})</script><p>where <script type="math/tex">\vec w^{(1)},\vec w^{(2)}</script> are the word representations, <script type="math/tex">\vec{p}</script> is the phrase representation, and <script type="math/tex">f</script> is the composition function.</p><p>The Additive model assumes that the meaning of the composition is a linear combination of the constituent words.<br>The Additive model described the phenomenon: people concatenate the meanings of two words when understanding phrases.</p><p>The Multiplicative model assumes that the meaning of composition is the element-wise product of the two vectors.<br>The multiplicative model has the highest correlation with the neural activity observed in human brain when reading adjective-noun phrase data.</p><p>Composition functions such as Matrix, RecNN(Recursive Neural Network) and RNN transform word vectors into another vector space through matrix transformations and nonlinear functions. These three functions differ primarily in the order of transformation.<br>The Matrix model first transforms component words into another vector space and then composes them using addition. The RecNN model takes word order into consideration, concatenates vector component words and then transforms the vector using a matrix and nonlinearity. The RNN model composes words in a sentence from left to right by forming new representations from previous representations and the representation of the current word.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Composition-in-Distributional-Models-of-Semantics&quot;&gt;&lt;a href=&quot;#Composition-in-Distributional-Models-of-Semantics&quot; class=&quot;headerlink&quot; t
      
    
    </summary>
    
      <category term="Natural Language Processing" scheme="http://haelchan.me/categories/Natural-Language-Processing/"/>
    
    
      <category term="reading note" scheme="http://haelchan.me/tags/reading-note/"/>
    
      <category term="compositional model" scheme="http://haelchan.me/tags/compositional-model/"/>
    
  </entry>
  
  <entry>
    <title>COMS W4705 Natural Language Processing Note</title>
    <link href="http://haelchan.me/2018/03/31/NLP-note/"/>
    <id>http://haelchan.me/2018/03/31/NLP-note/</id>
    <published>2018-03-31T15:45:27.000Z</published>
    <updated>2018-10-18T15:06:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Week-One"><a href="#Week-One" class="headerlink" title="Week One"></a>Week One</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>What is natural language processing?<br>computers using natural language as input and/or output<br>NLP contains two types: understanding(NLU) and generation(NLG).<br><img src="/images/NLP/overview.jpg" alt></p><h3 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h3><p><strong>Machine Translation</strong><br>Translate from one language to another.<br><strong>Information Extraction</strong><br>Take some text as input, and produce structured(database) representation of some key content in the text.<br>Goal: Map a document collection to structured database<br>Motivation: Complex searches, Statistical queries.</p><p><strong>Text Summarization</strong><br>Take a single document, or potentially a group of several documents, and try to condense them down to summarize main information in those documents.</p><p><strong>Dialogue Systems</strong><br>Human can interact with computer.</p><h3 id="Basic-NLP-Problems"><a href="#Basic-NLP-Problems" class="headerlink" title="Basic NLP Problems"></a>Basic NLP Problems</h3><h4 id="Tagging"><a href="#Tagging" class="headerlink" title="Tagging"></a>Tagging</h4><p>Strings to Tagged Sequences<br>Examples:<br><strong>Part-of-speech tagging</strong><br>Profits(/N) soared(/V) at(/P) Boeing(/N) Co.(/N) .(/.) easily(/ADV) topping(/V) forecasts(/N) on (/P) Wall(/N) Street(/N) .(/.)</p><p><strong>Name Entity Recognition</strong><br>Profits(/NA) soared(/NA) at(/NA) Boeing(/SC) Co.(/CC) .(/NA) easily(/NA) topping(/NA) forecasts(/NA) on (/NA) Wall(/SL) Street(/CL) .(/.)<br>/NA: not any entity<br>/SC: start of company<br>/CC: continuation of company<br>/SL: start of location<br>/CL: continuation of location</p><h4 id="Parsing"><a href="#Parsing" class="headerlink" title="Parsing"></a>Parsing</h4><p><img src="/images/NLP/parsing.jpg" alt></p><h3 id="Challenges"><a href="#Challenges" class="headerlink" title="Challenges"></a>Challenges</h3><blockquote><p>At last, a computer that understands you like your mother.</p></blockquote><h4 id="Ambiguity"><a href="#Ambiguity" class="headerlink" title="Ambiguity"></a>Ambiguity</h4><p>This sentence can be interpreted in different ways:<br>1.It understands you as well as your mother understands you.<br>2.It understands (that) you like your mother.<br>3.It understands you as well as it understands your mother.</p><p>Ambiguity at Many Levels:<br><strong>Acoustic</strong> level<br>In speech recognition:<br>1.”… a computer that understands <em>you like</em> your mother”<br>2.”… a computer that understands <em>lie cured</em> mother”</p><p><strong>Syntactic</strong> level<br><img src="/images/NLP/syntactic.jpg" alt></p><p><strong>Semantic</strong> level<br>A word may have a variety of meanings and it may cause word sense ambiguity.</p><p><strong>Discourse</strong> (multi-clause) level<br>Alice says they’ve built a computer that understands you like your mother<br>But she…<br>  … doesn’t know any details<br>  … doesn’t understand me at all<br>This is an instance of <strong>anaphora</strong>, where <em>she</em> referees to some other discourse entity.</p><h2 id="Language-Modeling"><a href="#Language-Modeling" class="headerlink" title="Language Modeling"></a>Language Modeling</h2><p>· We have some (finite) vocabulary, say <script type="math/tex">\mathcal{V}={\text{the, a, man, telescope, Beckham, two, ...}}</script></p><p>· We have an (infinite) set of strings, <script type="math/tex">\mathcal{V}^\dagger</script>:</p><blockquote><p>the STOP<br>a STOP<br>the fan STOP<br>the fan saw Beckham STOP<br>the fan saw saw STOP<br>the fan saw Beckham play for Real Madrid STOP</p></blockquote><p>· We have a <em>training sample</em> of example sentences in English<br>· We need to “learn” a probability distribution <em>p</em> i.e., <em>p</em> is a function that satisfies</p><script type="math/tex; mode=display">\sum_{x\in\mathcal{V^\dagger}}p(x)=1,\ p(x)\ge0\text{ for all }x\in\mathcal{V^\dagger}</script><p>Definition: A language model consists of a finite set <script type="math/tex">\mathcal{V}</script>, and a function <script type="math/tex">p(x_1,x_2,...,x_n)</script> such that:<br>1.For any <script type="math/tex"><x_1...x_n>\in\mathcal{V^\dagger}</script>, <script type="math/tex">p(x_1,x_2,...,x_n)\ge 0</script><br>2.In addition, <script type="math/tex">\sum_{<x_1...x_n>\in\mathcal{V^\dagger}}p(x_1,x_2,...,x_n)=1</script></p><p>Language models are very useful in a broad range of applications, the most obvious perhaps being speech recognition and machine translation. In many applications it is very useful to have a good “prior” distribution <script type="math/tex">p(x_1...x_n)</script> over which sentences are or aren’t probable in a language. For example, in speech recognition the language model is combined with an acoustic model that models the pronunciation of different words: one way to think about it is that the acoustic model generates a large number of candidate sentences, together with probabilities; the language model is then used to reorder these possibilities based on how likely they are to be a sentence in the language.<br>The techniques we describe for defining the function <em>p</em>, and for estimating the parameters of the resulting model from training examples, will be useful in several other contexts during the course: for example in hidden Markov models and in models for natural language parsing.</p><h3 id="A-Naive-Method"><a href="#A-Naive-Method" class="headerlink" title="A Naive Method"></a>A Naive Method</h3><p>We have N training sentences.<br>For any sentence <script type="math/tex">x_1...x_n,\ c(x_1...x_n)</script> is the number of times the sentence is seen in our training data.</p><script type="math/tex; mode=display">p(x_1...x_n)=\frac{c(x_1...x_n)}{N}</script><p>This is a poor model: in particular it will assign probability 0 to any sentence not seen in the training corpus. Thus it fails to generalize to sentences that have not seen in the training data. The key technical contribution of this chapter will be to introduce methods that do generalize to sentences that are not seen in our training data.</p><h3 id="Markov-Models"><a href="#Markov-Models" class="headerlink" title="Markov Models"></a>Markov Models</h3><p><strong>First-Order Markov Processes</strong></p><script type="math/tex; mode=display">\begin{aligned}&P(X_1=x_1,X_2=x_2,...,X_n=x_n)\\=&P(X_1=x_1)\prod_{i=2}^nP(X_i=x_i|X_1=x_1,...,X_{i-1}=x_{i-1})\\=&P(X_1=x_1)\prod_{i=2}^nP(X_i=x_i|X_{i-1}=x_{i-1})\end{aligned}</script><p>The first step is exact: by the chain rule of probabilities, <em>any</em> distribution <script type="math/tex">P(X_1=x_1...X_n=x_n)</script> can be written in this form. So we have made no assumptions in this step of the derivation. However, the second step is not necessarily exact: we have made the assumption that for any <script type="math/tex">i\in\{2...n\}</script>, for any <script type="math/tex">x_1...x_i</script>,</p><script type="math/tex; mode=display">P(X_i=x_i|X_1=x_1...X_{i-1}=x_{i-1})=P(X_i=x_i|X_{i-1}=x_{i-1})</script><p>This is a first-order <em>Markov assumption</em>. We have assumed that the identity of the i’th word in the sequence depends only on the identity of the previous word, <script type="math/tex">x_{i-1}</script>. More formally, we have assumed that the value of <script type="math/tex">X_i</script> is conditionally independent of <script type="math/tex">X_1...X_{i-2}</script>, given the value of <script type="math/tex">X_{i-1}</script>.</p><p><strong>Second-Order Markov Processes</strong></p><script type="math/tex; mode=display">\begin{aligned}&P(X_1=x_1,X_2=x_2,...,X_n=x_n)\\=&P(X_1=x_1)\times P(X_2=x_2|X_1=x_1)\prod_{i=3}^nP(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\\=&\prod_{i=1}^nP(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})\end{aligned}</script><p>(For convenience we assume <script type="math/tex">x_0=x_{-1}=*</script>, where * is a special “start” symbol.)</p><p>Compared with first-order Markov process, we make a slightly weaker assumption, namely that each word depends on the previous <em>two</em> words in the sequence. And the second-order Markov process will form the basis of trigram language models.</p><p>The length of sequence <em>n</em> can itself vary. We just assume that the <em>n</em>‘th word in the sequence, <script type="math/tex">X_n</script> is always equal to a special symbol, the STOP symbol. This symbol can only appear at the end of a sequence and it doesn’t belong to the set <script type="math/tex">\mathcal{V}</script>.</p><p>Process:</p><ol><li>Initialize <em>i</em> = 1, and <script type="math/tex">x_0=x_{-1}=*</script></li><li>Generate <script type="math/tex">x_i</script> from the distribution <script type="math/tex">P(X_i=x_i|X_{i-2}=x_{i-2},X_{i-1}=x_{i-1})</script></li><li>If <script type="math/tex">x_i=STOP</script> then return the sequence <script type="math/tex">x_1...x_n</script>. Otherwise, set <script type="math/tex">i=i+1</script> and return to step 2.</li></ol><h3 id="Trigram"><a href="#Trigram" class="headerlink" title="Trigram"></a>Trigram</h3><p>A trigram language model consists of a finite set <script type="math/tex">\mathcal{V}</script>, and a parameter <script type="math/tex">q(w|u,v)</script> for each trigram u,v,w such that <script type="math/tex">w\in\mathcal{V}\cup\{STOP\}</script>, and <script type="math/tex">u,v\in\mathcal{V}\cup\{*\}</script>. The value for <script type="math/tex">q(w|u,v)</script> can be interpreted as the probability of seeing the word <em>w</em> immediately after the bigram <em>(u,v)</em>. For any sentence <script type="math/tex">x_1...x_n</script> where <script type="math/tex">x_i\in\mathcal{V}</script> for <em>i</em> = 1…(n-1), and <script type="math/tex">x_n=STOP</script>, the probability of the sentence under the trigram language model is <script type="math/tex">p(x_1...x_n)=\prod_{i=1}^nq(x_i|x_{i-2},x_{i-1})</script>, where we define <script type="math/tex">x_0=x_{-1}=*</script>.</p><p>Estimation Problem:<br>A natural estimate (the “maximum likelihood estimate”):</p><script type="math/tex; mode=display">q(w_i|w_{i-2},w_{i-1})=\frac{Count(w_{i-2},w_{i-1},w_i)}{Count(w_{i-2},w_{i-1})}</script><p>For example:</p><script type="math/tex; mode=display">q(\text{barks|the, dog})=\frac{c(\text{the, dog, barks})}{c(\text{the, dog})}</script><p>This way of estimating parameters runs into a very serious issue. Say our vocabulary size is <script type="math/tex">N=|\mathcal{V}|</script>, then there are <script type="math/tex">N^3</script> parameters in the model. This leads to two problems:<br>· Many of the above estimates will be <script type="math/tex">q(w|u,v)=0</script>, due to the count in the numerator being 0. This will lead to many trigram probabilities being systematically underestimated: it seems unreasonable to assign probability 0 to any trigram not seen in training data, given that the number of parameters of the model is typically very large in comparison to the number of words in the training corpus.<br>· In cases where the denominator <script type="math/tex">c(u,v)</script> is equal to zero, the estimate is not well defined.</p><p><strong>Perplexity</strong><br>We have some test data sentences <script type="math/tex">x^{(1)},x^{(2)},...,x^{(m)}</script>. Note that test sentences are “held out”, in the sense that they are not part of the corpus used to estimate the language model. (Just the same as normal machine learning problems.)<br>We could look at the probability under our model <script type="math/tex">\prod_{i=1}^mp(s_i)</script>.<br>More conveniently, the log probability:</p><script type="math/tex; mode=display">\log\prod_{i=1}^mp(s_i)=\sum_{i=1}^m\log p(s_i)</script><p>In fact the usual evaluation measure is :</p><script type="math/tex; mode=display">\text{Perplexity}=2^{-l}, \text{where }l=\frac{1}{M}\sum_{i=1}^m\log p(s_i)</script><p>and M is the total number of words in the test data <script type="math/tex">M=\sum_{i=1}^mn_i</script>. <script type="math/tex">n_i</script> is the length of the <em>i</em>‘th test sentence.</p><h3 id="Linear-Interpolation"><a href="#Linear-Interpolation" class="headerlink" title="Linear Interpolation"></a>Linear Interpolation</h3><script type="math/tex; mode=display">q_{ML}(w|u,v)=\frac{c(u,v,w)}{c(u,v)}</script><script type="math/tex; mode=display">q_{ML}(w|v)=\frac{c(v,w)}{c(v)}</script><script type="math/tex; mode=display">q_{ML}(w)=\frac{c(w)}{c()}</script><p>The subscript <em>ML</em> means <em>maximum-likelihood</em> estimation. And <script type="math/tex">c(w)</script> is the number of times word <em>w</em> is seen in the training corpus, and <script type="math/tex">c()</script> is the total number of words seen in the training corpus.<br>The trigram, bigram and unigram estimates have different strengths and weaknesses. The idea in linear interpolation is to use all three estimates, by taking a weighted average of the three estimates:</p><script type="math/tex; mode=display">q(w|u,v)=\lambda_1\times q_{ML}(w|u,v)+\lambda_2\times q_{ML}(w|v)+\lambda_3\times q_{ML}(w)</script><p>Here <script type="math/tex">\lambda_1</script>, <script type="math/tex">\lambda_2</script> and <script type="math/tex">\lambda_3</script> are three additional parameters of the model, which satisfies <script type="math/tex">lambda_1+\lambda_2+\lambda_3=1</script> and <script type="math/tex">\lambda_i\ge0</script> for all <em>i</em>.<br>(Our estimate correctly defines a distribution.)</p><p>There are various ways of estimating the λ values. A common one is as follows. Say we have some additional held-out data(development data), which is separate from both our training and test corpora.<br>Define <script type="math/tex">c'(u,v,w)</script> to be the number of times that the trigram (u,v,w) is seen in the development data. </p><script type="math/tex; mode=display">L(\lambda_1,\lambda_2,\lambda_3)=\sum_{u,v,w}c'(u,v,w)\log q(w|u,v)\\=\sum_{u,v,w}c'(u,v,w)\log(lambda_1\times q_{ML}(w|u,v)+\lambda_2\times q_{ML}(w|v)+\lambda_3\times q_{ML}(w))</script><p>We would like to choose our λ values to maximize <script type="math/tex">L(\lambda_1,\lambda_2,\lambda_3)</script>.(It means minimize perplexity.)</p><p>The three parameters <script type="math/tex">\lambda_1,\lambda_2,\lambda_3</script> can be interpreted as an indication of the confidence, or weight, placed on each of the trigram, bigram and unigram estimates.</p><p>In practice, it is important to add an additional degree of freedom, by allowing the values for <script type="math/tex">\lambda_1,\lambda_2,\lambda_3</script> to vary.<br>Take a function <script type="math/tex">\Pi</script> that partitions histories<br>e.g.,</p><script type="math/tex; mode=display">\Pi(w_{i-2},w_{i-1})=\begin{cases}1\text{  If Count}(w_{i-1},w_{i-2})=0\\2\text{  If }1\le Count(w_{i-1},w_{i-2})\le2\\3\text{  If }3\le Count(w_{i-1},w_{i-2})\le5\\4\text{  Otherwise}\end{cases}</script><p>Introduce a dependence of the λ’s on the partition:</p><script type="math/tex; mode=display">\begin{aligned}q(w_i|w_{i-2},w_{i-1})=&\lambda_1^{\Pi(w_{i-2},w_{i-1})}\times q_{ML}(w_i|w_{i-2},w_{i-1})\\ +&\lambda_2^{\Pi(w_{i-2},w_{i-1})}\times q_{ML}(w_i|w_{i-1})\\ +&\lambda_3^{\Pi(w_{i-2},w_{i-1})}\times q_{ML}(w_i)\end{aligned}</script><p>where <script type="math/tex">\lambda_1^{\Pi(w_{i-2},w_{i-1})}+\lambda_2^{\Pi(w_{i-2},w_{i-1})}+\lambda_3^{\Pi(w_{i-2},w_{i-1})}=1</script>, and <script type="math/tex">\lambda_i^{\Pi(w_{i-2},w_{i-1})}\ge0</script> for all <em>i</em>.</p><p>By the way, <script type="math/tex">lambda_1</script> should be 0 if <script type="math/tex">c(u,v)=0</script>, because in this case the trigram estimate <script type="math/tex">q_{ML}(w|u,v)=\frac{c(u,v,w)}{c(u,v)}</script> is undefined. Similarly, if both <script type="math/tex">c(u,v)</script> and <script type="math/tex">c(v)</script> are equal to zero, we need <script type="math/tex">\lambda_1=\lambda_2=0</script></p><p>In the referencing note, a simple method is introduced:</p><script type="math/tex; mode=display">\lambda_1=\frac{c(u,v)}{c(u,v)+\gamma}</script><script type="math/tex; mode=display">\lambda_2=(1-\lambda_1)\times\frac{c(v)}{c(v)+\gamma}</script><script type="math/tex; mode=display">\lambda_3=1-\lambda_1-\lambda_2</script><p>γ&gt;0.<br>Under this definition, it can be seen that <script type="math/tex">\lambda_1</script> increases as <script type="math/tex">c(u,v)</script> increases, and similarly that <script type="math/tex">\lambda_2</script> increases as <script type="math/tex">c(v)</script> increases. This method is relatively crude, and is not likely to be optimal. It is, however, very simple, and in practice it can work well in some applications.</p><h3 id="Discounting-Methods"><a href="#Discounting-Methods" class="headerlink" title="Discounting Methods"></a>Discounting Methods</h3><p>For any bigram <script type="math/tex">Count(w_{i-1},w)</script> such that <script type="math/tex">Count(w_{i-1},w)>0</script>, we define the discounted count as</p><script type="math/tex; mode=display">Count^*(w_{i-1},w)=Count(w_{i-1},w)-\beta</script><p>where β is a value between 0 and 1 (a typical value might be β=0.5). Thus we simply subtract a constant value, β, from the count. This reflects the intuition that if we take counts from the training corpus, we will systematically over-estimate the probability of bigrams seen in the corpus(and under-estimate bigrams not seen in the corpus).<br>For any bigram <script type="math/tex">(w_{i-1},w)</script> such that <script type="math/tex">Count(w_{i-1},w)>0</script>, we can then define <script type="math/tex">q(w|w_{i-1})=\frac{Count^*(w_{i-1},w)}{Count(w_{i-1})}</script>. For any context <script type="math/tex">w_{i-1}</script>, this definition leads to some <em>missing probability mass</em>, defined as <script type="math/tex">\alpha(v)=1-\sum_{w:Count(w_{i-1},w)>0}\frac{Count^*(w_{i-1},w)}{Count(w_{i-1})}</script><br>The intuition behind discounted methods is to divide this “missing mass” between the words <em>w</em> such that <script type="math/tex">Count(w_{i-1},w)=0</script>.<br>Define two sets</p><script type="math/tex; mode=display">\mathcal{A}(w_{i-1})=\{w:Count(w_{i-1},w)>0</script><script type="math/tex; mode=display">\mathcal{B}(w_{i-1})=\{w:Count(w_{i-1},w)=0</script><p>Then the back-off model is defined as</p><script type="math/tex; mode=display">q_{BO}(w_i|w_{i-1})=\begin{cases}\frac{Count^*(w_{i-1},w_i)}{Count(w_{i-1})}\text{  if }w_i\in\mathcal{A}(w_{i-1})\\\alpha(w_{i-1})\frac{q_{ML}(w_i)}{\sum_{w\in\mathcal{B}(w_{i-1})}q_{ML}(w)}\text{  if }w_i\in\mathcal{B}(w_{i-1})\end{cases}</script><p>Thus if <script type="math/tex">Count(w_{i-1},w)>0</script> we return the estimate <script type="math/tex">\frac{Count^*(w_{i-1},w_i)}{Count(w_{i-1})}</script>; otherwise we divide the remaining probability mass <script type="math/tex">\alpha(w_{i-1})</script> in proportion to the unigram estimate <script type="math/tex">q_{ML}(w)</script>.<br>The method can be generalized to trigram language models in a natural, recursive way:<br><img src="/images/NLP/trigramDiscount.jpg" alt></p><h1 id="Week-Two"><a href="#Week-Two" class="headerlink" title="Week Two"></a>Week Two</h1><h2 id="Tagging-1"><a href="#Tagging-1" class="headerlink" title="Tagging"></a>Tagging</h2><p>Given the input to the tagging model(referred as a <em>sentence</em>) <script type="math/tex">x_1...x_n</script>, use <script type="math/tex">y_1...y_n</script> to denote the output of the tagging model(referred as the <em>state sequence</em> or <em>tag sequence</em>).<br>This type of problem, where the task is to map a sentence <script type="math/tex">x_1...x_n</script> to a tag sequence <script type="math/tex">y_1...y_n</script> is often referred to as a <strong>sequence labeling problem</strong>, or a <strong>tagging problem</strong>.<br>We will assume that we have a set of training examples, <script type="math/tex">(x^{(i)},y^{(i)})</script> for <script type="math/tex">i=1...m</script>, where each <script type="math/tex">x^{(i)}</script> is a sentence <script type="math/tex">x_1^{(i)}...x_{n_i}^{(i)}</script>, and each <script type="math/tex">y^{(i)}</script> is a tag sequence <script type="math/tex">y^{(i)}_1...y^{(i)}_{n_i}</script>(we assume that the <em>i</em>‘th training example is of length <script type="math/tex">n_i</script>). Hence <script type="math/tex">x_j^{(i)}</script> is the <em>j</em>‘th word in the <em>i</em>‘th training example, and <script type="math/tex">y_j^{(i)}</script> is the tag for that word. Our task is to learn a function that maps sentences to tag sequences from these training examples.</p><h3 id="POS-Tagging"><a href="#POS-Tagging" class="headerlink" title="POS Tagging"></a>POS Tagging</h3><p>POS Tagging: Part-of-Speech tagging.<br>The input to the problem is a sentence. The output is a tagged sentence, where each word in the sentence is annotated with its part of speech. Our goal will be to construct a model that recovers POS tags for sentences with high accuracy. POS tagging is one of the most basic problems in NLP, and is useful in many natural language applications.</p><h4 id="Tags"><a href="#Tags" class="headerlink" title="Tags"></a>Tags</h4><ul><li>D: determiner (a, an, the)</li><li>N: noun</li><li>V: verb</li><li>P: preposition</li><li>Adv: adverb</li><li>Adj: adjective</li><li>…</li></ul><h4 id="Challenges-1"><a href="#Challenges-1" class="headerlink" title="Challenges"></a>Challenges</h4><p><strong>Ambiguity</strong><br>Many words in English can take several possible parts of speech(as well as in Chinese and many other languages). A word can be a noun as well as a verb. E.g., <em>look</em>, <em>result</em>…<br><strong>Rare words</strong><br>Some words are rare and may not be seen in the training examples. Even with say a million words of training data, there will be many words in new sentences which have not been seen in training. It will be important to develop methods that deal effectively with words which have not been seen in training data.</p><h4 id="Sources-of-information"><a href="#Sources-of-information" class="headerlink" title="Sources of information"></a>Sources of information</h4><p><strong>Local</strong><br>Individual words have statistical preferences for their part of speech.<br>E.g., <em>can</em> is more likely to be a modal verb rather than a noun.</p><p><strong>Contextual</strong><br>The context has an important effect on the part of speech for a word. In particular, some sequences of POS tags are much more likely than others. If we consider POS trigrams, the sequence <code>D N V</code> will be frequent in English, whereas the sequence <code>D V N</code> is much less likely.<br>Sometimes these two sources of evidence are in conflict. For example, in the sentence <em>The trash can is hard to find</em>, the part of speech for <em>can</em> is a noun-however, <em>can</em> can also be a modal verb, and in fact it is much more frequently seen as a modal verb in English. In this sentence the context has overridden the tendency for <em>can</em> to be a verb as opposed to a noun.</p><h3 id="Named-Entity"><a href="#Named-Entity" class="headerlink" title="Named-Entity"></a>Named-Entity</h3><p>For named-entity problem, the input is again a sentence. The output is the sentence with entity-boundaries marked. Recognizing entities such as people, locations and organizations has many applications, and name-entity recognition has been widely studies in NLP research.</p><p>Once this mapping has been performed on training examples, we can train a tagging model on these training examples. Given a new test sentence we can then recover the sequence of tags from the model, and it is straightforward to identify the entities identified by the model.</p><h2 id="Generative-Models"><a href="#Generative-Models" class="headerlink" title="Generative Models"></a>Generative Models</h2><p><strong>Supervised Learning:</strong><br>Assume training examples <script type="math/tex">(x^{(1)},y^{(1)})...(x^{(m)},y^{(m)})</script>, where each example consists of an input <script type="math/tex">x^{(i)}</script> paired with a label <script type="math/tex">y^{(i)}</script>. We use <script type="math/tex">\mathcal{X}</script> to refer to the set of possible inputs, and <script type="math/tex">\mathcal{Y}</script> to refer to the set of possible labels. Our task is to learn a function <script type="math/tex">f:\mathcal{X}\to\mathcal{Y}</script> that maps any input x to a label f(x).<br>One way to define the function f(x) is through a <em>conditional model</em>. In this approach we define a model that defines the conditional probability <script type="math/tex">p(y|x)</script> for any x,y pair. The parameters of the model are estimated from the training examples. Given a new test example x, the output from the model is <script type="math/tex">f(x)=\arg\max_{y\in\mathcal{Y}}p(y|x)</script><br>Thus we simply take the most likely label <em>y</em> as the output from the model. If our model <script type="math/tex">p(y|x)</script> is close to the true conditional distribution of labels given inputs, the function f(x) will be close to optimal.</p><p><strong>Generative Models</strong><br>Rather than directly estimating the conditional distribution <script type="math/tex">p(y|x)</script>, in generative models we instead model the <em>joint</em> probability <script type="math/tex">p(x,y)</script> over <script type="math/tex">(x,y)</script> pairs. The parameters of the model <script type="math/tex">p(x,y)</script> are again estimated from the training examples <script type="math/tex">(x^{(i)},y^{(i)})</script> for <script type="math/tex">I=1...n</script>. In many cases we further decompose the probability <script type="math/tex">p(x,y)</script> as <script type="math/tex">p(x,y)=p(y)p(x|y)</script> and then estimate the models for <script type="math/tex">p(y)</script> and <script type="math/tex">p(x|y)</script> separately. These two model components have the following interpretations:<br>· <script type="math/tex">p(y)</script> is a <em>prior</em> probability distribution over labels <em>y</em>.<br>· <script type="math/tex">p(x|y)</script> is the probability of generating the input <em>x</em>, given that the underlying label is <em>y</em>.</p><p>Given a generative model, we can use Bayes rule to derive the condition probability <script type="math/tex">p(y|x)</script> for any <script type="math/tex">(x,y)</script> pair:</p><script type="math/tex; mode=display">p(y|x)=\frac{p(y)p(x|y)}{p(x)}</script><p>where</p><script type="math/tex; mode=display">p(x)=\sum_{y\in\mathcal{Y}}p(x,y)=\sum_{y\in\mathcal{Y}}p(y)p(x|y)</script><p>We use Bayes rule directly in applying the joint model to a new test example. Given an input <em>x</em>, the output of our model, f(x), can be derived as follows:</p><script type="math/tex; mode=display">\begin{aligned}f(x)&=\arg\max_yp(y|x)\\&=\arg\max_y\frac{p(y)p(x|y)}{p(x)}\text{  (2)}\\&=\arg\max_yp(y)p(x|y)\text{  (3)}\end{aligned}</script><p>Eq. 2 follows by Bayes rule. Eq. 3 follows because the denominator, <script type="math/tex">p(x)</script>, does not depend on <em>y</em>, and hence does not affect the arg max. This is convenient, because it mean that we do not need calculate <script type="math/tex">p(x)</script>, which can be an expensive operation.<br>Models that decompose a joint probability into terms <script type="math/tex">p(y)</script> and <script type="math/tex">p(x|y)</script> are often called <em>noisy-channel</em> models. Intuitively, when we see a test example <em>x</em>, we assume that has been generated in two steps: first, a label <em>y</em> has been chosen with probability <script type="math/tex">p(y)</script>; second, the example <em>x</em> has been generated from the distribution <script type="math/tex">p(x|y)</script>. The model <script type="math/tex">p(x|y)</script> can be interpreted as a “channel” which takes a label <em>y</em> as its input, and corrupts it to produce <em>x</em> as its output. Our task is to find the most likely label <em>y</em>, given that we observe <em>x</em>.</p><p>In summary:</p><ul><li>Our task is to learn a function from inputs <em>x</em> to labels <script type="math/tex">y=f(x)</script>. We assume training examples <script type="math/tex">(x^{(i)},y^{(i)})</script> for <script type="math/tex">i=1...n</script>.</li><li>In the noisy channel approach, we use the training example to estimate models <script type="math/tex">p(y)</script> and <script type="math/tex">p(x|y)</script>. These models define a joint(generative) model: <script type="math/tex">p(x,y)=p(y)p(x|y)</script></li><li>Given a new test example <em>x</em>, we predict the label <script type="math/tex">f(x)=\arg\max_{y\in\mathcal{Y}}p(y)p(x|y)</script>. Finding the output <em>f(x)</em> for an input <em>x</em> is often referred to as the <em>decoding problem</em>.</li></ul><p>Assume a finite set of words <script type="math/tex">\mathcal{V}</script>, and a finite set of tags <script type="math/tex">\mathcal{K}</script>. Define <script type="math/tex">\mathcal{S}</script> to be the set of all sequence/tag-sequence pairs <script type="math/tex"><x_1...x_n,y_1...y_n></script> such that <script type="math/tex">n\ge0,x_i\in\mathcal{V}for\ i=1...n,and\ y_i\in\mathcal{K}for\ i=1...n</script>. A generative tagging model is then a function p such that:</p><ol><li>For any <script type="math/tex"><x_1...x_n,y_1...y_n>\in\mathcal{S},p(x_1...x_n,y_1...y_n)\ge0</script></li><li>In addition, <script type="math/tex">\sum_{<x_1...x_n,y_1...y_n>\in\mathcal{S}}p(x_1...x_n,y_1...y_n)=1</script></li></ol><p>Hense <script type="math/tex">p(x_1...x_n,y_1...y_n)</script> is a probability distribution over pairs of sequences(i.e., a probability distribution over the set <script type="math/tex">\mathcal{S}</script>).<br>Given a generative tagging model, the function from sentences <script type="math/tex">x_1...x_n</script> to tag sequence <script type="math/tex">y_1...y_n</script> is defined as</p><script type="math/tex; mode=display">f(x_1...x_n)=\arg\max_{y_1...y_n}p(x_1...x_n,y_1...y_n)</script><p>where the <em>arg max</em> is taken over all sequences <script type="math/tex">y_1...y_n</script> such that <script type="math/tex">y_i\in\mathcal{K}for\ i\in\{1...n\}</script>. Thus for any input <script type="math/tex">x_1...x_n</script>, we take the highest probability tag sequence as the output from the model.</p><h2 id="Hidden-Markov-Models"><a href="#Hidden-Markov-Models" class="headerlink" title="Hidden Markov Models"></a>Hidden Markov Models</h2><h3 id="Trigram-HMMs"><a href="#Trigram-HMMs" class="headerlink" title="Trigram HMMs"></a>Trigram HMMs</h3><p>A trigram HMM consists of a finite set <script type="math/tex">\mathcal{V}</script> of possible words, and a finite set <script type="math/tex">\mathcal{K}</script> of possible tags, together with the following parameters:<br>· A parameter <script type="math/tex">q(s|u,v)</script> for any trigram <script type="math/tex">(u,v,s)</script> such that <script type="math/tex">s\in\mathcal{K]\cup\{STOP\}</script>, and <script type="math/tex">u,v\in\mathcal{K}\cup\{*\}</script>. The value for <script type="math/tex">q(s|u,v)</script> can be interpreted as the probability of seeing the tag <em>s</em> immediately after the bigram of tags <script type="math/tex">(u,v)</script>.<br>· A parameter <script type="math/tex">e(x|s)</script> for any <script type="math/tex">x\in\mathcal{V},s\in\mathcal{K}</script>. The value for <script type="math/tex">e(x|s)</script> can be interpreted as the probability of seeing observation <em>x</em> paired with state <em>s</em>.<br>Define <script type="math/tex">\mathcal{S}</script> to be the set of all sequence/tag-sequence pairs <script type="math/tex"><x_1...x_n,y_1...y_{n+1}></script> such that <script type="math/tex">n\ge0,x_i\in\mathcal{V}for\ i=1...n,y_i\in\mathcal{K}for\ i=1...n,and\ y_{n+1}=STOP</script>.<br>We then define the probability for any <script type="math/tex"><x_1...x_n,y_1...y_{n+1}\in\mathcal{S}</script> as</p><script type="math/tex; mode=display">p(x_1...x_n,y_1...y_{n+1})=\prod_{i=1}^{n+1}q(y_i|y_{i-2},y_{i-1})\prod_{i=1}^ne(x_i|y_i)</script><p>where we have assumed that <script type="math/tex">y_0=y_{-1}=*</script>.</p><p>E.g.<br>If we have n = 3, <script type="math/tex">x_1x_2x_3</script> equal to the sentence <em>the dog laughs</em>, and <script type="math/tex">y_1y_2y_3y_4</script> equal to the tag sequence <code>D N V STOP</code>, then</p><script type="math/tex; mode=display">p(x_1x_2x_3y_1y_2y_3y_4)=q(D|*,*)\times q(N|*,D)\times q(V|D,N)\times q(STOP|N,V)\\\times e(the|D)\times e(dog|N)\times e(laughs|V)</script><p>The quantity <script type="math/tex">q(D|*,*)\times q(N|*,D)\times q(V|D,N)\times q(STOP|N,V)</script> is the prior probability of seeing the tag sequence <code>D N V STOP</code>, where we have used a second-order Markov model(a trigram model).<br>The quantity <script type="math/tex">e(the|D)\times e(dog|N)\times e(laughs|V)</script> can be interpreted as the conditional probability <script type="math/tex">p(the\ dog\ laughs|D\ N\ V\ STOP)</script>: that is, the conditional probability <script type="math/tex">p(x|y)</script> where <em>x</em> is the sentence <em>the dog laughs</em>, and <em>y</em> is the sequence <code>D N V STOP</code>.</p><p>Consider a pair of sentences of random variables <script type="math/tex">X_1...X_n</script> and <script type="math/tex">Y_1...Y_n</script>, where <em>n</em> is the length of the sequences. To model the joint probability</p><script type="math/tex; mode=display">P(X_1=x_1...X_n=x_n,Y_1=y_1...Y_n=y_n</script><p>for any observation sequence <script type="math/tex">x_1...x_n</script> paired with a state sequence <script type="math/tex">y_1...y_n</script>, where each <script type="math/tex">x_i</script> is a member of <script type="math/tex">\mathcal{V}</script>, and each <script type="math/tex">y_i</script> is a member of <script type="math/tex">\mathcal{K}</script>.<br>Define one additional variable <script type="math/tex">Y_{n+1}</script> which always takes the value STOP, just as we did in variable-Markov sequences.<br>The key idea in hidden Markov models is the following definition:</p><script type="math/tex; mode=display">P(X_1=x_1...X_n=x_n,Y_1=y_1...Y_{n+1}=y_{n+1})=\prod_{i=1}^{n+1}P(Y_i=y_i|Y_{i-2}=y_{i-2},Y_{i-1}=y_{i-1})\prod_{i=1}^nP(X_i=x_i|Y_i=y_i)</script><p>We have assumed that for any <em>i</em>, for any values of <script type="math/tex">y_{i-2},y_{i-1},y_i</script>,</p><script type="math/tex; mode=display">P(Y_i=y_i|Y_{i-2}=y_{i-2},Y_{i-1}=y_{i-1})=q(y_i|y_{i-2},y_{i-1})</script><p>and that for any value of <em>i</em>, for any values of <script type="math/tex">x_i</script> and <script type="math/tex">y_i</script>,</p><script type="math/tex; mode=display">P(X_i=x_i|Y_i=y_i)=e(x_i|y_i)</script><p><strong>The derivation of hidden Markov models:</strong></p><script type="math/tex; mode=display">P(X_1=x_1...X_n=x_n,Y_1=y_1...Y_{n+1}=y_{n+1})=P(Y_1=y_1...Y_{n+1}=y_{n+1})\times P(X_1=x_1...X_n=x_n|Y_1=y_1...Y_{n+1}=y_{n+1})</script><p>Just by the chain rule of probabilities. The joint probability is decomposed into two terms: first, the probability of choosing tag sequence <script type="math/tex">y_1...y_{n+1}</script>; second, the probability of choosing the word sequence <script type="math/tex">x_1...x_n</script>, conditioned on the choice of tag sequence.</p><script type="math/tex; mode=display">P(Y_1=y_1...Y_{n+1}=y_{n+1})=\prod_{i=1}^{n+1}P(Y_i=y_i|Y_{i-2}=y_{i-2},Y_{i-1}=y_{i-1})</script><p>Consider the first item: Assume the sequence is a second-order Markov sequence.</p><script type="math/tex; mode=display">P(X_1=x_1...X_n=x_n|Y_1=y_1...Y_{n+1}=y_{n+1})=\prod_{i=1}^nP(X_i=x_i|X_1=x_1...X_{i-1}=x_{i-1},Y_1=y_1...Y_{n+1})=y_{n+1}=\prod_{i=1}^nP(X_i=x_i|Y_i=y_i)</script><p>Consider the second item: Assume that the value for the random variable <script type="math/tex">X_i</script> depends only on the value <script type="math/tex">Y_i</script>.</p><p><strong>Stochastic process</strong></p><ol><li>Initialize <script type="math/tex">i=1</script> and <script type="math/tex">y_0=y_{-1}=*</script>.</li><li>Generate <script type="math/tex">y_i</script> from the distribution <script type="math/tex">q(y_i|y_{i-1},y_{i-1})</script></li><li>If <script type="math/tex">y_i=STOP</script> then return <script type="math/tex">y_1...,x_1...x_{i-1}</script>. Otherwise, generate <script type="math/tex">x_i</script> from the distribution <script type="math/tex">e(x_i|y_i)</script>, set <script type="math/tex">i=i+1</script>, and return to step 2.</li></ol><h3 id="Estimating"><a href="#Estimating" class="headerlink" title="Estimating"></a>Estimating</h3><p>Define <script type="math/tex">c(u,v,s)</script> to be the number of times the sequence of three states <script type="math/tex">(u,v,s)</script> is seen in training data. Similarly, define <script type="math/tex">c(u,v)</script> to be the number of times the tag bigram<script type="math/tex">(u,v)</script> is seen and <script type="math/tex">c(s)</script> to be the number of times that the state <script type="math/tex">s</script> is seen in the corpus.<br>Define <script type="math/tex">c(s\leadsto x)</script> to be the number of times state <script type="math/tex">s</script> is seen paired with observation <script type="math/tex">x</script> in the corpus.<br>The <em>maximum-likelihood</em> estimates are</p><script type="math/tex; mode=display">q(s|u,v)=\frac{c(u,v,s)}{c(u,v)}</script><script type="math/tex; mode=display">e(x|s)=\frac{c(s\leadsto x)}{c(s)}</script><p>In some cases it is useful to smooth estimates of <script type="math/tex">q(s|u,v)</script>, using the techniques of smoothing:</p><script type="math/tex; mode=display">q(s|u,v)=\lambda_1\times q_{ML}(s|u,v)+\lambda_2\times q_{ML}(s|v)+\lambda_3\times q_ML(s)</script><h3 id="Dealing-with-Low-Frequency-Words"><a href="#Dealing-with-Low-Frequency-Words" class="headerlink" title="Dealing with Low-Frequency Words"></a>Dealing with Low-Frequency Words</h3><p>A common method is as follows:</p><p>· Step 1: Split vocabulary into two sets<br><strong>Frequent words</strong> : words occurring ≥ 5 times in training<br><strong>Low-frequency words</strong> : all other words<br>· Step 2: Map low frequency words into a small, finite set, depending on prefixes, suffixes etc.</p><p><img src="/images/NLP/lowFrequency.jpg" alt></p><h3 id="The-Viterbi-Algorithm"><a href="#The-Viterbi-Algorithm" class="headerlink" title="The Viterbi Algorithm"></a>The Viterbi Algorithm</h3><p>Problem: for an input <script type="math/tex">x_1...x_n</script>,  find <script type="math/tex">\arg\max_{y_1...y_{n+1}}p(x_1...x_n,y_1...y_{n+1})</script> where the arg max is taken over all sequences <script type="math/tex">y_1...y_{n+1}</script> such that <script type="math/tex">y_i\in\mathcal{K}</script> for <script type="math/tex">i=1...n</script>, and <script type="math/tex">y_{n+1}=STOP</script>.<br>We assume that <em>p</em> again takes the form <script type="math/tex">p(x_1...x_n,y_1...y_{n+1})=\prod_{i=1}^{n+1}q(y_i|y_{i-2},y_{i-1})\prod_{i=1}^ne(x_i|y_i)</script></p><p>The naive brute force method would be hopelessly inefficient. Instead, we can efficiently find the highest probability tag sequence using a dynamic programming algorithm that is often called <em>the Viterbi algorithm</em>. The input to the algorithm is a sentence <script type="math/tex">x_1...x_n</script>.</p><h1 id="Week-Three"><a href="#Week-Three" class="headerlink" title="Week Three"></a>Week Three</h1><h2 id="Parsing-1"><a href="#Parsing-1" class="headerlink" title="Parsing"></a>Parsing</h2><p>Input: a sentence<br>Output: a parse tree<br><img src="/images/NLP/parsingSyntactic.jpg" alt></p><h3 id="Parse-tree"><a href="#Parse-tree" class="headerlink" title="Parse tree"></a>Parse tree</h3><p>A <em>parse tree</em> is a tree structure with the words in the sentence at the leaves of the tree, and the tree has labels on the internal nodes.</p><p>Syntactic Formalisms: minimalism, lexical functional grammar(LFG), head-driven phrase-structure grammar(HPSG), tree adjoining grammars(TAG), categorial grammars, etc.<br>The lecture focuses on context-free grammars, which are fundamental and form the basis for all modern form atoms.</p><p>Data: Penn WSJ Treebank: 50000 sentences with associated trees(annotated by hand).</p><h4 id="The-Information-Conveyed-by-Parse-Tree"><a href="#The-Information-Conveyed-by-Parse-Tree" class="headerlink" title="The Information Conveyed by Parse Tree"></a>The Information Conveyed by Parse Tree</h4><h5 id="Part-of-speech-for-each-word"><a href="#Part-of-speech-for-each-word" class="headerlink" title="Part of speech for each word"></a>Part of speech for each word</h5><p>It plays the same role as POS tagging. For each word in the sentence, just put a tag for the word.<br>N = noun, V = verb, DT = determiner<br><img src="/images/NLP/parseInfo1.jpg" alt></p><h5 id="Phrases"><a href="#Phrases" class="headerlink" title="Phrases"></a>Phrases</h5><p>Phrases, or what are often called constituents, are compositions of words.<br>NP = noun phrase, VP = verb phrase<br><img src="/images/NLP/parseInfo2.jpg" alt></p><h5 id="Useful-Relationships"><a href="#Useful-Relationships" class="headerlink" title="Useful Relationships"></a>Useful Relationships</h5><p>Parse trees encode important grammatical relationships within a sentence.<br>Some templates: <em>subject+verb</em>, <em>verb+DIRECT OBJECT</em>.<br><img src="/images/NLP/parseInfo3.jpg" alt></p><h2 id="Context-Free-Grammar"><a href="#Context-Free-Grammar" class="headerlink" title="Context-Free Grammar"></a>Context-Free Grammar</h2><p>A <strong>context free grammar</strong> <script type="math/tex">G=(N,\Sigma,R,S)</script> where:</p><ul><li><script type="math/tex">N</script> is a set of non-terminal symbols</li><li><script type="math/tex">\Sigma</script> is a set of terminal symbols (the set of words in the dictionary)</li><li><script type="math/tex; mode=display">R$$ is a set of rules of the form $$X\toY_1Y_2...Y_n$$ for $$n\ge0,X\inN,Y_i\in(N\cup\Sigma)</script></li><li><script type="math/tex">S\inN</script> is a distinguished start symbol</li></ul><p><strong>Properties</strong></p><ul><li>A CFG defines a set of possible derivations</li><li>A string <script type="math/tex">s\in\Sigma^*</script> is in the <em>language</em> defined by the CFG if there is at least one derivation that yields <em>s</em></li><li>Each string in the language generated by the CFG may have more than one derivation(“ambiguity”)</li></ul><h3 id="Left-Most-Derivations"><a href="#Left-Most-Derivations" class="headerlink" title="Left-Most Derivations"></a>Left-Most Derivations</h3><p>A left-most derivation is a sequence of strings <script type="math/tex">s_1...s_n</script>, where </p><ul><li><script type="math/tex">s_1=S</script>, the start symbol</li><li><script type="math/tex">s_n\in\Sigma^*</script>, i.e. <script type="math/tex">s_n</script> is made up of terminal symbols only (<script type="math/tex">\Sigma^*</script> denotes the set of all possible strings made up of sequences of words taken from <script type="math/tex">\Sigma</script>)</li><li>Each <script type="math/tex">s_i</script> for <script type="math/tex">i=2...n</script> is derived from <script type="math/tex">s_{i-1}</script> by picking the left-most non-terminal <em>X</em> in <script type="math/tex">s_{i-1}</script> and replacing it by some <script type="math/tex">\beta</script> where <script type="math/tex">X\to\beta</script> is a rule in <em>R</em></li></ul><p>E.g. [S]→[NP VP]→[D N VP]→[the N VP]→[the man VP]→[the man Vi]→[the man sleeps]</p><h2 id="A-brief-sketch-of-the-syntax-of-English"><a href="#A-brief-sketch-of-the-syntax-of-English" class="headerlink" title="A brief sketch of the syntax of English"></a>A brief sketch of the syntax of English</h2><h3 id="Tags-1"><a href="#Tags-1" class="headerlink" title="Tags"></a>Tags</h3><p>· Nouns<br>NN: singular noun (e.g., man, dog, park)<br>NNS: plural noun (e.g., books, pens)<br>NNP: proper noun (e.g., Bob, IBM)<br>NP: noun phrase (e.g., the girl)</p><p>· Determiners<br>DT: determiner (e.g., the, a, some, every)</p><p>· Adjectives<br>JJ: adjective (e.g., good, quick, big)</p><p>· Prepositions<br>IN: preposition (e.g., of, in, out, beside, as)<br>PP: prepositional phrase</p><p>· Basic Verb Types<br>Vi: intransitive verb (e.g., sleep, walk)<br>Vt: transitive verb (e.g., like, see)<br>Vd: ditransitive verb (e.g., give)<br>VP: verb phrase (e.g., sleep in the car, go to school)</p><p>· New Verb Types<br>V[5]: clause directly followed by the verb (e.g., say, report)<br>V[6]: clause followed by the verb with one objective (e.g., tell, inform)<br>V[7]: clause followed by the verb with two objectives (e.g., bet)</p><p>· Complementizers<br>COMP: complementizer (e.g., that)</p><p>· Coordinators<br>CC: coordinator (e.g., and, or, but)</p><p>· Sentences<br>S: sentence (e.g., the dog sleeps)</p><h3 id="Rules"><a href="#Rules" class="headerlink" title="Rules"></a>Rules</h3><blockquote><p>N(bar) =&gt; NN<br>N(bar) =&gt; NN N(bar)<br>N(bar) =&gt; JJ N(bar)<br>N(bar) =&gt; N(bar) N(bar)<br>NP =&gt; DT N(bar)<br>PP =&gt; IN NP<br>N(bar) =&gt; N(bar) PP<br>VP =&gt; Vi<br>VP =&gt; Vt NP<br>VP =&gt; Vd NP NP<br>VP =&gt; VP PP<br>S =&gt; NP VP<br>SBAR =&gt; COMP S<br>VP =&gt; V[5] SBAR<br>VP =&gt; V[6] NP SBAR<br>VP =&gt; V[7] NP NP SBAR<br>NP =&gt; NP CC NP<br>N(bar) =&gt; N(bar) CC N(bar)<br>VP =&gt; VP CC VP<br>S =&gt; S CC S<br>SBAR =&gt; SBAR CC SBAR</p></blockquote><p>What is discussed in the lecture is only a small part of the syntax of English.<br>There’re some problems:<br><strong>Agreement</strong><br><em>The dogs laugh</em> vs. <em>The dog laughs</em><br><strong>Wh-movement</strong><br><em>The dog [that the cat] liked…</em><br><strong>Active vs. passive</strong><br><em>The dog saw the cat</em> vs. <em>The cat was seen by the dog</em></p><h1 id="Week-Four"><a href="#Week-Four" class="headerlink" title="Week Four"></a>Week Four</h1><h2 id="Probabilistic-Context-free-Grammar"><a href="#Probabilistic-Context-free-Grammar" class="headerlink" title="Probabilistic Context-free Grammar"></a>Probabilistic Context-free Grammar</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>A probabilistic context-free grammar consists of:</p><ul><li>A context-free grammar <script type="math/tex">G=(N,\Sigma,S,R)</script>.</li><li>A parameter <script type="math/tex">q(\alpha\to\beta)</script> for each rule <script type="math/tex">\alpha\to\beta\in R</script>. The parameter <script type="math/tex">q(\alpha\to\beta)</script> can be interpreted as the conditional probability of choosing rule <script type="math/tex">\alpha\to\beta</script> in a left-most derivation, given that the non-terminal being expanded is α.</li></ul><p>Constraint: </p><script type="math/tex; mode=display">\sum_{\alpha\to\beta\in R:\alpha=X}q(\alpha\to\beta)=1,X\in N</script><script type="math/tex; mode=display">q(\alpha\to\beta)\ge0,\alpha\to\beta\in R</script><p>Given a parse-tree <script type="math/tex">t\in\mathcal{T}_G</script> containing rules <script type="math/tex">\alpha_1\to\beta_1,\alpha_2\to\beta_2,...,\alpha_n\to\beta_n</script>, the probability of t under the PCFG is </p><script type="math/tex; mode=display">p(t)=\prod_{i=1}^nq(\alpha_i\to\beta_i)</script><h3 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h3><p>Assigns a probability to each left-most derivation, or parse-tree, allowed by the underlying CFG<br>Say we have a sentence <em>s</em>, set of derivations for that sentence is <script type="math/tex">\mathcal{T}(s)</script>. Then a PCFG assigns a probability <script type="math/tex">p(t)</script> to each member of <script type="math/tex">\mathcal{T}(s)</script>. i.e., we now have a ranking in order of probability.<br>The most likely parse tree for a sentence <em>s</em> is </p><script type="math/tex; mode=display">\arg\max_{t\in\mathcal{T}(s)}p(t)</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Week-One&quot;&gt;&lt;a href=&quot;#Week-One&quot; class=&quot;headerlink&quot; title=&quot;Week One&quot;&gt;&lt;/a&gt;Week One&lt;/h1&gt;&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; cla
      
    
    </summary>
    
      <category term="Natural Language Processing" scheme="http://haelchan.me/categories/Natural-Language-Processing/"/>
    
    
      <category term="learning note" scheme="http://haelchan.me/tags/learning-note/"/>
    
  </entry>
  
  <entry>
    <title>面向对象程序设计课堂笔记</title>
    <link href="http://haelchan.me/2018/03/14/oop-note/"/>
    <id>http://haelchan.me/2018/03/14/oop-note/</id>
    <published>2018-03-14T08:50:12.000Z</published>
    <updated>2018-10-18T15:06:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Lecture-1-绪论"><a href="#Lecture-1-绪论" class="headerlink" title="Lecture 1 绪论"></a>Lecture 1 绪论</h2><h3 id="序"><a href="#序" class="headerlink" title="序"></a>序</h3><p>程序设计范型是指设计程序的规范、模型和风格，它是一类程序设计语言的基础。</p><ul><li><strong>面向过程</strong>程序设计范型：程序=过程+调用 或 程序=算法+数据结构</li><li><strong>函数式</strong>程序设计范型：程序被看作“描述输入与输出之间关系”的数学函数。如LISP</li><li><strong>面向对象</strong>程序设计是一种新型的程序设计范型。这种范型的主要特征是：对象=（算法+数据结构）程序=对象+消息</li></ul><p>面向对象程序的主要结构特点：<br>一、程序一般由<strong>类的定义</strong>和<strong>类的使用</strong>两部分组成，在主程序中定义各对象并规定它们之间传递消息的规律。<br>二、程序中的一切操作都是通过<strong>向对象发送消息</strong>来实现的，对象接收到消息后，启动有关方法完成相应的操作。</p><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><ul><li>对象 Object</li><li>类 Class</li><li>消息 Message</li><li>方法 Method</li></ul><h4 id="对象"><a href="#对象" class="headerlink" title="对象"></a>对象</h4><p>在现实世界中，任何事物都是对象。可以使有形的具体存在的事物，也可以是无形的抽象的事件。<br>对象一般可以表示为：<strong>属性+操作</strong></p><p><strong>名字</strong>：用于区别不同的实体<br><strong>属性/状态</strong>：属性用于描述不同实体的特征状态由这个对象的属性和这些属性的当前值决定。<br><strong>操作</strong>：用于描述不同实体可具有的行为是对象提供给用户的一种服务，也叫行为或方法。<br>· 对象的操作可以分为两类，一类是<strong>自身所承受</strong>的操作(private/protected)，一类是<strong>施加于其他对象</strong>的操作(public)。</p><p>方法(Method)——就是对象所能执行的操作，即服务。方法描述了对象执行操作的算法，响应消息的方法。在C++中称为<strong>成员函数</strong>。<br>属性(Attribute)——就是类中所定义的数据，它是对客观世界实体所具有性质的抽象。C++中称为<strong>数据成员</strong>。</p><p>在面向对象程序设计中，对象是描述其属性的<strong>数据</strong>及对这些数据施加的一组<strong>操作</strong>封装在一起构成的<strong>统一体</strong>。<br>对象可以认为是：<strong>数据+方法（操作）</strong></p><h4 id="类"><a href="#类" class="headerlink" title="类"></a>类</h4><p>在现实世界中，<strong>类</strong>是一组具有相同<strong>属性</strong>和<strong>行为</strong>的对象的抽象。<br><strong>类</strong>和<strong>对象</strong>之间的关系式<strong>抽象</strong>和<strong>具体</strong>的关系。类是多个对象进行综合抽象的结果，一个对象是类的一个实例。<br>在面向对象程序设计中，类就是<strong>具有相同数据和相同操作的一组对象的集合</strong>。是对具有相同数据结构和相同操作的一类对象的描述。<br>在面向对象程序设计中，总是先声明类，再由类生成其对象。</p><p>注意不能把一组函数组合在一起构成类。即类不是函数的集合。</p><h4 id="消息"><a href="#消息" class="headerlink" title="消息"></a>消息</h4><p>面向对象设计技术必须提供一种机制允许一个对象与另一个对象的交互，这种机制叫<strong>消息传递</strong>。<br>在面向对象程序设计中，一个对象向另一个对象发出的<strong>请求</strong>被称为<strong>消息</strong>。当对象收到消息时，就调用有关的方法，执行相应的操作。<strong>消息是一个对象要求另一个对象执行某个操作的规格说明</strong>，通过消息传递才能完成对象之间的相互请求或相互协作。<br>消息具有三个性质：<br>(1).同一个对象可以接收不同形式的多个消息，作出不同的响应<br>(2).相同形式的消息可以传递给不同的对象，所作出的响应可以是不同的。<br>(3).对消息的响应并不是必需的，对象可以响应消息，也可以不响应。<br>分为两类：<strong>公有消息</strong>（其他对象发出），<strong>私有消息</strong>（向自己发出）。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>方法就是对象所能执行的操作。方法包括界面和方法体两部分。<br>方法的<strong>界面</strong>（接口）就是消息的模式，它给出了方法调用的协议；<br><strong>方法体</strong>则是实现某种操作的一系列计算步骤，就是一段程序<br>在C++语言中方法是通过函数来实现的，称为<strong>成员函数</strong><br><strong>消息和方法的关系</strong>是：对象根据接收到的消息，调用相应的方法；反过来，有了方法，对象才能响应相应的消息。</p><h3 id="面向对象程序设计的基本特征"><a href="#面向对象程序设计的基本特征" class="headerlink" title="面向对象程序设计的基本特征"></a>面向对象程序设计的基本特征</h3><ul><li>抽象 Abstraction</li><li>封装 Encapsulation</li><li>继承 Inheritance</li><li>多态 Polymorphism</li></ul><h4 id="抽象"><a href="#抽象" class="headerlink" title="抽象"></a>抽象</h4><p>抽象是通过特定的实例（对象）抽取<strong>共同性质</strong>以后形成概念的过程。抽象是对系统的简化描述和规范说明，他强调了系统中的一部分细节和特性，而<strong>忽略了其他部分</strong>。<br>抽象包括两个方面，<strong>数据抽象</strong>和<strong>代码抽象</strong>（或称行为抽象）。前者描述某类对象的属性和状况，也就是此类对象区别于彼类对象的特征物理量；后者描述了某类对象的共同行为特征或具有的共同操作。<br>在面向对象的程序设计方法中，对一个具体问题的抽象分析结果，是通过<strong>类</strong>来<strong>描述和实现</strong>的。</p><h4 id="封装"><a href="#封装" class="headerlink" title="封装"></a>封装</h4><p>在面向对象程序设计中，封装是指<strong>把数据和实现操作的代码</strong>集中起来放在对象内部，并尽可能隐藏对象的内部细节。<br>封装应该具有如下几个条件：<br>（1）对象具有一个清晰的边界，对象的私有数据和实现操作的代码被封装在该边界内。<br>（2）具有一个描述对象与其他对象如何相互作用的接口，该接口必须说明消息如何传递的使用方法。<br>（3）对象内部的代码和数据应受到保护，其他对象不能直接修改。</p><h4 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h4><p>继承是在一个已经建立的类的基础上再接着声明一个新类的扩展机制，原先已经建立的类称为<strong>基类</strong>，在基类之下扩展的类称为<strong>派生类</strong>，派生类又可以向下充当继续扩展的基类，因此构成层层派生的一个动态扩展过程。<br>派生类享有基类的数据结构和算法，而本身又具有增加的行为和特性，因此继承的机制促进了程序代码的可重用性。<br>一个基类可以有多个派生类，一个派生类反过来可以具有多个基类，形成复杂的继承树<strong>层次体系</strong>。</p><p>基类与派生类之间本质的关系：基类是一个简单的类，描述相对简单的事物，派生类是一个复杂些的类，处理相对复杂的现象。</p><p>继承的作用：<br>避免公用代码的重复开发，减少代码和数据冗余。<br>通过增强一致性来减少模块间的接口。<br>继承分为单继承和多继承。</p><h4 id="多态"><a href="#多态" class="headerlink" title="多态"></a>多态</h4><p>多态性是指<strong>不同的对象收到相同的消息时产生多种不同的行为方式</strong>。<br>C++支持两种多态性：编译时的多态性（重载）和运行时的多态性（虚函数）。</p><p><strong>OOP的主要优点</strong><br>（1）可提高程序的重用性<br>（2）可控制程序的复杂性<br>（3）可改善程序的可维护性<br>（4）能够更好地支持大型程序设计<br>（5）增强了计算机处理信息的范围<br>（6）能很好地适应新的硬件环境</p><p><strong>C++的优点</strong><br>C++继承了C的优点，并有自己的特点，主要有：<br>（1）全面兼容C，C的许多代码不经修改就可以为C++所用，用C编写的库函数和实用软件可以用于C++。<br>（2）用C++编写的程序可读性更好，代码结构更为合理，可直接在程序中映射问题空间结构。<br>（3）生成代码的质量高，运行效率高。<br>（4）从开发时间、费用到形成软件的可重用性、可扩充性、可维护性和可靠性等方面有了很大提高，使得大中型的程序开发项目变得容易得多。<br>（5）支持面向对象的机制，可方便地构造出模拟现实问题的实体和操作。</p><h3 id="C-对C的补充"><a href="#C-对C的补充" class="headerlink" title="C++对C的补充"></a>C++对C的补充</h3><h4 id="注释与续行"><a href="#注释与续行" class="headerlink" title="注释与续行"></a>注释与续行</h4><p>注释符：<code>/* */</code>或<code>//</code><br>续行符：<code>\</code>。当一个语句太长时可以用该符号分段写在几行中<br>note: 其实不加续航符直接换行也可以0.0<br>E.g.</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; <span class="string">"hello "</span></span><br><span class="line">         &lt;&lt; <span class="string">"world"</span></span><br><span class="line">         &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br></pre></td></tr></table></figure><p>This program will print <code>hello world</code> in a line.</p><h4 id="输入输出流"><a href="#输入输出流" class="headerlink" title="输入输出流"></a>输入输出流</h4><p>C: <code>scanf</code>和<code>printf</code><br>C++: <code>cin&gt;&gt;</code>和<code>cout&lt;&lt;</code>（用C的也可以，但是不推荐……）<br>cout和cin分别是C++的标准输出流和输入流。C++支持重定向，但一般cout指的是屏幕，cin指的是键盘。操作符<code>&lt;&lt;</code>和<code>&gt;&gt;</code>除了具有C语言中定义的左移和右移的功能外，在这里符号<code>&lt;&lt;</code>是把右方的参数写到标准输出流cout中；相反，符号<code>&gt;&gt;</code>则是将标准输入流的数据赋给右方的变量。<br>cin和<code>&gt;&gt;</code>，cout和<code>&lt;&lt;</code>配套使用<br>使用cout和cin时，也可以对输入和输出的格式进行控制，比如可用不同的进制方式显示数据，只要设置转换基数的操作符dec、hex和oct即可。</p><h4 id="灵活的变量说明"><a href="#灵活的变量说明" class="headerlink" title="灵活的变量说明"></a>灵活的变量说明</h4><p>定义变量的位置<br>在程序中的不同位置采用不同的变量定义方式，决定了该变量具有不同的特点。变量的定义一般可由以下三种位置：<br>（1）函数体内部<br>在函数体内部定义的变量称为局部变量。<br>（2）形式参数<br>当定义一个有参函数时，函数名后面括号内的变量，统称为形式参数。<br>（3）全局变量：在所有函数体外部定义的变量，其作用范围是整个程序，并在整个程序运行期间有效。</p><p>在C语言中，全局变量声明必须在任何函数之前，局部变量必须集中在可执行语句之前。<br>C++中的变量声明非常灵活。它允许变量声明与可执行语句交替执行，随时声明。<code>for (int i = 0; i &lt; 10; i++)</code></p><h4 id="结构、联合和枚举名"><a href="#结构、联合和枚举名" class="headerlink" title="结构、联合和枚举名"></a>结构、联合和枚举名</h4><p>在C++中，结构名、联合名、枚举名都是类型名。在定义变量时，不必在结构名、联合名或枚举名前冠以struct、union或enum。<br>如：定义枚举类型<code>boole</code>: <code>enum boole{FALSE, TRUE};</code><br>在C语言中定义变量需写成<code>enum boole done;</code>，但在C++中，可以说明为<code>boole done;</code>。</p><h4 id="函数原型"><a href="#函数原型" class="headerlink" title="函数原型"></a>函数原型</h4><p>C语言<strong>建议</strong>编程者为程序中的每一个函数建立圆形，而C++<strong>要求</strong>为每一个函数建立原型，以说明函数的名称、参数类型与个数，以及函数返回值的类型。其主要目的是让C++编译程序进行类型检查，即形参与实参的类型匹配检查，以及返回值是否与原型相符，以维护程序的正确性。<br>在程序中，要求一个函数的原型出现在该函数的调用语句之前。说明：<br>（1）函数原型的参数表中<strong>可不包含参数的名字</strong>，而只包含它们的类型。例如<code>long Area(int, int);</code><br>（2）函数定义由函数首部和函数体构成。函数首部和函数原型基本一样，但函数首部中的参数必须给出名字而且不包含结尾的分号。<br>（3）C++的参数说明必须放在函数说明后的括号内，不可将函数参数说明放在函数首部和函数体之间。这种方法只在C中成立。<br>（4）主函数不必进行原型说明，因为它被看成自动说明原型的函数。<br>（5）原型说明中没有指定返回类型的函数（包括主函数main），C++默认该函数的返回类型是int。<br>（6）如果一个函数没有返回值，则必须在函数原型中注明返回类型为void，主函数类似处理。<br>（7）如果函数原型中未注明参数，C++假定该函数的参数表为空(void)。</p><h4 id="const修饰符"><a href="#const修饰符" class="headerlink" title="const修饰符"></a>const修饰符</h4><p>C语言中习惯用<code>#define</code>定义常量，C++利用const定义正规常数<br>一般格式 <code>const 数据类型标识符 常数名 = 常量值</code><br>采用这种方式定义的常量是类型化的，它有地址，可以用指针指向这个值，但不能修改它。<br>const必须放在被修饰类型符和类型名前面。<br>数据类型是可选项，用来指定常数值的数据类型，如果省略了数据类型，那么默认是int。<br>const的作用于<code>#define</code>相似，但它消除了<code>#define</code>的不安全性。</p><p>const可以与指针一起使用。<br>指向常量的指针、常指针和指向常量的常指针。<br>1）<strong>指向常量的指针</strong>是指：一个指向常量的指针变量。<br>2）<strong>常指针</strong>是指：把指针本身，而不是它指向的对象声明为常量。<br>3）<strong>指向常量的常指针</strong>是指：这个指针本身不能改变，它所指向的值也不能改变。要声明一个指向常量的常指针，二者都要声明为const。</p><p>说明：<br>（1）如果用const定义的是一个整型变量，关键词int<strong>可以省略</strong>。<br>（2）常量一旦被建立，在程序的任何地方都<strong>不能再更改</strong>。<br>（3）与<code>#define</code>定义的常量有所不同，const定义的常量可以<strong>有自己的数据类型</strong>，这样C++的编译程序可以进行更加严格的类型检查，具有良好的编译时的检测性。<br>（4）函数参数也可以用const说明，用于保证实参在该函数内部不被改动。</p><h4 id="void型指针"><a href="#void型指针" class="headerlink" title="void型指针"></a>void型指针</h4><p>void通常表示无值，但将void作为指针的类型时，它却表示<strong>不确定的类型</strong>。这种void型指针是一种通用型指针，也就是说任何类型的指针值都可以赋给void类型的指针变量。<br>void型指针可以接受任何类型的指针的赋值，但对已获值的void型指针，对它在进行处理，如输出或传递指针值时，则必须进行<strong>强制类型转换</strong>，否则会出错。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">void</span> *pc;</span><br><span class="line">    <span class="keyword">int</span> i = <span class="number">456</span>;</span><br><span class="line">    <span class="keyword">char</span> c = <span class="string">'a'</span>;</span><br><span class="line">    pc = &amp;i;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; *(<span class="keyword">int</span> *)pc &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    pc = &amp;c;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; *(<span class="keyword">char</span> *)pc &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="内联函数"><a href="#内联函数" class="headerlink" title="内联函数"></a>内联函数</h4><p>调用函数时系统要付出一定的开销，用于信息入栈出栈和参数传递等。<br>C++引进了内联函数(inline function)的概念。在进行程序的编译时，编译器将内联函数的目标代码作拷贝并将其插入到调用内联函数的地方。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">double</span> <span class="title">circle</span><span class="params">(<span class="keyword">double</span> r)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">3.1416</span> * r * r;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; ++i) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; <span class="string">"r = "</span> &lt;&lt; i &lt;&lt; <span class="string">" area = "</span> &lt;&lt; circle(i) &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>说明：<br>（1）内联函数在第一次被调用前必须进行声明或定义。否则编译器无法知道应该插入什么代码<br>（2）C++的内联函数具有与C中的宏定义<code>#define</code>相同的作用和类似机理，但消除了<code>#define</code>的不安全性。<br>（3）内联函数体内一般不能有循环语句和开关语句。<br>（4）后面类结构中所有在类说明体内定义的函数都是内联函数。<br>（5）通常较短的函数才定义为内联函数。</p><h4 id="带有缺省参数值的函数"><a href="#带有缺省参数值的函数" class="headerlink" title="带有缺省参数值的函数"></a>带有缺省参数值的函数</h4><p>在C++中，函数的参数可以有缺省值。当调用有缺省参数的函数时，如果相应的参数没有给出实参，则自动用相应的缺省参数作为其实参。函数的缺省参数，是在<strong>函数原型</strong>中给定的。<br>说明<br>（1）在<strong>函数原型</strong>中，所有取缺省值的参数必须出现在不取缺省值的参数的右边。<br>（2）在<strong>函数调用</strong>时，若某个参数省略，则其后的参数皆应省略而采用缺省值。</p><h4 id="函数重载"><a href="#函数重载" class="headerlink" title="函数重载"></a>函数重载</h4><p>函数重载是指一个函数可以和<strong>同一作用域</strong>中的其他函数具有<strong>相同的名字</strong>，但这些同名函数的<strong>参数类型</strong>、<strong>参数个数</strong>不同。<br>为什么要使用函数重载？<br>对于具有同一功能的函数，如果只是由于参数类型不一样，则可以定义相同名称的函数。</p><p>调用步骤：<br>（1）寻找一个严格的匹配，即：调用与实参的数据类型、个数完全相同的那个函数。<br>（2）通过内部转换寻求一个匹配，即：通过（1）的方法没有找到相匹配的函数时，则由C++系统对实参的数据类型进行内部转换，转换完毕后，如果有匹配的函数存在，则执行该函数。<br>（3）通过用户定义的转换寻求一个匹配，若能查出有唯一的一组转换，就调用那个函数。即：在函数调用处由程序员对实参进行<strong>强制类型转换</strong>，以此作为查找相匹配的函数的依据。</p><p>注意事项：<br>重载函数不能只是函数的返回值不同，应至少在<strong>形参的个数</strong>、参数<strong>类型</strong>或参数<strong>顺序</strong>上有所不同。<br>应使所有的重载函数的功能相同。如果让重载函数完成不同的功能，会破坏程序的可读性。</p><p><strong>函数模板</strong><br>函数模板：建立一个通用函数，其函数类型和形参类型不具体指定，而是一个虚拟类型。<br>应用情况：凡是函数体相同的函数都可以用这个模板来代替，不必定义多个函数，只需在模板中定义一次即可。在调用函数时系统会根据实参的类型来取代模板中的虚拟类型，从而实现了不同函数的功能。<br><code>template&lt;typename T&gt;通用函数定义</code>，<code>template&lt;class T&gt;通用函数定义</code>(class和typename可以通用)</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">T <span class="title">max</span><span class="params">(T a, T b)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (b &gt; a) <span class="keyword">return</span> b;</span><br><span class="line">  <span class="keyword">else</span> <span class="keyword">return</span> a;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>与重载函数比较：用函数模板比函数重载更方便，程序更简洁。但应注意它只适用于：<strong>函数参数个数相同而类型不同，且函数体相同</strong>的情况。如果参数的个数不同，则不能用函数模板。</p><h4 id="作用域标识符"><a href="#作用域标识符" class="headerlink" title="作用域标识符::"></a>作用域标识符::</h4><p>通常情况下，如果有两个同名变量，一个是全局的，另一个是局部的，那么局部变量在其作用域内具有较高的优先权。<br>在全局变量加上<code>::</code>，此时<code>::var</code>代表全局变量。</p><h4 id="无名联合"><a href="#无名联合" class="headerlink" title="无名联合"></a>无名联合</h4><p>无名联合是C++中的一种特殊联合，可以声明一组无标记名共享同一段内存地址的数据项。如: <code>union {int i; float j;}</code><br>在此无名联合中，声明了变量i和f具有相同的存储地址。无名联合可通过使用其中数据项名字直接存取，例如可以直接使用上面的变量i或f。</p><h4 id="强制类型转换"><a href="#强制类型转换" class="headerlink" title="强制类型转换"></a>强制类型转换</h4><p>C中数据类型转换的一般形式 (数据类型标识符) 表达式<br>C++支持这样的格式，还提供了一种更为方便的函数调用方法，即将类型名作为函数名使用，是的类型转换的执行看起来好像调用了一个函数。形式为：数据类型标识符 (表达式)。<br>推荐使用后一种方式。</p><h4 id="动态内存分配"><a href="#动态内存分配" class="headerlink" title="动态内存分配"></a>动态内存分配</h4><p>作为对C语言中malloc和free的替换，C++引进了new和delete操作符。它们的功能是<strong>实现内存的动态分配和释放</strong>。<br><code>指针变量=new 数据类型;</code><br>或<br><code>指针变量=new 数据类型(初始值);</code></p><p>例如：<br><code>int *a, *b;</code><br><code>a = new int;</code><br><code>b = new int(10);</code></p><p>释放由new操作动态分配的内存时，用delete操作。<br><code>delete 指针变量;</code><br>例如<code>delete a;</code>，<code>delete b;</code>。</p><p>优点：<br>（1）new和delete操作自动计算需要分配和释放类型的长度。这不但<strong>省去了用sizeof计算长度</strong>的步骤，更主要的是避免了内存分配和释放时因长度出错带来的严重后果。<br>（2）new操作自动返回需分配类型的指针，<strong>无需使用强制类型转换</strong>。<br>（3）new操作能初始化所分配的类型变量。<br>（4）new和delete都<strong>可以被重载</strong>，允许建立自定义的内存管理法。</p><p>说明：<br>（1）用new分配的空间，使用结束后应该用delete显示的释放，否则这部分空间将不能回收而变成死空间。<br>（2）使用new动态分配内存时，如果没有足够的内存满足分配要求，new将返回空指针（NULL）。因此通常要对内存的动态分配是否成功进行检查。<br>（3）使用new可以为数组动态分配内存空间。这时需要在类型后面加上数组大小。<br><code>指针变量 = new 类型名[下标表达式];</code><br>使用new为多维数组分配空间时，必须提供所有维的大小。<br>（4）释放动态分配的数组存储区时，可使用delete运算符，语法格式为<code>delete []指针变量;</code><br>（5）new 可在为简单变量分配内存空间的同时，进行初始化。这时的语法形式为：<br><code>指针变量 = new 类型名(初始值列表)</code></p><h4 id="引用"><a href="#引用" class="headerlink" title="引用"></a>引用</h4><p>引用就是某一变量（目标）的一个别名，这样对引用的操作就是对目标的操作。<br>引用的声明方法：<code>类型标识符 &amp;引用名=目标变量名;</code><br>说明：<br>（1）<code>&amp;</code>在此不是求地址运算，而是起标识作用。<br>（2）<strong>类型</strong>标识符是指目标变量的类型。<br>（3）声明引用时，必须同时对其进行<strong>初始化</strong>。<br>（4）引用声明完毕后，相当于目标变量名有两个名称。<br>（5）声明一个引用，不是新定义了一个变量，系统并不给引用分配存储单元。</p><p>引用的使用<br>（1）引用名可以是任何合法的变量名。除了用作函数的参数或返回类型外，在声明时，必须立即对它进行初始化，不能声明完后再赋值。<br>（2）引用不能重新赋值，不能再把该引用名作为其他变量名的别名，任何对该引用的赋值就是该引用对应的目标变量名的赋值。对引用求地址，就是对目标变量求地址。<br>（3）由于指针变量也是变量，所以，可以声明一个指针变量的引用。方法是<code>类型标识符 *&amp;引用名=指针变量名</code><br>（4）引用是对某一变量或目标对象的引用，它本身不是一种数据类型，因此引用本身不占存储单元，这样，就不能声明引用的引用，也不能定义引用的指针。<br>（5）不能建立数组的引用，因为数组是一个由若干个元素所组成的集合，所以就无法建立一个数组的别名。<br>（6）不能建立空指针的引用。<br>（7）不能建立空类型void的引用。<br>（8）尽管引用运算符与地址操作符使用相同的符号，但是不一样的。引用仅在声明时带有引用运算符<code>&amp;</code>，以后就像普通变量一样使用，不能再带<code>&amp;</code>。其他场合使用的<code>&amp;</code>都是地址操作符。</p><p>用引用作为函数的参数<br>一个函数的参数可以定义成引用的形式。<br>在主调函数的调用点处，直接以<strong>变量</strong>作为<strong>实参</strong>进行调用即可，不需要实参变量有任何的特殊要求。</p><p>用引用返回函数值<br>函数可以返回一个引用，将函数说明为返回一个引用。<br>主要目的是：为了将函数用在赋值运算符的左边。要以引用返回函数值。<br><code>类型标识符 &amp;函数名 (形参列表及类型说明){函数体}</code><br>（1）以引用返回函数值，定义函数时需要在函数名前加<code>&amp;</code><br>（2）用引用返回一个函数值的最大好处是，在内存中不产生返回值的副本。<br>在定义返回引用的函数时，注意不要返回该函数内的自动变量（局部变量）的引用，由于自动变量的生存期仅限于函数内部，当函数返回时，自动变量就消失了。</p><p>一个返回引用的函数值<strong>作为赋值表达式的左值</strong>。<br>一般情况下，赋值表达式的左边只能是变量名，即被赋值的对象必须是变量，只有变量才能被赋值。</p><h2 id="Lecture-2-类和对象"><a href="#Lecture-2-类和对象" class="headerlink" title="Lecture 2 类和对象"></a>Lecture 2 类和对象</h2><h3 id="类的构成"><a href="#类的构成" class="headerlink" title="类的构成"></a>类的构成</h3><p>类是C++对C中结构的扩展。<br>C语言中的struct是数据成员集合，而C++中的类，则是数据成员和成员函数的集合。<br>struct是用户定义的数据类型，是一种构造数据类型。类和struct一样，也是一种用户定义的数据类型，是一种构造数据类型。<br>C结构无法对数据进行保护和权限控制，所以结构中的数据是不安全的。C++中的类将数据和与之相关联的数据封装在一起，形成一个整体，具有良好的外部接口可以防止数据未经授权的访问，提供了模块间的独立性。</p><p>类的成员分两部分：一部分对应数据的状态，称为数据成员，另一部分作用于该数据状态的函数，称为成员函数。</p><h4 id="private-protected-public"><a href="#private-protected-public" class="headerlink" title="private, protected, public"></a>private, protected, public</h4><p>· <code>private</code> 部分称为类的私有部分，这一部分的数据成员和成员函数称为类的私有成员。私有成员只能由本类的成员函数访问，而类外部的任何访问都是非法的。（只能在定义、实现的时候访问）<br>· <code>public</code> 部分称为类的共有部分，这部分的数据成员和成员函数称为类的公有成员。公有成员可以由程序中的函数访问，它对外是完全开放的。<br>· <code>protected</code> 部分称为类的保护部分，这部分的数据成员和成员函数称为类的保护成员。保护成员可以由本类的成员函数访问，也可以由本类的派生类的成员函数访问，而类外的任何访问都是非法的。</p><p>（1）类声明格式中的3个部分并非一定要全有，但至少要有其中的一个部分。<br>一般一个类的数据成员应该声明为私有成员，成员函数声明为公有成员。<br>（2）类声明中的private, protected, public三个关键字可以按任意顺序出现任意次。但是，如果把所有的私有成员、保护成员和公有成员归类放在一起，程序将更加清晰。<br>（3）private处于类体重第一部分时，关键字private可以省略。<br>（4）数据成员可以是任何数据类型，但不能用自动(auto)、寄存器(register)或外部(extern)进行声明。<br>（5）不能在类声明中给数据成员赋值。C++规定，只有在类对象定义之后才能给数据成员赋初值。</p><h3 id="成员函数的声明"><a href="#成员函数的声明" class="headerlink" title="成员函数的声明"></a>成员函数的声明</h3><p>普通成员函数形式<br>在类的声明中(.h)只给出成员函数的原型，而成员函数体写在类的外部(.cpp)。</p><p>内联函数形式<br>直接将函数声明在类内部；<br>在类声明中只给出成员函数的原型，而成员函数体写在类的外部，在成员函数返回类型前冠以关键字<code>inline</code>。</p><h3 id="对象的定义和使用"><a href="#对象的定义和使用" class="headerlink" title="对象的定义和使用"></a>对象的定义和使用</h3><p>在C++中，可以把相同数据结构和相同操作集的对象看成属于同一类。<br>当定义了一个类的对象后，就可以访问对象的成员了。在类的外部可以通过类的对象对<strong>公有成员</strong>进行访问，访问对象成员要使用操作符<code>.</code>（称为对象选择符，简称点运算符）。<br>在定义对象时，若定义的是指向对象的指针，则访问此对象的成员时，要用<code>-&gt;</code>操作符。</p><p>在类的内部所有成员之间都可以通过成员函数直接访问，但是类的外部不能访问对象的私有成员。</p><p>类成员的访问属性<br>说明为public的成员不但可以被类中成员函数访问；还可以在类的外部，通过类的对象进行访问<br>说明为private的成员只能被类中成员函数访问，不能在类的外部，通过类的对象进行访问<br>说明为protected的成员除了类本身的成员函数可以访问外，该类的派生类的成员也可以访问，但不能在类的外部，通过类的对象进行访问</p><p>类的成员对类对象的可见性和对类的成员函数的可见性是不同的。<br>类的成员函数可以访问类的所有成员，而类的对象对类的成员的访问是受类成员的访问属性的制约的。</p><p>一般来说，公有成员是类的对外接口，而私有成员和保护成员是类的内部数据和内部实现，不希望外界访问。将类的成员划分为不同的访问级别有两个好处：一是<strong>信息隐蔽</strong>，即实现封装；二是<strong>数据保护</strong>，即将类的重要信息保护起来，以免其他程序不恰当地修改。</p><p>对象赋值语句<br>两个同类型的变量之间可以相互赋值。同类型的对象间也可以进行赋值，当一个对象赋值给另一个对象时，所有的数据成员都会逐位拷贝。<br>说明：<br>·在使用对象赋值语句进行对象赋值时，两个对象的类型必须相同，如果对象的类型不同，编译时将出错。<br>·两个对象之间的赋值，仅仅使这些对象中数据成员相同，而两个对象仍是分离的。<br>·<code>=</code>的对象赋值是通过缺省的赋值运算符函数实现的。（复杂的需要重载）<br>·当类中存在指针时，使用缺省的赋值运算符进行对象赋值，可能会产生错误。</p><h3 id="构造函数与析构函数"><a href="#构造函数与析构函数" class="headerlink" title="构造函数与析构函数"></a>构造函数与析构函数</h3><p>构造函数和析构函数都是类的成员函数，但它们都是特殊的成员函数，执行特殊的功能，不用调用便自动执行，而且这些函数的名字与类的名字有关。<br>C++语言中有一些成员函数性质是特殊的，这些成员函数负责对象的建立、删除。这些函数的特殊性在于可以由编译器自动地隐含调用，其中一些函数调用格式采用运算符函数重载的语法。C++引进一个自动完成对象初始化过程的机制，这就是类的构造函数。</p><p>对象的初始化<br>1）数据成员是不能在声明类时初始化<br>2）类型对象的初始化方法：<br>·调用对外接口(public成员函数)实现 声明类→定义对象→调用接口给成员赋值<br>·应用构造函数(constructor)实现 声明类→定义对象→同时给成员赋值</p><h4 id="构造函数"><a href="#构造函数" class="headerlink" title="构造函数"></a>构造函数</h4><p>构造函数是一种特殊的成员函数，它主要用于为对象分配空间，进行初始化。构造函数具有一些特殊的性质：<br>（1）构造函数的名字必须与类名相同。<br>（2）构造函数可以有任意类型的参数，但不能指定返回类型。它有隐含的返回值，该值由系统内部使用。<br>（3）构造函数是特殊的成员函数，函数体可写在类体内，也可写在类体外。<br>（4）构造函数可以重载，即一个类中可以定义多个参数个数或参数类型不同的构造函数。构造函数不能继承。<br>（5）构造函数被声明为公有函数，但它不能像其他成员函数那样被显式地调用，它是在定义对象的同时调用的<br>·在声明类时如果没有定义类的构造函数，编译系统就会在编译时自动生成一个默认形式的构造函数。<br>·默认构造函数是构造对象时不提供参数的构造函数。<br>·除了无参数构造函数是默认构造函数外，带有全部默认参数值的构造函数也是默认构造函数。<br>·自动调用：构造函数在定义类对象时自动调用，不需用户调用，也不能被用户调用。在对象使用前调用。<br>·调用顺序：在对象进入其作用域时（对象使用前）调用构造函数。</p><p>利用构造函数创建对象的两种方法：<br>（1）利用构造函数直接创建对象，其一般形式为：<code>类名 对象名[(实参表)];</code><br>这里的“类名”与构造函数名相同，“实参表”是为构造函数提供的实际参数。<br>（2）利用构造函数创建对象时，通过指针和new来实现。其一般语法形式为：<code>类名 *指针变量 = new 类名 [(实参表)];</code></p><h4 id="成员初始化表"><a href="#成员初始化表" class="headerlink" title="成员初始化表"></a>成员初始化表</h4><p>对于常量类型和引用类型的数据成员，不能在构造函数中用赋值语句直接赋值，C++提供初始化表进行置初值。</p><p>类名::构造函数名([参数表])[:(成员初始化表)]<br>成员初始化表的一般形式为：数据成员名1(初始值1),数据成员名2(初始值2),…</p><p>如果需要将数据成员存放在堆中或数组中，则应在构造函数中使用赋值语句，即使构造函数有成员初始化表也应如此。</p><p>类成员是按照它们在类里被声明的顺序初始化的，与它们在初始化表中列出的顺序无关。</p><h4 id="拷贝构造函数"><a href="#拷贝构造函数" class="headerlink" title="拷贝构造函数"></a>拷贝构造函数</h4><p>拷贝构造函数是一种特殊的构造函数，其形参是本类对象的引用。其作用是使用一个已经存在的对象去初始化另一个同类的对象。<br>通过等号复制对象时，系统会自动调用拷贝构造函数</p><p>拷贝函数特点：<br>该函数也是一种构造函数，所以其函数名与类名相同，并且该函数也没有返回值类型<br>该函数只有一个参数，并且是同类对象的引用<br>每个类必须有一个拷贝构造函数。可以根据需要定义特定的拷贝构造函数，以实现同类对象之间数据成员的传递。如果没有定义类的拷贝构造函数，系统就会自动生成产生一个缺省的拷贝构造函数</p><p>缺省的拷贝构造函数<br>如果没有编写自定义的拷贝构造函数，C++会自动地将一个已存在的对象复制给新对象，这种按成员逐一复制的过程是由缺省拷贝构造函数自动完成的。</p><p>调用拷贝构造函数的三种情况：<br>（1）当用类的一个对象去初始化该类的另一个对象时。（代入法与赋值法）<br>（2）当函数的形参是类的对象，调用函数，进行形参和实参结合时。<br>（3）当函数的返回值是对象，函数执行完成，返回调用者时。</p><p>浅拷贝与深拷贝<br>所谓浅拷贝，就是由缺省的拷贝构造函数所实现的数据成员逐一赋值，若类中含有指针类型数据，则会产生错误。<br>为了解决浅拷贝出现的错误，必须显示地定义一个自己的拷贝构造函数，使之不但拷贝数据成员，而且为对象1和对象2分配各自的内存空间，这就是所谓的深拷贝。</p><h4 id="析构函数"><a href="#析构函数" class="headerlink" title="析构函数"></a>析构函数</h4><p>析构函数也是一种特殊的成员函数。它执行与构造函数相反的操作，通常用于撤销对象时的一些清理任务，如释放分配给对象的内存空间等。<br>析构函数有以下一些特点：<br>①析构函数与构造函数名字相同，但它前面必须加一个波浪号(~)；<br>②析构函数没有参数，也没有返回值，而且不能重载。因此在一个类中只能有一个析构函数；<br>③当撤销对象时，编译系统会自动调用析构函数。如果程序员没有定义析构函数，系统将自动生成和调用一个默认析构函数，默认析构函数只能释放对象的数据成员所占用的空间，但不包括堆内存空间。</p><p>析构函数被调用的两种情况：<br>(1)若一个对象被定义在一个函数体内，当这个函数结束时，析构函数被自动调用。<br>(2)若一个对象是使用new运算符动态创建，在使用delete释放时，自动调用析构函数。</p><h4 id="调用构造函数和析构函数的顺序"><a href="#调用构造函数和析构函数的顺序" class="headerlink" title="调用构造函数和析构函数的顺序"></a>调用构造函数和析构函数的顺序</h4><p>1） 一般顺序：调用析构函数的次序正好与调用构造函数的次序相反：最先被调用的构造函数，其对应的析构函数最后被调用，而最后被调用的构造函数，其对应的析构函数最先被调用。<br>2） 全局对象：在全局范围中定义的对象（即在所有函数之外定义的对象），它的构造函数在所有函数（包括main函数）执行之前调用。在程序的流程离开其作用域时（如main函数结束或调用exit函数）时，调用该全局对象的析构函数。<br>3） auto局部对象：局部自动对象（例如在函数中定义的对象），则在建立对象时调用其构造函数。如果函数被多次调用，则在每次建立对象时都要调用构造函数。在函数调用结束、对象释放时先调用析构函数。<br>4） static局部对象：如果在函数中定义静态局部对象，则只在程序第一次调用此函数建立对象时调用构造函数一次，在调用结束时对象并不释放，因此也不调用析构函数，只在main函数结束或调用exit函数结束程序时，才调用析构函数。</p><p>对象的生存期<br>（1）局部对象：当对象被定义时，调用构造函数，该对象被创建；当程序退出该对象所在的函数体或程序块时，调用析构函数，对象被释放。<br>（2）全局对象：当程序开始运行时，调用构造函数，该对象被创建；当程序结束时，调用析构函数，该对象被释放。<br>（3）静态对象：当程序中定义静态对象时，调用构造函数，该对象被创建；当整个程序结束时，调用析构函数，对象被释放。<br>（4）动态对象：执行new运算符调用构造函数，动态对象被创建；用delete释放对象时，调用析构函数。<br>局部对象是倍定义在一个函数体或程序块内的，它的作用域限定在函数体或程序块内，生存期较短。<br>静态对象是被定义在一个文件中，它的作用域从定义时起到文件结束时为止。生存期较长。<br>全局对象是被定义在某个文件中，它的作用域包含在该文件的整个程序中，生存期是最长的。<br>动态对象是由程序员掌握的，它的作用域和生存期是由new和delete之间的间隔决定的。</p><h2 id="Lecture-3-类和对象"><a href="#Lecture-3-类和对象" class="headerlink" title="Lecture 3 类和对象"></a>Lecture 3 类和对象</h2><h3 id="自引用指针this"><a href="#自引用指针this" class="headerlink" title="自引用指针this"></a>自引用指针this</h3><p>每一个类的成员函数都有一个隐藏定义的常量指针，称为this指针。<br>this指针的类型就是成员函数所属的类的类型。<br>每当调用成员函数时，它被初始化为被调函数所在类的对象的地址。也就是自动地将对象的指针传给它。不同的对象调用同一个成员函数时，编译器将根据成员函数的this指针所指向的对象来确定应该引用哪一个对象的数据成员。<br>在通常情况下，this指针在系统中是隐含地存在的，也可以显示地表示出来。</p><p>this指针是一个const指针，不能在程序中修改它或给它赋值。<br>this指针是一个局部数据，它的作用域仅在一个对象的内部。</p><h3 id="对象数组与对象指针"><a href="#对象数组与对象指针" class="headerlink" title="对象数组与对象指针"></a>对象数组与对象指针</h3><h4 id="对象数组"><a href="#对象数组" class="headerlink" title="对象数组"></a>对象数组</h4><p>所谓对象数组是指每一数组元素都是对象的数组。<br>与基本数据类型的数组一样，在使用对象数组时也只能访问单个数组元素，也就是一个对象，通过这个对象，也可以访问到它的公有成员。<br>如果需要建立某个类的对象数组，在设计类的构造函数时要充分考虑到数组元素初始化的需要：<br>当各个元素的初值要求为相同的值时，应该在类中定义出不带参数的构造函数或带缺省参数值的构造函数<br>当各元素对象的初值要求为不同的值时需要定义带形参（无缺省值）的构造函数<br>定义对象数组时，可通过初始化表进行赋值</p><h4 id="对象指针"><a href="#对象指针" class="headerlink" title="对象指针"></a>对象指针</h4><p>每一个对象在初始化后都会在内存中占有一定的空间。因此，既可以通过对象名访问一个对象，也可以通过对象地址来访问一个对象。对象指针就是用于存放对象地址的变量。<code>类名 * 对象指针名</code></p><p>用指针访问单个对象成员<br>初始化指向一个已创建的对象，用<code>-&gt;</code>操作符访问对象的公有成员</p><p>用对象指针访问对象数组<br>对象指针++即指向下一个数组对象元素</p><p>指向类的成员的指针<br>类的成员自身也是一些变量、函数或者对象等，因此也可以直接将它们的地址存放到一个指针变量中，这样就可以使指针直接指向对象的成员，进而可以通过指针访问对象的成员。<br>指向成员的指针只能访问公有数据成员和成员函数。<br>使用要先声明，再赋值，然后访问。<br><strong>指向数据成员的指针</strong><br>声明：<code>类型说明符 类名::*数据成员指针名</code><br>赋值：<code>数据成员指针名 = &amp;类名::数据成员名</code><br>使用：<code>对象名.*数据成员指针名</code> <code>对象指针名-&gt;*数据成员指针名</code></p><p><strong>指向成员函数的指针</strong><br>声明：<code>类型说明符 (类名:: *指针名)(参数表)</code><br>赋值：<code>成员函数指针名 = 类名::成员函数名</code><br>使用：<code>(对象名.*成员函数指针名)(参数表)</code> <code>(对象指针名-&gt;*成员函数指针名)(参数表)</code></p><h3 id="向函数传递对象"><a href="#向函数传递对象" class="headerlink" title="向函数传递对象"></a>向函数传递对象</h3><p><strong>对象</strong>可以作为参数传递给函数，其方法与传递其他类型的数据相同。在向函数传递对象时，是通过传值调用传递给函数的。因此，函数中对对象的任何修改均不影响调用该函数的对象本身。<br><strong>对象指针</strong>可以作为函数的参数，使用对象指针作为函数参数可以实现传址调用，即可在被调用函数中改变函数的参数对象的值，实现函数之间的信息传递。同时使用对象指针实参仅将对象的地址值传给形参，而不进行副本的拷贝，这样可以提高运行效率，减少时空开销。<br>使用<strong>对象引用</strong>作为函数参数不但具有用对象指针作函数参数的优点，而且用对象引用作函数参数将更简单、更直接。</p><h3 id="静态成员"><a href="#静态成员" class="headerlink" title="静态成员"></a>静态成员</h3><p>引入目的：实现一个类的不同对象之间数据和函数共享</p><p><strong>静态数据成员</strong><br>用关键字<code>static</code>声明<br>该类的所有对象维护该成员的同一个拷贝<br>必须在类外定义和初始化，用<code>(::)</code>来指明所属的类<br>与一般的数据成员不同，无论建立多少个类的对象，都只有一个静态数据的拷贝。从而实现了同一个类的不同对象之间的数据共享。它不因对象的建立而产生，也不因对象的析构而删除。<br>静态数据成员初始化的格式：<br><code>&lt;数据类型&gt;&lt;类名&gt;::&lt;静态数据成员名&gt;=&lt;值&gt;;</code><br>初始化时使用作用域运算符来标明它所属的类，因此，静态数据成员是类的成员，而不是对象的成员。<br>引用静态数据成员时，采用如下格式：<br><code>&lt;类名&gt;::&lt;静态成员名&gt;</code></p><p>如何使用静态数据成员？<br>(1)静态数据成员的定义与一般数据成员相似，但前面要加上static关键词<br>(2)静态数据成员的初始化与一般数据成员不同。初始化位置在定义对象之前，一般在类定义后，main()前进行<br>(3)访问方式（只能访问公有静态数据成员）<br>可用类名访问：<code>类名::静态数据成员</code><br>也可用对象访问：<code>对象名.静态数据成员</code>,<code>对象指针-&gt;静态数据成员</code><br>(4)私有静态数据成员不能被类外部函数访问，也不能用对象进行访问<br>(5)支持静态数据成员的一个主要原因是可以不必使用全局变量。静态数据成员的主要用途是定义类的各个对象所公用的数据。</p><p><strong>静态成员函数</strong><br>类外代码可以使用类名和作用域符来调用公有静态成员函数<br>静态成员函数只能引用属于该类的静态数据成员或静态成员函数。访问非静态数据成员，必须通过参数传递方式得到对象名，通过对象名访问。</p><p>可以通过定义和使用静态成员函数来访问静态数据成员。<br>所谓静态成员函数就是使用static关键字声明函数成员。同静态数据成员一样，静态成员函数也属于整个类，由同一个类的所有对象共同维护，为这些对象所共享。<br>静态成员函数作为成员函数，它的访问属性可以受到类的严格控制。对公有静态成员函数，可以通过类名或对象名来调用；而一般的非静态公有成员函数只能通过对象名来调用。<br>静态成员函数可以直接访问该类的静态数据成员和函数成员；而访问非静态数据成员，必须通过参数传递方式得到对象名，然后通过对象名来访问。<br>定义：<br><code>static 返回类型 静态成员函数名(参数表);</code><br>使用：<br><code>类名::静态成员函数名(实参表)</code><br><code>对象.静态成员函数名(实参表)</code><br><code>对象指针-&gt;静态成员函数名(实参表)</code></p><p>注意：<br>(1)静态成员函数可以定义成内嵌的，也可以在类外定义，在类外定义时不能用static前缀。<br>(2)静态成员函数主要用来访问全局变量或同一个类中的静态数据成员。特别是，当它与静态数据成员一起使用时，达到了对同一个类中对象之间共享数据进行维护的目的。<br>(3)私有静态成员函数不能被类外部函数和对象访问。<br>(4)使用静态成员函数的一个原因是，可以用它在建立任何对象之前处理静态数据成员。这是普通成员函数不能实现的。<br>(5)静态成员函数中没有指针this，所以静态成员函数不能访问类中的非静态数据成员，若确实需要则只能通过对象名作为参数访问。</p><p>可以通过指针访问静态数据成员和静态成员函数</p><h3 id="友元"><a href="#友元" class="headerlink" title="友元"></a>友元</h3><p>友元可以访问与其有好友关系的类中的私有成员。友元包括友元函数和友元类。</p><h4 id="友元函数"><a href="#友元函数" class="headerlink" title="友元函数"></a>友元函数</h4><p>友元函数不是当前类的成员函数，而是独立于当前类的外部函数，但它可以访问该类的所有对象的成员，包括私有、保护和公有成员。</p><p>友元函数的声明：<br>位置：当前类体中<br>格式：函数名前加<code>friend</code><br>友元函数的定义：<br>类体外：同一般函数（函数名前不能加<code>类名::</code>）<br>类体内：函数名前加<code>friend</code></p><p>说明：<br>(1)友元函数毕竟不是成员函数，因此，在类的外部定义友元函数时，不能在函数名前加上<code>类名::</code>。<br>(2)友元函数一般带有一个该类的入口参数。因为友元函数不是类的成员函数，没有this指针，所以不能直接引用对象成员的名字，也不能通过this指针引用对象的成员，它必须通过作为入口参数传递进来的对象名或对象指针来引用该对象的成员。</p><p>引入友元机制的原因<br>(1)友元机制是对类的封装机制的补充，利用此机制，一个类可以赋予某些函数访问它的私有成员的特权。<br>(2)友元提供了不同类的成员函数之间、类的成员函数与一般函数之间进行数据互享的机制。</p><h4 id="友元成员函数"><a href="#友元成员函数" class="headerlink" title="友元成员函数"></a>友元成员函数</h4><p>一个类的成员函数也可以作为另一个类的友元，这种成员函数不仅可以访问自己所在类对象中的所有成员，还可以访问friend声明语句所在类对象中的所有成员。<br>这样能使两个类相互合作、协调工作，完成某一任务。<br>一个类的成员函数作为另一个类的友元函数时，必须先定义这个类。</p><h4 id="友元类"><a href="#友元类" class="headerlink" title="友元类"></a>友元类</h4><p>一个类也可以作为另一个类的友元。<br>类Y的所有成员函数都是类X的友元函数<br>在实际工作中，除非确有必要，一般并不把整个类声明为友元类，而只将确实有需要的成员函数声明为友元函数，这样更安全一些。</p><p>友元的关系是单向的而不是双向的。<br>友元的关系不能传递。</p><h3 id="共用数据的保护-const"><a href="#共用数据的保护-const" class="headerlink" title="共用数据的保护(const)"></a>共用数据的保护(const)</h3><p>const对象的一般形式<br><code>类型名 const 对象名[(构造实参表列)];</code><br><code>const 类型名 对象名[(构造实参表列)];</code><br>常对象必须要有初值。<br>定义为const的对象的所有数据成员的值都不能被修改。凡出现调用非const的成员函数，将出现编译错误。<br>对数据成员声明为mutable时，即使是const对象，仍然可以修改该数据成员值。</p><h4 id="常数据成员"><a href="#常数据成员" class="headerlink" title="常数据成员"></a>常数据成员</h4><p>用const声明的常数据成员，其值是不能改变的。只能通过构造函数的参数初始化表对场数据成员进行初始化。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Time</span> &#123;</span></span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">int</span> hour;</span><br><span class="line">    Time(<span class="keyword">int</span> h):hour(h)&#123;&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><h4 id="常成员函数"><a href="#常成员函数" class="headerlink" title="常成员函数"></a>常成员函数</h4><p>成员函数声明中包含const时为常成员函数。此时，该函数只能引用本类中的数据成员，而不能修改它们，即成员数据不能作为语句的左值。(mutable可以)<br><code>类型说明符 函数名(参数表) const;</code><br><code>const</code>的位置在函数名和括号之后，是函数类型的一部分，在声明函数和定义函数时都要有const关键字。<br>如果将一个对象声明为常对象，则通过该对象只能调用它的常成员函数，而不能调用普通成员函数。而且常成员函数也不能更新对象的数据成员。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">show_Time</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="keyword">void</span> Time::show_Time <span class="keyword">const</span> &#123;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; hour &lt;&lt; minute &lt;&lt; sec &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="指向常对象的指针变量"><a href="#指向常对象的指针变量" class="headerlink" title="指向常对象的指针变量"></a>指向常对象的指针变量</h3><p>指向常对象的指针变量的一般形式：<br><code>const 类型 *指针变量名</code></p><p>指向常对象（变量）的指针变量，不能通过它来改变所指向目标对象的值，但指针变量的值是可以改变的。<br>如果被声明为常对象（变量），只能用指向常对象（变量）的指针变量指向它，而不能非const型指针变量去指向它。<br>指向常对象（变量）的指针变量除了可以指向常对象（变量）外，还可以指向未被声明为const的对象（变量）。此时不能通过此指针变量改变该变量的值。<br>指向常对象（变量）的指针变量可以指向const和非const型的对象（变量），而指向非const型变量的指针变量只能指向非const的对象（变量）。<br>如果函数的形参是指向非const型变量的指针，实参只能用指向非const变量的指针，而不能用指向const变量的指针，这样，在执行函数的过程中可以改变形参指针变量所指向的变量的值。<br>如果函数形参是指向const型变量的指针，允许实参是指向const变量的指针，或指向非const变量的指针。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">f</span><span class="params">(Time *pt)</span></span>;</span><br><span class="line">Time *p1;</span><br><span class="line"><span class="keyword">const</span> Time *p2;</span><br><span class="line">f(p1);  <span class="comment">//正确</span></span><br><span class="line">f(p2);  <span class="comment">//错误</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">g</span><span class="params">(<span class="keyword">const</span> Time *pt)</span></span>;</span><br><span class="line">Time *p1;</span><br><span class="line"><span class="keyword">const</span> Time *p2;</span><br><span class="line">f(p1);  <span class="comment">//正确</span></span><br><span class="line">f(p2);  <span class="comment">//错误</span></span><br></pre></td></tr></table></figure><p>| Time const t = Time(1,2,3); const Time t = Time(1,2,3);<br>const int a = 10;<br>int const a = 10; | t是常对象，其成员值在任何情况下都不能被改变 a是常变量，其值不能被改变 |<br>| —- | —- |<br>| void Time::fun() const; | fun是Time类的常成员函数，可以调用该函数，但不能修改本类中的数据成员(非mutable) |<br>| Time <em> const pt; int </em> const pa; | pt是指向Time对象的常指针，pa是指向整数的常指针。指针值不能改变  |<br>| const Time <em>pt; const int </em>pa; | pt是指向Time类常对象的指针，pa是指向常整数的指针，不能通过指针来改变指向的对象（值） |</p><h2 id="Lecture-4-派生类与继承"><a href="#Lecture-4-派生类与继承" class="headerlink" title="Lecture 4 派生类与继承"></a>Lecture 4 派生类与继承</h2><h3 id="继承与派生类"><a href="#继承与派生类" class="headerlink" title="继承与派生类"></a>继承与派生类</h3><p>继承目的：代码的重用和代码的扩充<br>继承方法程序设计思路：一般-&gt;特殊<br>继承种类：单继承、多继承<br>继承方式：public protected private<br>继承内容：除构造函数、析构函数、私有成员外的其他成员</p><p>保持已有类的特性而构造新类的过程称为<strong>继承</strong>。<br>在已有类的基础上新增自己的特性而产生新类的过程称为<strong>派生</strong>。<br>被继承的已有类称为基类（父类）。<br>派生出的新类称为派生类。</p><h4 id="继承的访问控制"><a href="#继承的访问控制" class="headerlink" title="继承的访问控制"></a>继承的访问控制</h4><p>三种继承方式：public, private, protected<br>派生类成员的访问权限：inaccessible, public, private, protected</p><div class="table-container"><table><thead><tr><th>在基类中的访问属性</th><th>继承方式</th><th>在派生类中的访问属性</th></tr></thead><tbody><tr><td>private</td><td>public</td><td>inaccessible</td></tr><tr><td>private</td><td>private</td><td>inaccessible</td></tr><tr><td>private</td><td>protected</td><td>inaccessible</td></tr><tr><td>public</td><td>public</td><td>public</td></tr><tr><td>public</td><td>private</td><td>private</td></tr><tr><td>public</td><td>protected</td><td>protected</td></tr><tr><td>protected</td><td>public</td><td>protected</td></tr><tr><td>protected</td><td>private</td><td>private</td></tr><tr><td>protected</td><td>protected</td><td>protected</td></tr></tbody></table></div><p><strong>私有继承的访问规则</strong><br>基类的public成员和protected成员被继承后作为派生类的private成员，派生类的其他成员可以直接访问它们，但是在类外部通过派生类的对象无法访问。<br>基类的private成员在私有派生类中是不可直接访问的，所以无论是派生类成员还是通过派生类的对象，都无法直接访问从基类继承来的private成员，但是可以通过基类提供的public成员函数间接访问。<br>通过派生类的对象不能访问基类中的任何成员。</p><p><strong>公有继承的访问规则</strong><br>基类的public成员和protected成员被继承到派生类中仍作为派生类的public成员和protected成员，派生类的其他成员可以直接访问它们。但是，类的外部的使用者只能通过派生类的对象访问继承来的public成员。<br>派生类的对象只能访问基类的public成员。</p><p>1.派生的对象可以赋给基类的对象</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Derived d;      <span class="comment">// Derived public inherit from Base</span></span><br><span class="line">Base b;</span><br><span class="line">b = d;</span><br></pre></td></tr></table></figure><p>2.派生类的对象可以初始化基类的引用</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Derived d;</span><br><span class="line">Base &amp;br = d;</span><br></pre></td></tr></table></figure><p>3.派生类的对象的地址可以赋给指向基类的指针</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Derived d;</span><br><span class="line">Base *pa = &amp;d;</span><br></pre></td></tr></table></figure><p>通过指针或引用只能访问对象d中所继承的基类成员。</p><p><strong>保护继承的访问规则</strong><br>基类的public成员和protected成员被继承到派生类中都作为派生类的protected成员，派生类的其他成员可以直接访问它们，但是类的外部使用者不能通过派生类的对象来访问它们。<br>通过派生类的对象不能访问基类中的任何成员。</p><p>基类与派生类的关系<br>派生类是基类的具体化<br>派生类是基类定义的延续<br>派生类是基类的组合</p><h3 id="派生类的构造函数和析构函数"><a href="#派生类的构造函数和析构函数" class="headerlink" title="派生类的构造函数和析构函数"></a>派生类的构造函数和析构函数</h3><p>基类的构造函数和析构函数不能被继承，一般派生类要加入自己的构造函数。</p><p>通常情况下，当创建派生类对象时，首先执行基类的构造函数，随后再执行派生类的构造函数；<br>当撤销派生类对象时，则先执行派生类的析构函数，随后再执行基类的析构函数。</p><p>当基类的构造函数没有参数，或没有显示定义构造函数时，派生类可以不向基类传递参数，甚至可以不定义构造函数；当基类含有带参数的构造函数时，派生类必须定义构造函数，以提供把参数传递给基类构造函数的途径。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">派生类名(参数总表):基类名(参数表)</span><br><span class="line">&#123;&#125;</span><br></pre></td></tr></table></figure><p>当派生类中含有内嵌对象成员时，其构造函数的一般形式为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">派生类名(参数总表):基类名(参数表<span class="number">1</span>),内嵌对象名<span class="number">1</span>(内嵌对象参数表<span class="number">1</span>),内嵌对象名n(内嵌对象参数表n)</span><br><span class="line">&#123;&#125;</span><br></pre></td></tr></table></figure><p>在定义派生类对象时，构造函数的执行顺序如下：<br>调用基类的构造函数；<br>调用内嵌对象成员（子对象类）的构造函数（有多个对象成员时，调用顺序由它们在类中声明的顺序确定）；<br>派生类中的构造函数体中的内容<br>撤销对象时，析构函数的调用顺序与构造函数的调用顺序相反。</p><p>当基类构造函数不带参数时，派生类可不定义构造函数，但基类构造函数带有参数，则派生类必须定义构造函数。<br>若基类使用缺省构造函数或不带参数的构造函数，则在派生类中定义构造函数时可略去<code>:基类构造函数名(参数表)</code><br>如果派生类的基类也是一个派生类，每个派生类只需负责其直接基类的构造，依次上溯。<br>由于析构函数是不带参数的，在派生类中是否定义析构函数与它所属的基类无关，基类的析构函数不会因为派生类没有析构函数而得不到执行，基类和派生类的析构函数是各自独立的。</p><h3 id="多继承"><a href="#多继承" class="headerlink" title="多继承"></a>多继承</h3><p>派生类只有一个基类，这种派生方法称为单基派生或单继承<br>当一个派生类具有多个基类时，这种派生方法称为多基派生或多继承<br><code>class 派生类名:继承方式1 基类名1,...,继承方式n,基类名n {}</code></p><p>构造函数的执行顺序同单继承：<br>先执行基类构造函数，再执行对象成员的构造函数，最后执行派生类构造函数。<br>必须同时负责该派生类所有基类构造函数的调用。派生类的参数个数必须包含完成所有基类初始化所需的参数个数。<br>处于同一层次各基类构造函数执行顺序，取决于声明派生类时所制定各基类的顺序，与派生类构造函数中所定义的成员初始化列表的各项顺序无关。</p><p>对基类成员的访问必须是无二义性，使用类名限定可以消除二义性。</p><h4 id="虚基类"><a href="#虚基类" class="headerlink" title="虚基类"></a>虚基类</h4><p>当某一个类的多个直接基类是从另一个共同基类派生而来时，这些直接基类中从上一级基类继承来的成员就拥有相同的名称。在派生类的对象中，这些同名成员在内存中同时拥有多个拷贝。一种分辨方法是使用作用域标示符来唯一表示它们。另一种方法就是定义<strong>派生类</strong>，使派生类中只保留一份拷贝。<br><code>class 派生类名:virtual 继承方式 类名 {}</code></p><p>如果在虚基类中定义有带形参的构造函数，并且没有定义缺省形参的构造函数，则整个继承结构中，所有直接或间接的派生类都必须在构造函数的成员初始化表中列出对虚基类构造函数的调用，以初始化在虚基类中定义的数据成员。<br>建立一个对象时，如果这个对象中含有从虚基类继承来的成员，则虚基类的成员是由最远派生类的构造函数通过调用虚基类的构造函数进行初始化的。该派生类的其他基类对虚基类构造函数的调用都自动被忽略。<br>若同一层次中同时包含虚基类和非虚基类，应先调用虚基类的构造函数，再调用非虚基类的构造函数，最后调用派生类的构造函数。<br>对于多个虚基类，构造函数的执行顺序仍然是先左后右，自上而下。<br>对于非虚基类，构造函数的执行顺序仍然是先左后右，自上而下。<br>若虚基类由非虚基类派生而来，则仍然先调用基类构造函数，再调用派生类的构造函数。</p><h3 id="赋值兼容规则"><a href="#赋值兼容规则" class="headerlink" title="赋值兼容规则"></a>赋值兼容规则</h3><p>所谓赋值兼容规则是指在需要基类对象的任何地方都可以使用<strong>公有派生类</strong>的对象来替代。这样，公有派生类实际上具备了基类的所有特性，凡基类能解决的问题，公有派生类也能解决。（在公有派生已提及）</p><p>(1)可以用派生类对象给基类对象赋值。<br>(2)可以用派生类对象来初始化基类的引用。<br>(3)可以把派生类的地址赋值给指向基类的指针。（这种形式的转换，是在实际应用中最常见到的）<br>(4)可以把指向派生类对象的指针赋值给指向基类对象的指针</p><p>说明<br>(1)声明为指向基类对象的指针可以指向它的公有派生的对象，但不允许指向它的私有派生的对象<br>(2)允许将一个声明为指向基类的指针指向其公有派生类的对象，但是不能将一个声明为指向派生类对象的指针指向其基类的一个对象<br>(3)声明为指向基类对象的指针，当其指向公有派生类对象时，只能用它来直接访问派生类中从基类继承来的成员，而不能直接访问公有派生类中定义的成员。<br>若想访问其公有派生类的特定成员，可以将基类指针用显示类型转换为派生类指针。</p><h2 id="Lecture-5-多态性与虚函数"><a href="#Lecture-5-多态性与虚函数" class="headerlink" title="Lecture 5 多态性与虚函数"></a>Lecture 5 多态性与虚函数</h2><h3 id="多态性"><a href="#多态性" class="headerlink" title="多态性"></a>多态性</h3><p>所谓多态性就是不同对象收到相同的消息时，产生不同的动作。<br>C++中的多态性：<br>通用多态：参数多态，包含多态<br>专用多态：重载多态，强制多态<br><strong>参数多态</strong>与类属函数和类属类相关联，函数模板和类模板就是这种多态<br><strong>包含多态</strong>是研究类族中定义于不同类中的同名成员函数的多态行为，主要是通过虚函数来实现的<br><strong>重载多态</strong>如函数重载、运算符重载等。普通函数及类的成员函数的重载多属于重载多态<br><strong>强制多态</strong>是指将一个变元的类型加以变化，以符合一个函数或操作的要求，例如加法运算符在进行浮点数与整型数相加时，首先进行类型强制转换，把整型数变为浮点数再相加的情况，就是强制多态的实例</p><p>在C++中，<strong>编译时多态性</strong>主要是通过函数重载和运算符重载实现的，<strong>运行时多态性</strong>主要是通过虚函数来实现的</p><h3 id="虚函数"><a href="#虚函数" class="headerlink" title="虚函数"></a>虚函数</h3><p>虚函数允许函数调用与函数体之间的联系在运行时才建立，也就是在运行时才决定如何动作，即所谓的动态联编。<br>虚函数是成员函数，而且是非static的成员函数。是动态联编的基础。<br><code>virtual &lt;类型说明符&gt;&lt;函数名&gt;(&lt;参数表&gt;)</code><br>如果某类中的一个成员函数被说明为虚函数，这就意味着该成员函数在派生类中可能有不同的实现。当使用这个成员函数操作指针或引用所标识对象时，对该成员函数调用采取动态联编方式，即在运行时进行关联或束定。<br>动态联编只能通过指针或引用标识对象来操作虚函数。如果采用一般类型的标识对象来操作虚函数，则将采用静态联编方式调用虚函数。</p><p>派生类中对基类的虚函数进行替换时，要求派生类中说明的虚函数与基类中的被替换的虚函数之间满足如下条件：<br>(1)与基类的虚函数有相同的参数个数<br>(2)其参数的类型与基类的虚函数的对应参数类型相同<br>(3)其返回值或者与基类虚函数的相同,或者都返回指针或引用,并且派生类虚函数所返回的指针或引用的基类型是基类中被替换的虚函数所返回的指针或引用的基类型的子类型</p><p><strong>虚函数的作用</strong><br>虚函数同派生类的结合可使C++支持运行时的多态性,实现了在基类定义派生类所拥有的通用接口,而在派生类定义具体的实现方法,即常说的”同一接口,多种方法”,它帮助程序员处理越来越复杂的程序</p><p><strong>虚函数的定义</strong><br><code>virtual 函数类型 函数名(形参表){}</code><br>派生类中重新定义时,其函数原型,包括返回类型、函数名、参数个数、参数类型的顺序，都必须与其基类中的原型完全相同。</p><p>C++规定，如果在派生类中，没有用virtual显示地给出虚函数声明，这时系统就会遵循以下的规则来判断一个成员函数是不是虚函数：<br>·该函数与基类的虚函数有相同的名称<br>·该函数与基类的虚函数有相同的参数个数及相同的对应参数类型<br>·该函数与基类的虚函数有相同的返回类型或满足赋值兼容规则的指针、引用型的返回类型<br>派生类的函数满足了上述条件，就被自动确定为虚函数</p><p>说明：<br>(1)通过定义虚函数来使用C++提供的多态机制时，派生类应该从它的基类<strong>公有派生</strong>。赋值兼容规则成立的条件是派生类从其基类公有派生。<br>(2)必须首先在基类中定义虚函数。在实际应用中，应该在类等级内需要具有动态多态性的几个层次中的最高层类内首先声明虚函数。<br>(3)在派生类对基类中声明的虚函数进行重新定义时，关键字virtual可以写或不写。<br>(4)使用对象名和点运算符的方式也能调用虚函数，但是这种调用在编译时进行的是静态联编，它没有充分利用虚函数的特性。只有通过基类指针访问虚函数时才能获得运行时的多态性。<br>(5)一个虚函数无论被公有继承多少次，它仍然保持其虚函数的特性。<br>(6)虚函数必须是其所在类的成员函数，而不能是友元函数，也不能是静态成员函数，因为虚函数调用要靠特定的对象来决定该激活哪个函数。但是虚函数可以在另一个类中被声明为友元函数。<br>(7)内联函数不能是虚函数，因为内联函数是不能在运行中动态确定其位置的。即使虚函数在类的内部定义，编译时仍将其看作是非内联的。<br>(8)构造函数不能是虚函数。因为虚函数作为运行过程中多态的基础，主要是针对对象的，而构造函数是在产生对象之前运行的，因此虚构造函数是没有意义的。<br>(9)析构函数可以是虚函数，而且通常声明为虚函数。</p><p><strong>虚析构函数</strong><br>在程序用带指针参数的delete运算符撤销对象时，会发生一种情况：系统会只执行基类的析构函数，而不执行派生类的析构函数。<br>解决方法：将基类的析构函数声明为虚函数<br>析构函数设置为虚函数后，在使用指针引用时可以动态联编，实现运行时的多态，保证使用基类类型的指针能够调用适当的析构函数针对不同的对象进行清理工作</p><p>虚函数与重载函数的关系<br>在一个派生类中重新定义基类的虚函数是函数重载的另一种形式，但它不同于一般的函数重载。<br>普通的函数重载时，其函数的参数个数或参数类型必须有所不同，函数的返回类型也可以不同。<br>当重载一个虚函数时，也就是说在派生类中重新定义虚函数时，要求函数名、返回类型、参数个数、参数的类型和顺序与基类中的虚函数原型完全相同。<br>若仅仅函数名相同，而参数的个数、类型或顺序不同，系统将它作为普通的函数重载，这时将失去虚函数的特性。</p><h3 id="纯虚函数和抽象类"><a href="#纯虚函数和抽象类" class="headerlink" title="纯虚函数和抽象类"></a>纯虚函数和抽象类</h3><h4 id="纯虚函数"><a href="#纯虚函数" class="headerlink" title="纯虚函数"></a>纯虚函数</h4><p><strong>纯虚函数</strong>是一个在基类中说明的虚函数，它在基类中没有定义，但要求在它的派生类中必须定义自己的版本，或重新说明为纯虚函数。<br><code>virtual &lt;函数类型&gt;&lt;函数名&gt;(参数表)=0;</code><br>纯虚函数与一般虚函数成员的原型在书写形式上的不同就在于后面加了<code>=0</code>，表明在基类中不用定义该函数，它的实现部分（函数体）留给派生类去做。</p><p>纯虚函数没有函数体<br>最后面的<code>=0</code>并不表示函数返回值为0<br>这是一个声明语句，最后有<code>;</code><br>纯虚函数只有函数的名字而不具备函数的功能，不能被调用。在派生类中对此函数提供定义后，它才能具备函数的功能，可被调用。<br>如果在一个类中声明了纯虚函数，而在其派生类中没有对该函数定义，则该虚函数在派生类中仍然为纯虚函数。<br>一个具有纯虚函数的类称为<strong>抽象类</strong></p><h4 id="抽象类"><a href="#抽象类" class="headerlink" title="抽象类"></a>抽象类</h4><p>如果一个类至少有一个纯虚函数，那么就称该类为抽象类。<br>抽象类只能作为其他类的基类来使用，不能建立抽象类对象，其纯虚函数的实现由派生类给出。<br>派生类中必须重载基类中的纯虚函数，否则它仍将被看作一个抽象类。</p><p>规定：<br>(1)由于抽象类中至少包含一个没有定义功能的纯虚函数，因此，抽象类只能作为其他类的基类来使用，不能建立抽象类的对象，纯虚函数的实现由派生类给出<br>(2)不允许从具体类派生出抽象类<br>(3)抽象类不能用作参数类型、函数返回类型或显示转换的类型<br>(4)可以声明指向抽象类的指针或引用，此指针可以指向它的派生类，进而实现多态性<br>(5)抽象类的析构函数可以被声明为纯虚函数，这时，应该至少提供该析构函数的一个实现<br>(6)如果派生类中没有重定义纯虚函数，而派生类只是继承基类的纯虚函数，则这个派生类仍然是一个抽象类。如果派生类中给出了基类纯虚函数的实现，则该派生类就不是抽象类了，它是一个可以建立对象的具体类<br>(7)在抽象类中也可以定义普通成员函数或虚函数，虽然不能为抽象类声明对象，但仍然可以通过派生类对象来调用这些不是纯虚函数的函数。</p><h3 id="运算符重载"><a href="#运算符重载" class="headerlink" title="运算符重载"></a>运算符重载</h3><p>运算符重载是使同一个运算符作用于不同类型的数据时具有不同的行为。运算符重载实质上将运算对象转化为运算函数的实参，并根据实参的类型来确定重载的运算函数。</p><p>运算符重载的规则<br>1.只能重载C++中已有的运算符，不能臆造新的运算符<br>2.类属关系运算符<code>.</code>、作用域分辨符<code>::</code>、成员指针运算符<code>*</code>、<code>sizeof</code>运算符和三目运算符<code>?:</code>不能重载<br>3.重载之后运算符的优先级和结合性都不能改变，单目运算符只能重载为单目运算符，双目运算符只能重载为双目运算符<br>4.运算符重载后的功能应当与原有功能相类似<br>5.重载运算符含义必须清楚，不能有二义性</p><h4 id="将运算符重载为类的成员函数"><a href="#将运算符重载为类的成员函数" class="headerlink" title="将运算符重载为类的成员函数"></a>将运算符重载为类的成员函数</h4><p>将运算符重载为类的成员函数就是在类中用关键字operator定义一个成员函数，函数名就是重载的运算符。运算符如果重载为类的成员函数，它就可以自由地访问该类的数据成员。<br><code>&lt;类型&gt;&lt;类名&gt;::operator&lt;要重载的运算符&gt;(形参表){}</code></p><p><strong>双目运算</strong><br>op1 B op2<br>把B重载为op1所属类的成员函数，只有一个形参，形参的类型是op2所属类。<br>例如，经过重载后，<code>op1+op2</code>就相当于<code>op1.operator+(op2)</code></p><p><strong>单目运算</strong><br>(1)前置单目运算：U op<br>把U重载为operand所属类的成员函数，没有形参。<br>例如，<code>++</code>重载的语法格式为:<code>&lt;函数类型&gt; operator ++();</code><br><code>++op</code>就相当于函数调用<code>op.operator ++();</code></p><p>(2)后置单目运算：op V<br>运算符V重载为op所属类的成员函数，带有一个整型(int)形参。<br>例如，后置单目运算符<code>--</code>重载的语法格式为:<code>&lt;函数类型&gt; operator --(int);</code><br><code>op--</code>就相当于函数调用<code>op.operator--(0);</code></p><p>对于++(—)运算符的重载，因为编译器不能区分出++(—)是前置还是后置的，所以要加上(int)来区分。</p><p><strong>赋值运算</strong><br>赋值运算符重载一般包括以下几个步骤，首先要检查是否自赋值，如果是要立即返回，如果不返回，后面的语句会把自己所指空间删掉，从而导致错误；第二步要释放原有的内存资源；第三步要分配新的内存资源，并复制内容；第四步是返回本对象的引用。如果没有指针操作，则没有第二步操作。<br>赋值运算符与拷贝构造函数在功能上有些类似，都是用一个对象去填另一个对象，但拷贝构造函数是在对象建立的时候执行，赋值运算符是在对象建立之后执行。</p><h4 id="运算符重载为友元函数"><a href="#运算符重载为友元函数" class="headerlink" title="运算符重载为友元函数"></a>运算符重载为友元函数</h4><p><code>friend &lt;函数返回类型&gt; operator &lt;二元运算符&gt;(&lt;形参1&gt;,&lt;形参2&gt;);</code><br><code>friend &lt;函数返回类型&gt; operator &lt;一元运算符&gt;(类名 &amp;对象){}</code></p><p>其中，函数返回类型为运算符重载函数的返回类型。<code>operator&lt;重载函数符&gt;</code>为重载函数名。当重载函数作为友元普通函数时，重载函数不能用对象调用，所以参加运算的对象必须以形参方式传送到重载函数体内，在二元运算符重载函数为友元函数时，形参通常为两个参加运算的对象。</p><p><strong>双目运算</strong><br>op1 B op2<br>双目运算符B重载为op1所属类的友元函数，该函数有两个形参，表达式<code>op1 B op2</code>相当于函数调用<code>operator B(op1, op2)</code></p><p><strong>单目运算</strong><br>(1)前置单目运算 U op<br>前置单目运算符U重载为op所属类的友元函数，表达式<code>U op</code>相当于函数调用<code>operator U(op)</code></p><p>(2)后置单目运算 op U<br>后置单目运算符V重载为op所属类的友元函数，表达式<code>op V</code>相当于函数调用<code>operator V(op, int)</code></p><h3 id="重载流插入和流提取运算符"><a href="#重载流插入和流提取运算符" class="headerlink" title="重载流插入和流提取运算符"></a>重载流插入和流提取运算符</h3><p>istream和ostream是C++的预定义流类，cin是istream的对象，cout是ostream的对象。运算符&lt;&lt;由ostream重载为插入操作，运算符&gt;&gt;由istream重载为提取操作，用于输入和输出基本类型数据。可用重载&lt;&lt;和&gt;&gt;运算符，用于输入和输出用户自定义的数据类型，必须定义为类的友元函数。</p><h4 id="输出操作符的重载"><a href="#输出操作符的重载" class="headerlink" title="输出操作符的重载"></a>输出操作符的重载</h4><p><code>ostream &amp; operator &lt;&lt;(ostream &amp;, const 自定义类&amp;);</code><br>第一个参数和函数的类型都必须是<code>ostream &amp;</code>类型，第二个参数是对要进行输出的类类型的引用，它可以是const，因为一般而言输出一个对象不应该改变对象。返回类型是一个ostream引用，通常是输出操作符所操作的ostream对象。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ostream &amp;<span class="keyword">operator</span>&lt;&lt;(ostream &amp;output,Date &amp;d)</span><br><span class="line">&#123;</span><br><span class="line">    output&lt;&lt;d.year&lt;&lt;“-”&lt;&lt;d.month&lt;&lt;“-”&lt;&lt;d.day；</span><br><span class="line">    <span class="keyword">return</span> output;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="输入操作符的重载"><a href="#输入操作符的重载" class="headerlink" title="输入操作符的重载"></a>输入操作符的重载</h4><p><code>istream &amp; operator &gt;&gt;(istream &amp;, 自定义类 &amp;)</code><br>与输出操作符类似，输入操作符的第一个形参是一个引用，指向要读的流，并且返回的也是同一个流的引用。第二个形参是对要读入的对象的非const引用，该形参必须为非const，因为输入操作符的目的是将数据读到这个对象中。和输出操作符不同的是输入操作符必须处理错误和文件结束的可能性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Lecture-1-绪论&quot;&gt;&lt;a href=&quot;#Lecture-1-绪论&quot; class=&quot;headerlink&quot; title=&quot;Lecture 1 绪论&quot;&gt;&lt;/a&gt;Lecture 1 绪论&lt;/h2&gt;&lt;h3 id=&quot;序&quot;&gt;&lt;a href=&quot;#序&quot; class=&quot;he
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>deep-learning-note</title>
    <link href="http://haelchan.me/2018/02/17/deep-learning-note/"/>
    <id>http://haelchan.me/2018/02/17/deep-learning-note/</id>
    <published>2018-02-18T04:01:32.000Z</published>
    <updated>2018-10-18T15:07:30.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Neural-Networks-and-Deep-Learning"><a href="#Neural-Networks-and-Deep-Learning" class="headerlink" title="Neural Networks and Deep Learning"></a>Neural Networks and Deep Learning</h2><h3 id="Week-One"><a href="#Week-One" class="headerlink" title="Week One"></a>Week One</h3><p>(Neural network is also introduced in Machine Learning course, with <a href="http://haelchan.me/2017/11/01/machine-learning-note/#Week-Four">my learning note</a>).</p><p>House price Prediction can be regarded as the simplest neural network:<br><img src="/images/DL/neuron.jpg" alt><br>The function can be ReLU (REctified Linear Unit), which we’ll see a lot.<br><img src="/images/DL/ReLU.jpg" alt></p><p>This is a single neuron. A larger neural network is then formed by taking many of the single neurons and stacking them together.</p><p>Almost all the economic value created by neural networks has been through <strong>supervised learning</strong>.</p><div class="table-container"><table><thead><tr><th>Input(x)</th><th>Output(y)</th><th>Application</th><th>Neural Network</th></tr></thead><tbody><tr><td>House feature</td><td>Price</td><td>Real estate</td><td>Standard NN</td></tr><tr><td>Ad, user info</td><td>Click on ad?(0/1)</td><td>Online advertising</td><td>Standard NN</td></tr><tr><td>Photo</td><td>Object(Index 1,…,1000)</td><td>Photo tagging</td><td>CNN</td></tr><tr><td>Audio</td><td>Text transcript</td><td>Speech recognition</td><td>RNN</td></tr><tr><td>English</td><td>Chinese</td><td>Machine translation</td><td>RNN</td></tr><tr><td>Image, Radar info</td><td>Position of other cars</td><td>Autonomous driving</td><td>Custom/Hybrid</td></tr></tbody></table></div><p><strong>Neural Network examples</strong><br><img src="/images/DL/NNexamples.jpg" alt><br>CNN: often for image data<br>RNN: often for one-dimensional sequence data</p><p><strong>Structured data and Unstructured data</strong><br><img src="/images/DL/structured.jpg" alt></p><p><strong>Scale drives deep learning progress</strong><br><img src="/images/DL/scaleDrive.jpg" alt><br>Scale: both the size of the neural network and the scale of the data.</p><ul><li>Data</li><li>Computation</li><li>Algorithms</li></ul><p>Using ReLU instead of sigmoid function as activation function can improve efficiency.</p><h3 id="Week-Two"><a href="#Week-Two" class="headerlink" title="Week Two"></a>Week Two</h3><p><strong>Notation</strong><br><em>(x,y)</em>: a single training example. <script type="math/tex">x\in\mathbb{R}^{n_x},y\in\{0,1\}</script><br><em>m</em> training examples: <script type="math/tex">\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...,(x^{(m)},y^{(m)})\}</script></p><script type="math/tex; mode=display">X=\begin{bmatrix}|&|&\ &|\\x^{(1)}&x^{(2)}&...&x^{(m)}\\|&|&\ &|\end{bmatrix}</script><script type="math/tex; mode=display">X\in\mathbb{R}^{n_x\times m}</script><p>Take training set inputs x1, x2 and so on and stacking them in columns. (This make the implementation much easier than X’s transpose)</p><script type="math/tex; mode=display">Y=\begin{bmatrix}y^{(1)}&y^{(2)}&...&y^{(m)}\end{bmatrix}</script><script type="math/tex; mode=display">Y\in\mathbb{R}^{1\times m}</script><h4 id="Logistic-Regression"><a href="#Logistic-Regression" class="headerlink" title="Logistic Regression"></a>Logistic Regression</h4><p><strong>Differences with former course</strong><br>Notation is a bit different from what is introduced in Machine Learning(<a href="http://haelchan.me/2017/11/01/machine-learning-note/#Week-Four">note</a>).<br>Originally, we add <script type="math/tex">x_0=1</script> so that <script type="math/tex">x\in\mathbb{R}^{n_x+1}</script>.</p><script type="math/tex; mode=display">\hat{y}=\sigma(\theta^Tx)</script><p>where <script type="math/tex">\theta=\begin{bmatrix}\theta_0\\\theta_1\\\theta_2\\...\\\theta_{n_x}\end{bmatrix}</script>.</p><p>Here in Deep Learning course, we use <em>b</em> to represent <script type="math/tex">\theta_0</script>, and <em>w</em> to represent <script type="math/tex">\theta_1,...,\theta_{n_m}</script>. Just keep <em>b</em> and <em>w</em> as separate parameters.<br>Given <em>x</em>, want <script type="math/tex">\hat{y}=P(y=1|x)</script>. <script type="math/tex">x\in\mathbb{R}^{n_x},0\le\hat y\le1</script><br>Parameters: <script type="math/tex">w\in\mathbb{R}^{n_x},b\in\mathbb{R}</script><br>Output: <script type="math/tex">\hat{y}=\sigma(w^Tx+b)</script><br>σ() is sigmoid function: <script type="math/tex">\sigma(z)=\frac{1}{1+e^{-z}}</script><br><img src="/images/DL/sigmoidFunction.jpg" alt></p><p><strong>Cost Function</strong></p><script type="math/tex; mode=display">\mathscr{L}(\hat{y},y)=-(y\log\hat{y}+(1-y)\log(1-\hat{y}))</script><p>If <script type="math/tex">y=1:\mathscr{L}(\hat{y},y)=-\log\hat{y}</script><br>If <script type="math/tex">y=0:\mathscr{L}(\hat{y},y)=-\log(1-\hat{y})</script></p><p>Cost function:</p><script type="math/tex; mode=display">J(w,b)=\frac{1}{m}\sum_{i=1}^m\mathscr{L}(\hat{y}^{(i)},y^{(i)})=-\frac{1}{m}\sum_{i=1}^m(y^{(i)}\log\hat{y}^{(i)}+(1-y^{(i)})\log(1-\hat{y}^{(i)}))</script><p>Loss function is applied to just a single training example.<br>Cost function is the cost of your parameters, it is the average of the loss functions of the entire training set.</p><p><strong>Gradient Descent</strong><br>Usually initialize the value to zero in logistic regression. Random initialization also works, but people don’t usually do that for logistic regression.</p><p>Repeat {</p><script type="math/tex; mode=display">w:=w-\alpha\frac{\partial J(w,b)}{\partial w}</script><script type="math/tex; mode=display">b:=b-\alpha\frac{\partial J(w,b)}{\partial b}</script><p>}</p><p><img src="/images/DL/logisticDerivative.jpg" alt><br>From forward propagation, we calculate <em>z</em>, <em>a</em> and finally <script type="math/tex">\mathscr{L}(a,y)</script><br>From back propagation, we calculate the derivatives step by step:</p><script type="math/tex; mode=display">da=\frac{d\mathscr{L}(a,y)}{da}=-\frac{y}{a}+\frac{1-y}{1-a}</script><script type="math/tex; mode=display">dz=\frac{d\mathscr{L}(a,y)}{dz}=\frac{d\mathscr{L}}{da}\cdot\frac{da}{dz}=(-\frac{y}{a}+\frac{1-y}{1-a})\cdot a(1-a)=a-y</script><script type="math/tex; mode=display">dw_1=\frac{d\mathscr{L}}{dw_1}=x_1\cdot dz</script><script type="math/tex; mode=display">dw_2=x_2\cdot dz</script><script type="math/tex; mode=display">db=dz</script><p><strong>Algorithm</strong><br>(Repeat)<br>J=0; dw1,dw2,…dwn=0; db=0<br>for i = 1 to m</p><script type="math/tex; mode=display">z^{(i)}=w^Tx^{(i)}+b</script><script type="math/tex; mode=display">a^{(i)}=\sigma(z^{(i)})</script><script type="math/tex; mode=display">J+=-[y^{(i)}\log a^{(i)}+(1-y^{(i)})\log(1-a^{(i)})]</script><script type="math/tex; mode=display">dz^{(i)}=a^{(i)}-y^{(i)}</script><p>  for j = 1 to n: <script type="math/tex">dw_j+=x^{(i)}_jdz^{(i)}</script></p><script type="math/tex; mode=display">db+=dz^{(i)}</script><p>J /= m;<br>dw1,dw2,…,dwn /= m;<br>db /= m</p><p>w1:=w1-αdw1<br>w2:=w2-αdw2<br>b:=b-αdb</p><p>In the for loop, there’s no superscript i for <em>dw</em> variable, because the value of <em>dw</em> in the code is cumulative. While <em>dz</em> is referring to one training example.</p><p><strong>Vectorization</strong><br>Original <code>for</code> loop:<br>for i = 1 to m</p><script type="math/tex; mode=display">z^{(i)}=w^Tx^{(i)}+b</script><script type="math/tex; mode=display">a^{(i)}=\sigma(z^{(i)})</script><p>Vectorized:</p><script type="math/tex; mode=display">Z=\begin{bmatrix}z^{(1)}&z^{(2)}&...&z^{(m)}\end{bmatrix}=w^TX+\begin{bmatrix}b&b&...&b\end{bmatrix}=\begin{bmatrix}w^Tx^{(1)}+b&w^Tx^{(2)}+b&...&w^Tx^{(m)}+b\end{bmatrix}</script><script type="math/tex; mode=display">A=\begin{bmatrix}a^{(1)}&a^{(2)}&...&a^{(m)}\end{bmatrix}=\sigma(z)</script><script type="math/tex; mode=display">Y=\begin{bmatrix}y^{(1)}&...&y^{(m)}\end{bmatrix}</script><script type="math/tex; mode=display">dZ=\begin{bmatrix}dz^{(1)}&dz^{(2)}&...&dz^{(m)}\end{bmatrix}=A-Y</script><p>Code:<br><code>z = np.dot(w.T, X) + b</code><br><code>dz = A - Y</code><br><code>cost = -1 / m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))</code><br><code>db = 1 / m * np.sum(dZ)</code><br><code>dw = 1 / m * X * dZ.T</code></p><h4 id="About-Python"><a href="#About-Python" class="headerlink" title="About Python"></a>About Python</h4><p><code>A.sum(axis = 0)</code>: sum vertically<br><code>A.sum(axis = 1)</code>: sum horizontally</p><p><strong>Broadcasting</strong><br>If an (m, n) matrix operates with (+-*/) a (1, n) row vector, just expand the vector vertically to (m, n) by copying m times.<br>If an (m, n) matrix operates with a (m, 1) column vector, just expand the vector horizontally to (m, n) by copying n times.<br>If an row/column vector operates with a real number, just expand the real number to the corresponding vector.<br><a href="https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html" target="_blank" rel="noopener">documention</a></p><p><strong>Rank 1 Array</strong><br><code>a = np.random.randn(5)</code> creates a <em>rank 1 array</em> whose shape is <code>(5,)</code>.<br>Try to avoid using rank 1 array. Use <code>a = a.reshape((5, 1))</code> or <code>a = np.random.randn(5, 1)</code>.</p><p>Note that <code>np.dot()</code> performs a matrix-matrix or matrix-vector multiplication. This is different from <code>np.multiply()</code> and the <code>*</code> operator (which is equivalent to <code>.*</code> in MATLAB/Octave), which performs an element-wise multiplication.</p><h3 id="Week-Three"><a href="#Week-Three" class="headerlink" title="Week Three"></a>Week Three</h3><h4 id="Neural-Network-Overview"><a href="#Neural-Network-Overview" class="headerlink" title="Neural Network Overview"></a>Neural Network Overview</h4><p>Superscript with square brackets denotes the layer, superscript with round brackets refers to i’th training example.</p><p><img src="/images/DL/singleNeuron.jpg" alt><br>Logistic regression can be regarded as the simplest neural network. The neuron takes in the inputs and make two computations: <script type="math/tex">z=w^Tx+b$,a=\sigma(z)</script></p><p><img src="/images/DL/neuralRepre.jpg" alt><br>Neural network functions similarly. (Note that this neural network has 2 layers. When counting layers, input layer is not included.)<br>Take the first node in the hidden layer as example:</p><script type="math/tex; mode=display">z^{[1]}_1=w_1^{[1]T}x+b^{[1]}_1</script><script type="math/tex; mode=display">a^{[1]}_1=\sigma(z^{[1]}_1)</script><p>The superscript <script type="math/tex">[l]</script> denotes the layer, and subscript <code>i</code> represents the node in layer.<br>Similarly,</p><script type="math/tex; mode=display">z^{[1]}_2=w_2^{[1]T}x+b^{[1]}_2</script><script type="math/tex; mode=display">a^{[1]}_2=\sigma(z^{[1]}_2)</script><script type="math/tex; mode=display">z^{[1]}_3=w_1^{[1]T}x+b^{[1]}_3</script><script type="math/tex; mode=display">a^{[1]}_3=\sigma(z^{[1]}_3)</script><script type="math/tex; mode=display">z^{[1]}_4=w_1^{[1]T}x+b^{[1]}_4</script><script type="math/tex; mode=display">a^{[1]}_4=\sigma(z^{[1]}_4)</script><p>Vectorization:</p><script type="math/tex; mode=display">z^{[1]}=\begin{bmatrix}-&w_1^{[1]}&-\\-&w_2^{[1]}&-\\-&w_3^{[1]}&-\\-&w_4^{[1]}&-\end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\end{bmatrix}+\begin{bmatrix}b_1^{[1]}\\b_2^{[1]}\\b_3^{[1]}\\b_4^{[1]}\end{bmatrix}=\begin{bmatrix}w_1^{[1]T}x+b_1^{[1]}\\w_2^{[1]T}x+b_2^{[1]}\\w_3^{[1]T}x+b_3^{[1]}\\w_4^{[1]T}x+b_4^{[1]}\end{bmatrix}=\begin{bmatrix}z^{[1]}_1\\z^{[1]}_2\\z^{[1]}_3\\z^{[1]}_4\end{bmatrix}</script><script type="math/tex; mode=display">a^{[1]}=\begin{bmatrix}a_1^{[1]}\\a^{[1]}_2\\a^{[1]}_3\\a^{[1]}_4\end{bmatrix}=\sigma(z^{[1]})</script><p>Formula:</p><script type="math/tex; mode=display">z^{[1]}=W^{[1]}a[0]+b^{[1]}</script><script type="math/tex; mode=display">a^{[1]}=\sigma(z^{[1]})</script><script type="math/tex; mode=display">z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">a^{[2]}=\sigma(z^{[2]})</script><p>(dimensions: <script type="math/tex">z^{[1]}:(4,1),W^{[1]}:(4,3),a^{[0]}:(3,1),b^{[1]}:(4,1),a^{[1]}:(4,1);z^{[2]}:(1,1),W^{[2]}:(1,4),b^{[2]}:(1,1),a^{[2]}:(1,1)</script>)</p><p>Vectorizing across multiple examples:</p><script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]}</script><script type="math/tex; mode=display">A[1]=\sigma(Z^{[1]})</script><script type="math/tex; mode=display">Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">A^{[2]}=\sigma(Z^{[2]})</script><p>Explanation<br><img src="/images/DL/x1.jpg" alt><br><img src="/images/DL/a1.jpg" alt><br>Stack elements in column.<br>Each column represents a training example, each row represents a hidden unit.</p><p><img src="/images/DL/justification.jpg" alt></p><h4 id="Activation-Function"><a href="#Activation-Function" class="headerlink" title="Activation Function"></a>Activation Function</h4><p><strong>Sigmoid Function</strong></p><script type="math/tex; mode=display">a=\frac{1}{1+e^{-z}}</script><p><img src="/images/DL/act0.jpg" alt><br>Only used in binary classification’s output layer(with output 0 or 1).<br>Not used in other occasion. <code>tanh</code> is a better choice.</p><script type="math/tex; mode=display">g'(z)=\frac{d}{dz}g(z)=\frac{1}{1+e^{-z}}(1-\frac{1}{1+e^{-z}})=a(1-a)</script><p><strong>tanh Function</strong></p><script type="math/tex; mode=display">a=\tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}</script><p><img src="/images/DL/act1.jpg" alt><br>With a range of <script type="math/tex">(-1,1)</script>, it performs better than sigmoid function because the mean of its output is closer to zero.<br>Both sigmoid and tanh function have a disadvantage that when <em>z</em> is very large(<script type="math/tex">\to\infty</script>) or very small(<script type="math/tex">\to-\infty</script>), the derivative can be close to 0, so the gradient descent would be very slow.</p><script type="math/tex; mode=display">g'(z)=\frac{d}{dz}g(z)=1-(tanh(z)^2)=1-a^2</script><p><strong>ReLU</strong></p><script type="math/tex; mode=display">a=\max(0,z)</script><p><img src="/images/DL/act2.jpg" alt><br>Default choice of activation function.<br>With <script type="math/tex">g'(z)=1</script> when z is positive, it performs well in practice.<br>(Although the <code>g&#39;(z)=0</code> when z is positive, and technically the derivative when <script type="math/tex">z=0</script> is not well-defined)</p><p><strong>Leaky ReLU</strong></p><script type="math/tex; mode=display">a=\max(0.01z,z)</script><p><img src="/images/DL/act3.jpg" alt><br>Makes sure that derivatives not equal to 0 when z &lt; 0.</p><p>Linear Activation Function</p><script type="math/tex; mode=display">g(z)=z</script><p>Also called identity function.<br>Not used in neural network, because even many hidden layers still gets a linear result. Just used in machine learning when the output is a real number.</p><h4 id="Gradient-descent"><a href="#Gradient-descent" class="headerlink" title="Gradient descent"></a>Gradient descent</h4><p>Forward propagation:</p><script type="math/tex; mode=display">Z^{[1]}=W^{[1]}X+b^{[1]}</script><script type="math/tex; mode=display">A^{[1]}=g^{[1]}(Z^{[1]})</script><script type="math/tex; mode=display">Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}</script><script type="math/tex; mode=display">A^{[2]}=g^{[2]}(Z^{[2]})</script><p>Backward propagation:</p><script type="math/tex; mode=display">dZ^{[2]}=A^{[2]}-Y</script><script type="math/tex; mode=display">dW^{[2]}=\frac{1}{m}dZ^{[2]}A^{[1]T}</script><script type="math/tex; mode=display">db^{[2]}=\frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)</script><script type="math/tex; mode=display">dZ^{[1]}=W^{[2]T}dZ^{[2]}*g^{[1]'}(Z^{[1]})</script><script type="math/tex; mode=display">dW^{[1]}=\frac{1}{m}dZ^{[1]}X^T</script><script type="math/tex; mode=display">db^{[1]}=\frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)</script><p>note:<br><code>keepdims=True</code> makes sure that Python won’t produce <em>rank-1 array</em> with shape of <code>(n,)</code>.<br><code>*</code> is element-wise product. <script type="math/tex">dZ^{[1]}</script>:(n[1],m);<script type="math/tex">W^{[2]T}dZ^{[2]}</script>:(n[1],m);<script type="math/tex">g^{[1]'}(Z^{[1]})</script>:(n[1],m).</p><h4 id="Random-Initialization"><a href="#Random-Initialization" class="headerlink" title="Random Initialization"></a>Random Initialization</h4><p>In logistic regression, it’s okay to initialize all parameters to zero. However, it’s not feasible in neural network.<br>Instead, initialize <em>w</em> with <strong>random small</strong> value to break symmetry.  It’s okay to initialize <em>b</em> to zeros. Symmetry is still broken so long as <script type="math/tex">W^{[l]}</script> is initialized randomly.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W1 = np.random.randn((2, 2)) * 0.01</span><br><span class="line">b1 = np.zeros((2, 1))</span><br><span class="line">W2 = np.random.randn((1, 2)) * 0.01</span><br><span class="line">b2 = 0</span><br></pre></td></tr></table></figure><p><strong>Random</strong><br>If the parameter <em>w</em> are all zeros, then the neurons in hidden layers are symmetric(“identical”). Even if after gradient descent, they keep the same. So use random initialization.</p><p><strong>Small</strong><br>Both sigmoid and tanh function has greatest derivative at <code>z=0</code>. If <em>z</em> had large or small value, the derivative would be close to zero, and consequently gradient descent would be slow. Thus, it’s a good choice to make the value small.</p><h3 id="Week-Four"><a href="#Week-Four" class="headerlink" title="Week Four"></a>Week Four</h3><p><strong>Deep neural network notation</strong><br>-<script type="math/tex">l</script>: number of layers<br>-<script type="math/tex">n^{[l]}</script>: number of units in layer <em>l</em><br>-<script type="math/tex">a^{[l]}</script>: activations in layer <em>l</em>. <script type="math/tex">a^{[l]}=g^{[l]}(z^{[l]})</script><br>(<script type="math/tex">a^{[0]}=x, a^{[l]}=\hat{y}</script>)<br>-<script type="math/tex">W^{[l]}</script>: weights for <script type="math/tex">z^{[l]}</script><br>-<script type="math/tex">b^{[l]}</script>: bias for <script type="math/tex">z^{[l]}</script></p><p><strong>Forward Propagation</strong><br>for l = 1 to L:</p><script type="math/tex; mode=display">Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">A^{[l]}=g^{[l]}(Z^{[l]})</script><p>Well, this <code>for</code> loop is inevitable.</p><p><strong>Matrix Dimensions</strong><br>-<script type="math/tex">W^{[l]}=dW^{[l]}:(n^{[l]},n^{[l-1]})</script><br>-<script type="math/tex">b^{[l]}=db^{[l]}:(n^{[l]},m)</script>(here the dimension can be <script type="math/tex">(n^{[l]},1)</script> with Python’s broadcasting)<br>-<script type="math/tex">Z^{[l]}=A^{[l]}:(n^{[l]},m)</script><br>-<script type="math/tex">dZ^{[l]}=dA^{[l]}=Z^{[l]}=A^{[l]}</script></p><p><strong>cache</strong><br><em>Cache</em> is used to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backward propagation to compute derivatives.</p><p>Why deep representations?<br>Informally: There are functions you can compute with a “small” L-layer deep neural network that shallower networks require exponentially more hidden units to compute.</p><h4 id="Forward-and-Backward-Propagation"><a href="#Forward-and-Backward-Propagation" class="headerlink" title="Forward and Backward Propagation"></a>Forward and Backward Propagation</h4><p><strong>Forward propagation for layer l</strong><br>Input <script type="math/tex">a^{[l-1]}</script><br>Output <script type="math/tex">a^{[l]}</script>, cache <script type="math/tex">(z^{[l]})</script></p><script type="math/tex; mode=display">Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}</script><script type="math/tex; mode=display">A^{[l]}=g^{[l]}(Z^{[l]})</script><p><strong>Backward propagation for layer l</strong><br>Input <script type="math/tex">da^{[l]}</script><br>Output <script type="math/tex">da^{[l-1]},dW^{[l]},db^{[l]}</script></p><script type="math/tex; mode=display">dZ^{[l]}=dA^{[l]}*g^{[l]'}(Z^{[l]})</script><script type="math/tex; mode=display">dW^{[l]}=\frac{1}{m}dZ^{[l]}A^{[l-1]T}</script><script type="math/tex; mode=display">db^{[l]}=\frac{1}{m}np.sum(dZ^{[l]},axis=1,keepdims=True)</script><script type="math/tex; mode=display">dA^{[l-1]}=W^{[l]T}dZ^{[l]}</script><h4 id="Hyperparameters-and-Parameters"><a href="#Hyperparameters-and-Parameters" class="headerlink" title="Hyperparameters and Parameters"></a>Hyperparameters and Parameters</h4><p>Hyperparameters determine the final value parameters.</p><p><strong>Parameters</strong><br>· <script type="math/tex">W^{[1]},b^{[1]},W^{[2]},b^{[2]},W^{[3]},b^{[3]}...</script></p><p><strong>Hyperparameters</strong><br>· learning rate <script type="math/tex">\alpha</script><br>· number of iterations<br>· number of hidden layers <script type="math/tex">L</script><br>· number of hidden units <script type="math/tex">n^{[1]},n^{[2]},...</script><br>· choice of activation function<br>· momentum, minibatch size, regularizations, etc.</p><h2 id="Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization"><a href="#Improving-Deep-Neural-Networks-Hyperparameter-tuning-Regularization-and-Optimization" class="headerlink" title="Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization"></a>Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization</h2><h3 id="Week-One-1"><a href="#Week-One-1" class="headerlink" title="Week One"></a>Week One</h3><h4 id="Setting-up-your-Machine-Learning-Application"><a href="#Setting-up-your-Machine-Learning-Application" class="headerlink" title="Setting up your Machine Learning Application"></a>Setting up your Machine Learning Application</h4><h5 id="Train-dev-test-sets"><a href="#Train-dev-test-sets" class="headerlink" title="Train/dev/test sets"></a>Train/dev/test sets</h5><p><strong>Training set</strong>:<br>Keep on training algorithms on the training sets.</p><p><strong>Development set</strong><br>Also called <em>Hold-out cross validation set</em>, <em>Dev set</em> for short.<br>Use <em>dev set</em> to see which of many different models performs best on the <em>dev set</em>.</p><p><strong>Test set</strong><br>To get an unbiased estimate of how well your algorithm is doing.</p><p><strong>Proportion</strong><br>Previous era: the data amount is not too large, it’s common to take all the data and split it as 70%/30% or 60%/20%/20%.<br>Big data: there’re millions of examples, 10000 examples used in dev set and 10000 examples used in test set is enough. The proportion can be 98/1/1 or even 99.5/0.4/0.1</p><p><strong>Notes</strong><br>Make sure dev set and test set come from same distribution.</p><p>Not having a test set might be okay if it’s not necessary to get an unbiased estimate of performance. Though dev set is called ‘test set’ if there’s no real test set.</p><h5 id="Bias-Variance"><a href="#Bias-Variance" class="headerlink" title="Bias/Variance"></a>Bias/Variance</h5><p><strong>Solutions</strong><br>High bias:<br>Bigger network<br>Train longer<br>(Neural network architecture search)</p><p>High variance:<br>More data<br>Regularization<br>(Neural network architecture search)</p><p><strong>Bias Variance trade-off</strong><br>Originally, reducing bias may increase variance, and vice versa. So it’s necessary to trade-off between bias and variance.<br>But in deep learning, there’re ways to reduce one without increasing another. So don’t worry about bias variance trade-off.</p><h4 id="Regularization"><a href="#Regularization" class="headerlink" title="Regularization"></a>Regularization</h4><h5 id="L2-Regularization"><a href="#L2-Regularization" class="headerlink" title="L2 Regularization"></a>L2 Regularization</h5><p><strong>Logistic regression</strong></p><script type="math/tex; mode=display">J(w,b)=\frac{1}{m}\sum_{i=1}^m\mathscr{L}(\hat{y},y^{(i)})+\frac{\lambda}{2m}||w||_2^2</script><p>L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.<br>Weights end up smaller(“weight decay”): Weights are pushed to smaller values.<br><em>L2 regularization</em>: <script type="math/tex">||w||_2^2=\sum_{j=1}^{n_x}w_j^2=w^Tw</script><br>L1 regularization: <script type="math/tex">||w||_1=\sum_{j=1}^{n_x}|w_j|</script><br>(L1 regularization leads <script type="math/tex">w</script> to be sparse, but not very effictive)</p><p><strong>Neural network</strong></p><script type="math/tex; mode=display">J(w^{[1]},b^{[1]},...,w^{[L]},b^{[L]})=\frac{1}{m}\sum_{i=1}^m\mathscr{L}(\hat{y},y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L||w^{[l]}||_F^2</script><p>-<script type="math/tex">||w^{[l]}||_F^2=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^2</script>, it’s called <em>Frobenius norm</em> which is different from Euclidean distance.</p><p>Back propagation:</p><script type="math/tex; mode=display">dW^{[l]}=(from\ backprop)+\frac{\lambda}{m}W^{[l]}</script><script type="math/tex; mode=display">W^{[l]}:=W^{[l]}-\alpha dW^{[l]}</script><h5 id="Dropout-regularization"><a href="#Dropout-regularization" class="headerlink" title="Dropout regularization"></a>Dropout regularization</h5><p>With dropout, what we’re going to do is go through each of the layers of the network and set some probability of eliminating a node in neural network.<br>For each training example, you would train it using one of these neural based networks.<br>The idea behind drop-out is that at each iteration, you train a different model that uses only a subset of your neurons. With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.<br><img src="/images/DL/dropout.jpg" alt><br>Usually used in Computer Vision.</p><p><strong>Implementation with layer 3</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">d3 = np.random.rand(a3.shape[<span class="number">0</span>], a3.shape[<span class="number">1</span>]) &lt; keep_prob <span class="comment"># boolean matrix with 0/1</span></span><br><span class="line">a3 = np.multiply(a3, d3)  <span class="comment"># a3 *= d3, element-wise multiply</span></span><br><span class="line">a3 /= keep_prob <span class="comment"># ensures that the expected value of a3 remains the same. Make test time easier because of less scaling problem</span></span><br></pre></td></tr></table></figure><ul><li>Dropout is a regularization technique.</li><li>You only use dropout during training. Don’t use dropout (randomly eliminate nodes) during test time.</li><li>Apply dropout both during forward and backward propagation.</li><li>During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.</li></ul><h5 id="Other-regularization-methods"><a href="#Other-regularization-methods" class="headerlink" title="Other regularization methods"></a>Other regularization methods</h5><p><strong>Data augmentation</strong><br>Take image input for example. Flipping the image horizontally, rotating and sort of randomly zooming, distortion, etc.<br>Get more training set without paying much to reduce overfitting.</p><p><strong>Early stopping</strong><br>Stop early so that <script type="math/tex">||w||_F^2</script> is relatively small.<br>Early stopping violates <em>Orthogonalization</em>, which suggests separate <strong>Optimize cost function J</strong> and <strong>Not overfit</strong>.</p><h4 id="Optimization"><a href="#Optimization" class="headerlink" title="Optimization"></a>Optimization</h4><h5 id="Normalizing-inputs"><a href="#Normalizing-inputs" class="headerlink" title="Normalizing inputs"></a>Normalizing inputs</h5><p><strong>Subtract mean</strong></p><script type="math/tex; mode=display">\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)}</script><script type="math/tex; mode=display">x:=x-\mu</script><p><strong>Normalize variance</strong></p><script type="math/tex; mode=display">\sigma^2=\frac{1}{m}\sum_{i=1}^mx^{(i)}**2</script><script type="math/tex; mode=display">x/=\sigma^2</script><p>Note: use same <script type="math/tex">\mu,\sigma^2</script> to normalize test set.</p><p>Intuition:<br><img src="/images/DL/normalization.jpg" alt></p><h5 id="Vanishing-Exploding-gradients"><a href="#Vanishing-Exploding-gradients" class="headerlink" title="Vanishing/Exploding gradients"></a>Vanishing/Exploding gradients</h5><p>Since the number of layers in deep learning may be quite large, the product of <em>L</em> layers may tend to <script type="math/tex">\infty</script> or <script type="math/tex">0</script>. (just think about <script type="math/tex">1.001^{1000}</script> and <script type="math/tex">0.999^{1000}</script>)</p><p><strong>Weight initialization for deep networks</strong><br>Take a single neuron as example: <script type="math/tex">z=w_1x_1+w_2x_2+...+w_nx_n</script><br>If <script type="math/tex">n</script> is large, then <script type="math/tex">w_i</script> would be smaller. Our goal is to get <script type="math/tex">Var(w:)=\frac{1}{n}or\frac{2}{n}</script></p><p>Random initialization for ReLU:(known as He initialization, named for the first author of He et al., 2015.)</p><script type="math/tex; mode=display">W^{[l]}=np.random.randn(shape.)*np.sqrt(\frac{2}{n^{[l-1]}})</script><p>For tanh: use <script type="math/tex">\sqrt(\frac{1}{n^{[l-1]}})</script><br>Xavier initialization: <script type="math/tex">\sqrt(\frac{2}{n^{[l-1]}+n^{[l]}})</script></p><h5 id="Gradient-checking"><a href="#Gradient-checking" class="headerlink" title="Gradient checking"></a>Gradient checking</h5><script type="math/tex; mode=display">g(\theta)=\frac{f(\theta+\epsilon)-f(\theta-\epsilon)}{2\epsilon}</script><p>Take <script type="math/tex">W^{[1]},b^{[1]},...,W^{[L]},b^{[L]}</script> and reshape into a big vector <script type="math/tex">\theta</script>: <script type="math/tex">J(W^{[1]},b^{[1]},...,W^{[L]},b^{[L]})=J(\theta)</script><br>Take <script type="math/tex">dW^{[1]},db^{[1]},...,dW^{[L]},db^{[L]}</script> and reshape into a big vector <script type="math/tex">d\theta</script></p><p>for each i:<br>  -<script type="math/tex">d\theta_{approx}[i]=\frac{J(\theta_1,\theta_2,...,\theta_i+\epsilon,...)-J(\theta_1,\theta_2,...,\theta_i-\epsilon,...)}{2\epsilon}</script></p><p>check if <script type="math/tex">d\theta_{approx}\approx d\theta</script>?<br>Calculate <script type="math/tex">\frac{||d\theta_{approx}-d\theta||_2}{||d\theta_{approx}||_2+||d\theta||}</script>. (<script type="math/tex">10^{-7}</script> is great)</p><p><strong>Note</strong><br>Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).<br>Gradient checking is slow, so we don’t run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process.</p><ul><li>Don’t use in training - only to debug.</li><li>If algorithm fails grad check, look at components to try to identify bug.</li><li>Remember regularization.</li><li>Doesn’t work with dropout.</li><li>Run at random initialization; perhaps again after some training.</li></ul><h3 id="Week-Two-1"><a href="#Week-Two-1" class="headerlink" title="Week Two"></a>Week Two</h3><h4 id="Mini-batch-gradient-descent"><a href="#Mini-batch-gradient-descent" class="headerlink" title="Mini-batch gradient descent"></a>Mini-batch gradient descent</h4><p>Batch gradient descent (original gradient descent that we’ve known) calculates the entire training set, and just update the parameters <script type="math/tex">W,b</script> a little step. If the training set is pretty large, the training would be quite slow. And the idea of mini-batch gradient descent is use part of the training set, and update the parameters faster.<br>For example, if <script type="math/tex">X</script>‘s dimension is <script type="math/tex">(n_x,m)</script>, divide the training set into parts with dimension of <script type="math/tex">(n_x,1000)</script>, i.e. <script type="math/tex">X^{\{1\}}=[x^{(1)}\ x^{(2)}\ ...\ x^{(1000}],X^{\{2\}}=[x^{(1001)}\ x^{(1002)}\ ...\ x^{(2000)}],...</script><br>Similarly, <script type="math/tex">Y^{\{1\}}=[y^{(1)}\ y^{(2)}\ ...\ y^{(1000}],Y^{\{2\}}=[y^{(1001)}\ y^{(1002)}\ ...\ y^{(2000)}],...</script>.<br>One iteration of mini-batch gradient descent(computing on a single mini-batch) is faster than one iteration of batch gradient descent.</p><p>Two steps of mini-batch gradient descent:<br><img src="/images/DL/shuffle.jpg" alt><br><img src="/images/DL/partition.jpg" alt></p><p>repeat {<br>　for t = 1,…,5000 {<br>　　Forward prop on <script type="math/tex">X^{\{t\}}</script><br>　　　<script type="math/tex">Z^{[1]}=W^{[1]}X^{\{t\}}+b^{[1]}</script><br>　　　<script type="math/tex">A^{[1]}=g^{[1]}(Z^{[1]})</script><br>　　　…<br>　　　<script type="math/tex">A^{[L]}=g^{[L]}(Z^{[L]})</script><br>　　Compute cost <script type="math/tex">J^{\{t\}}=\frac{1}{1000}\sum_{i=1}^l\mathscr{L}(\hat{y}^{(i)},y^{(i)})+\frac{\lambda}{2*1000}\sum_l||W^{[l]}||_F^2)</script><br>　　Backprop to compute gradients cost <script type="math/tex">J^{\{t\}}</script> (using <script type="math/tex">(X^{\{t\}},Y^{\{t\}})</script>)<br>　　<script type="math/tex">W^{[l]}:=W^{[l]}-\alpha dW^{[l]},b^{[l]}:=b^{[l]}-\alpha db^{[l]}</script>　<br>　}    <em># this is called 1 epoch</em><br>}</p><p><strong>Choosing mini-batch size</strong><br>Mini-batch size = m: Batch gradient descent. <script type="math/tex">(X^{\{1\}},Y^{\{1\}})=(X,Y)</script><br>It has to process the whole training set before making progress, which takes too long for per iteration.</p><p>Mini-batch size = 1: Stochastic gradient descent. <script type="math/tex">(X^{\{1\}},Y^{\{1\}})=(X^{(1)},Y^{(1)})</script><br>It loses the benefits of vectorization across examples.</p><p>Mini-batch size in between 1 and m.<br>Fastest learning: using vectorization and make process without processing entire training set.</p><p>If training set is small(m≤2000): just use batch gradient descent.<br>Typical mini-batch sizes: 64, 128, 256, 512 (1024)</p><h4 id="Exponentially-weighted-averages"><a href="#Exponentially-weighted-averages" class="headerlink" title="Exponentially weighted averages"></a>Exponentially weighted averages</h4><script type="math/tex; mode=display">v_t=\beta v_{t-1}+(1-\beta)\theta_t</script><p>E.g.<br>-<script type="math/tex">v_{100}=0.9v_{99}+0.1\theta_{100}</script><br>-<script type="math/tex">v_{99}=0.9v_{98}+0.1\theta_{99}</script><br>-<script type="math/tex">v_{98}=0.9v_{97}+0.1\theta_{98}</script><br>-<script type="math/tex">...</script><br>Replace <script type="math/tex">v_{99}</script> with the second equation, then replace <script type="math/tex">v_{98}</script> with the third equation, and so on. Finally we’d get <script type="math/tex">v_{100}=0.1\theta_{100}+0.1*0.9\theta_{99}+0.1*0.9^2\theta_{98}+...+0.1*0.9^{99}\theta_1</script><br>This is why it is called <em>exponentially weighted averages</em>. In practice, <script type="math/tex">0.9^{10}\approx0.35\approx\frac{1}{e}</script>, thus it show an average of 10 examples.</p><p><strong>Bias correction</strong><br><img src="/images/DL/biasCorrect.jpg" alt><br>As is shown above, the purple line is exponentially weighted average without bias correction, it’s much lower than the exponentially weighted average with bias correction(green line) at the very beginning.<br>Since <script type="math/tex">v_0</script> is set to be zero(and assume <script type="math/tex">\beta=0.98</script>), the first calculation <script type="math/tex">v_1=0.98v_0+0.02\theta_1</script> has quite small result. The result is small until t gets larger(say <script type="math/tex">t=50</script> for <script type="math/tex">\beta=0.98</script>) To avoid such situation, bias correction introduces another step:</p><script type="math/tex; mode=display">\frac{v_t}{1-\beta^t}</script><h4 id="Gradient-descent-with-momentum"><a href="#Gradient-descent-with-momentum" class="headerlink" title="Gradient descent with momentum"></a>Gradient descent with momentum</h4><p>Set <script type="math/tex">v_{dW}=0,v_{db}=0</script><br>On iteration t:<br>　Compute <em>dW,db</em> on the current mini-batch<br>　<script type="math/tex">v_{dW}=\beta v_{dW}+(1-\beta)dW</script><br>　<script type="math/tex">v_{db}=\beta v_{db}+(1-\beta)db</script><br>　<script type="math/tex">W=W-\alpha v_{dW},b=b-\alpha v_{db}</script><br>　<br><img src="/images/DL/momentum.jpg" alt><br>Momentum takes past gradients into account to smooth out the steps of gradient. Gradient descent with momentum has the same idea as exponentially weighted average(while some may not use <script type="math/tex">(1-\beta)</script> in momentum). Just as the example shown above, we want slow learning horizontally and faster learning vertically. The exponentially weighted average helps to eliminate the horizontal oscillation and makes gradient descent faster. Note there’s no need for gradient descent with momentum to do bias correction. After several iterations, the algorithm will be okay.</p><h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p>On iteration t:<br>　Compute <em>dW,db</em> on the current mini-batch<br>　<script type="math/tex">s_{dW}=\beta_2S_{dW}+(1-\beta)dW^2</script><br>　<script type="math/tex">s_{db}=\beta_2S_{db}+(1-\beta)db^2</script><br>　<script type="math/tex">W:=W-\alpha\frac{dW}{\sqrt{s_{dW}+\epsilon}},b:=b-\alpha\frac{db}{\sqrt{s_{db}+\epsilon}}</script></p><p>RMS means Root Mean Square, it uses division to help to adjust gradient descent.</p><h4 id="Adam-optimization-algorithm"><a href="#Adam-optimization-algorithm" class="headerlink" title="Adam optimization algorithm"></a>Adam optimization algorithm</h4><p>Combine momentum and RMSprop together:<br>1.It calculates an exponentially weighted average of past gradients, and stores it in variable <em>v</em> (before bias correction) and <em>v_corrected</em> (with bias correction).<br>2.It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variable <em>s</em> (before bias correction) and <em>s_corrected</em> (with bias correction).<br>3.It updates parameters in a direction based on combining information from <em>1</em> and <em>2</em>.</p><p>Set <script type="math/tex">v_{dW}=0,S_{dW}=0,v_{db}=0,S_{db}=0</script><br>On iteration t:<br>　Compute <em>dW,db</em> on the current mini-batch<br>　<script type="math/tex">v_{dW}=\beta_1v_{dW}+(1-\beta_1)dW,v_{db}=\beta_1V_{db}+(1-\beta_1)db</script><br>　<script type="math/tex">s_{dW}=\beta_2s_{dW}+(1-\beta_2)dW^2,s_{db}=\beta_2S_{db}+(1-\beta_2)db^2</script><br>　<script type="math/tex">v_{dW}^{corrected}=v_{dW}/(1-\beta^t_1),v_{db}^{corrected}=v_{db}/(1-\beta^t_1)</script><br>　<script type="math/tex">s_{dW}^{corrected}=s_{dW}/(1-\beta^t_2),s_{db}^{corrected}=s_{db}/(1-\beta_2^t)</script><br>　<script type="math/tex">W:=W-\alpha\frac{V_{dW}^{corrected}}{\sqrt{S_{dW}^{corrected}}+\epsilon},b:=b-\alpha\frac{V_{db}^{corrected}}{\sqrt{S_{db}^{corrected}}+\epsilon}</script></p><p>Hyperparameters:<br>-<script type="math/tex">\alpha</script>: needs to be tune<br>-<script type="math/tex">\beta_1</script>: 0.9<br>-<script type="math/tex">\beta_2</script>: 0.999<br>-<script type="math/tex">\epsilon$:10^{-8}</script></p><p>(<strong>Adam</strong> just means Adaption moment estimation)</p><h4 id="Learning-rate-decay"><a href="#Learning-rate-decay" class="headerlink" title="Learning rate decay"></a>Learning rate decay</h4><p>Mini-batch gradient descent won’t converge, but step around at the optimal instead. To help converge, it’s advisable to decay learning rate with the number of iterations.<br>Some formula:<br>-<script type="math/tex">\alpha=\frac{1}{1+decay-rate*epoch-num}\alpha_0</script><br>-<script type="math/tex">\alpha=0.95^{epoch-num}\alpha_0</script><br>-<script type="math/tex">\alpha=\frac{k}{\sqrt{epoch-num}}\alpha_0</script><br>-discrete stair case (half α after some iterations)<br>-manual decay</p><h3 id="Week-Three-1"><a href="#Week-Three-1" class="headerlink" title="Week Three"></a>Week Three</h3><h4 id="Hyperparameter-tuning"><a href="#Hyperparameter-tuning" class="headerlink" title="Hyperparameter tuning"></a>Hyperparameter tuning</h4><p>Hyperparameters: <script type="math/tex">\alpha,\beta,\beta_1,\beta_2,\epsilon</script>, number of layers, number of units, learning rate decay, mini-batch size, etc.<br>Priority: <script type="math/tex">\alpha>\beta,\#hidden\ units,mini-batch\ size>\#layers,learning\ rate\ decay</script></p><p>Try to use random values of hyperparameters rather than grid.<br><em>Coarse to fine</em>: if finds some region with good result, try more in that region.</p><p>Appropriate scale:<br>It’s okay to sample uniformly at random for some hyperparameters: number of layers, number of units.<br>While for some hyperparameters like <script type="math/tex">\alpha,\beta</script>, instead of sampling uniformly at random, sample randomly on logarithmic scale.</p><p>Pandas &amp; Caviar<br>Panda: babysitting one model at a time<br>Caviar: training many models in parallel<br>Largely determined by the amount of computational power you can access.</p><h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>Using the idea of normalizing input, make normalization in hidden layers.</p><p>Given some intermediate value in neural network <script type="math/tex">z^{(1)},...,z^{(m)}</script>(specifically <script type="math/tex">z^{[l](i)}</script> in a single layer)<br>　<script type="math/tex">\mu=\frac{1}{m}\sum_iz^{(i)}</script><br>　<script type="math/tex">\sigma^2=\frac{1}{m}\sum_i(z^{(i)}-\mu)^2</script><br>　<script type="math/tex">z^{(i)}_{norm}=\frac{z^{(i)}-\mu}{\sqrt{\sigma^2+\epsilon}}</script><br>　<script type="math/tex">\tilde{z}^{(i)}=\gamma z_{norm}^{(i)}+\beta</script><br>Use <script type="math/tex">\tilde{z}^{[l](i)}</script> instead of <script type="math/tex">z^{[l](i)}</script></p><p><strong>Batch Norm as regularization</strong><br>Each mini-batch is scaled by the mean/variance computed on just that mini-batch.<br>This adds some noise to the values <script type="math/tex">z^{[l]}</script> within that mini-batch. So similar to dropout, it adds some noise to each hidden layer’s activations.<br>This has a slight regularization effect.</p><p>Batch Norm at test time: use exponentially weighted averages to compute average <script type="math/tex">\mu,\sigma^2</script> for test.</p><h4 id="Multi-class-classification"><a href="#Multi-class-classification" class="headerlink" title="Multi-class classification"></a>Multi-class classification</h4><p><strong>Softmax</strong><br>The output layer is a vector with dimension <em>C</em> rather than a real number. <em>C</em> is the number of classes.<br>Activation function:</p><script type="math/tex; mode=display">t=e^{(z^{[L]})}</script><script type="math/tex; mode=display">a^{[L]}=\frac{e^{z^{[L]}}}{\sum_{j=1}^Ct_j}</script><p><strong>Cost function</strong></p><script type="math/tex; mode=display">\mathscr{L}(\hat{y},y)=-\sum_{j=1}^Cy_j\log\hat{y_j}</script><script type="math/tex; mode=display">J(W^{[1]},b^{[1]},...)=\frac{1}{m}\sum_{i=1}^m\mathscr{L}(\hat{y},y)</script><h4 id="Deep-Learning-frameworks"><a href="#Deep-Learning-frameworks" class="headerlink" title="Deep Learning frameworks"></a>Deep Learning frameworks</h4><ul><li>Caffe/Caffe2</li><li>CNTK</li><li>DL4J</li><li>Keras</li><li>Lasagne</li><li>mxnet</li><li>PaddlePaddle</li><li>TensorFlow</li><li>Theano</li><li>Torch</li></ul><p><strong>Choosing deep learning frameworks</strong><br>Easy of programming (development and deployment)<br>Running speed<br>Truly Open (open source with good governance)</p><h5 id="TensorFlow"><a href="#TensorFlow" class="headerlink" title="TensorFlow"></a>TensorFlow</h5><p>Writing and running programs in TensorFlow has the following steps:</p><ol><li>Create Tensors(variables) that are not yet executed/evaluated.</li><li>Write operations between those Tensors.</li><li>Initialize your Tensors.</li><li>Create a Session.</li><li>Run the Session. This will run the operations you’d written above.</li></ol><p><code>tf.constant(...)</code>: to create a constant value<br><code>tf.placeholder(dtype = ..., shape = ..., name = ...)</code>: a placeholder is an object whose value you can specify only later</p><p><code>tf.add(..., ...)</code>: to do an addition<br><code>tf.multiply(..., ...)</code>: to do a multiplication<br><code>tf.matmul(..., ...)</code>: to do a matrix multiplication</p><p>2 typical ways to create and use sessions in TensorFlow:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line"><span class="comment"># Run the variables initialization (if needed), run the operations</span></span><br><span class="line">result = sess.run(..., feed_dict = &#123;...&#125;)</span><br><span class="line">sess.close() <span class="comment"># Close the session</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># run the variables initialization (if needed), run the operations</span></span><br><span class="line">    result = sess.run(..., feed_dict = &#123;...&#125;)</span><br><span class="line">    <span class="comment"># This takes care of closing the session</span></span><br></pre></td></tr></table></figure><h2 id="Structuring-Machine-Learning-Projects"><a href="#Structuring-Machine-Learning-Projects" class="headerlink" title="Structuring Machine Learning Projects"></a>Structuring Machine Learning Projects</h2><h3 id="Week-One-2"><a href="#Week-One-2" class="headerlink" title="Week One"></a>Week One</h3><h4 id="Orthogonalization"><a href="#Orthogonalization" class="headerlink" title="Orthogonalization"></a>Orthogonalization</h4><p>Orthogonalization or orthogonality is a system design property that assures that modifying an instruction or a component of an algorithm will not create or propagate side effects to other components of the system. It becomes easier to verify the algorithms independently from one another, and it reduces testing and development time.</p><p>When a supervised learning system is designed, these are the 4 assumptions that need to be true and orthogonal.</p><ol><li>Fit training set well on cost function - bigger network, Adam, etc</li><li>Fit dev set well on cost function - regularization, bigger training set, etc</li><li>Fit test set well on cost function - bigger dev set</li><li>Performs well in real world - change dev set or cost function</li></ol><h4 id="Single-number-evaluation-metric"><a href="#Single-number-evaluation-metric" class="headerlink" title="Single number evaluation metric"></a>Single number evaluation metric</h4><p><strong>Precision</strong></p><script type="math/tex; mode=display">Precision(\%)=\frac{True\ positive}{Number\ of\ predicted\ positive}\times100=\frac{True\ positive}{True\ positive+False\ positive}\times100</script><p>Among all the prediction, estimate how much predictions are right.</p><p><strong>Recall</strong></p><script type="math/tex; mode=display">Recall(\%)=\frac{True\ positive}{Number\ of\ predicted\ actually\ positive}\times100=\frac{True\ positive}{True\ positive+False\ negative}\times100</script><p>Among all the positive examples, estimate how much positive examples are correctly predicted.</p><p><strong>F1-Score</strong></p><script type="math/tex; mode=display">F1\_Score=\frac{2}{\frac{1}{p}+\frac{1}{r}}</script><p>The problem with using precision/recall as the evaluation metric is that you are not sure which one is better since in this case, both of them have a good precision et recall. F1-score, a harmonic mean, combine both precision and recall.</p><p><strong>Satisficing and optimizing metric</strong><br>There are different metrics to evaluate the performance of a classifier, they are called evaluation matrices. They can be categorized as satisficing and optimizing matrices. It is important to note that these evaluation matrices must be evaluated on a training set, a development set or on the test set.<br>The general rule is:</p><script type="math/tex; mode=display">N_{metric}=\begin{cases}1&\text{Optimizing metric}\\N_{metric}-1 &\text{Satisficing metric}\end{cases}</script><p>For example:</p><div class="table-container"><table><thead><tr><th>Classifier</th><th>Accuracy</th><th>Running Time</th></tr></thead><tbody><tr><td>A</td><td>90%</td><td>80ms</td></tr><tr><td>B</td><td>92%</td><td>95ms</td></tr><tr><td>C</td><td>95%</td><td>1500ms</td></tr></tbody></table></div><p>For example, there’re two evaluation metrics: accuracy and running time. Take accuracy as optimizing metric and the following(running time) as satisficing metric(s). The satisficing metric has to meet expectation set and improve the optimizing metric as much as possible.</p><h4 id="Train-Dev-Test-Set"><a href="#Train-Dev-Test-Set" class="headerlink" title="Train/Dev/Test Set"></a>Train/Dev/Test Set</h4><p>It’s important to choose the development and test sets from the same distribution and it must be taken randomly from all the data.<br><strong>Guideline</strong>: Choose a dev set and test set to reflect data you expect to get in the future and consider important to do well on.</p><p><strong>Size</strong><br>Old way of splitting data:<br>We had smaller data set, therefore, we had to use a greater percentage of data to develop and test ideas and models.<br><img src="/images/DL/oldWay.jpg" alt></p><p>Modern era - Big data:<br>Now, because a larger amount of data is available, we don’t have to compromise and can use a greater portion to train the model.<br><img src="/images/DL/modernWay.jpg" alt></p><p>Set your dev set to be big enough to detect differences in algorithms/models you’re trying out.<br>Set your test set to be big enough to give high confidence in the overall performance of your system.</p><p><strong>When to change dev/test sets and metrics</strong><br><img src="/images/DL/example.jpg" alt></p><p>Orthogonalization:<br>How to define a metric to evaluate classifiers.<br>Worry separately about how to do well on this metric.</p><p>If doing well on your metric + dev/test set does not correspond to doing well on your application, change your metric and/or dev/test set.</p><h4 id="Comparing-to-human-level-performance"><a href="#Comparing-to-human-level-performance" class="headerlink" title="Comparing to human-level performance"></a>Comparing to human-level performance</h4><p><img src="/images/DL/bayes.jpg" alt><br>The graph shows the performance of humans and machine learning over time.<br>Machine learning progresses slowly when it surpasses human-level performance. One of the reason is that human-level performance can be close to Bayes optimal error, especially for natural perception problem.<br><strong>Bayes optimal error</strong> is defined as the best possible error. In other words, it means that any functions mapping from x to y can’t surpass a certain level of accuracy(for different reasons, e.g. blurring images, audio with noise, etc).</p><p>Humans are quite good at a lot of tasks. So long as machine learning is worse than humans, you can:</p><ul><li>Get labeled data from humans</li><li>Gain insight from manual error analysis: Why did a person get this right?</li><li>Better analysis of bias/variance</li></ul><p>Human-level error as a proxy for Bayes error(i.e. Human-level error ≈ Bayes error).<br>The difference between Human-level error and training error is also regarded as <strong>“Avoidable bias”</strong>.</p><p>If the difference between human-level error and the training error is bigger than the difference between the training error and the development error. The focus should be on bias reduction technique.<br>· Train bigger model<br>· Train longer/better optimization algorithms(momentum, RMSprop, Adam)<br>· NN architecture/hyperparameters search(RNN,CNN)</p><p>If the difference between training error and the development error is bigger than the difference between the human-level error and the training error. The focus should be on variance reduction technique<br>· More data<br>· Regularization(L2, dropout, data augmentation)<br>· NN architecture/hyperparameters search</p><p>Problems where machine significantly surpasses human-level performance<br>Feature: Structured data, not natural perception, lots of data.<br>· Online advertising<br>· Product recommendations<br>· Logistics(predicting transit time)<br>· Loan approvals</p><p>The two fundamental assumptions of supervised learning:<br>You can fit the training set pretty well.(avoidable bias ≈ 0)<br>The training set performance generalizes pretty well to the dev/test set.(variance ≈ 0) </p><h3 id="Week-Two-2"><a href="#Week-Two-2" class="headerlink" title="Week Two"></a>Week Two</h3><h4 id="Error-Analysis"><a href="#Error-Analysis" class="headerlink" title="Error Analysis"></a>Error Analysis</h4><p>Spread sheet:<br>Before deciding how to improve the accuracy, set up a spread sheet find out what matters.<br>For example:</p><div class="table-container"><table><thead><tr><th>Image</th><th>Dog</th><th>Great Cat</th><th>Blurry</th><th>Comment</th></tr></thead><tbody><tr><td>1</td><td>√</td><td></td><td></td><td>small white dog</td></tr><tr><td>2</td><td></td><td>√</td><td>√</td><td>lion in rainy day</td></tr><tr><td>…</td><td></td><td></td><td></td><td></td></tr><tr><td>Percentage</td><td>5%</td><td>41%</td><td>63%</td></tr></tbody></table></div><p><strong>Mislabeled examples</strong> refer to if your learning algorithm outputs the wrong value of Y.<br><strong>Incorrectly labeled examples</strong> refer to if in the data set you have in the training/dev/test set, the label for Y, whatever a human label assigned to this piece of data, is actually incorrect.</p><p>Deep learning algorithms are quite robust to random errors in the training set, but less robust to systematic errors.</p><p>Guideline: Build system quickly, then iterate.</p><ol><li>Set up development/test set and metrics</li></ol><ul><li>Set up a target</li></ul><ol><li>Build an initial system quickly</li></ol><ul><li>Train training set quickly: Fit the parameters</li><li>Development set: Tune the parameters</li><li>Test set: Assess the performance</li></ul><ol><li>Use bias/variance analysis &amp; Error analysis to prioritize next steps</li></ol><h4 id="Mismatched-training-and-dev-test-set"><a href="#Mismatched-training-and-dev-test-set" class="headerlink" title="Mismatched training and dev/test set"></a>Mismatched training and dev/test set</h4><p>The development set and test should come from the same distribution. However, the training set’s distribution might be a bit different. Take a mobile application of cat recognizer for example:<br>The images from webpages have high resolution and are professionally framed. However, the images from app’s users are relatively low and blurrier.<br>The problem is that you have a different distribution:<br>Small data set from pictures uploaded by users. (10000)This distribution is important for the mobile app.<br>Bigger data set from the web.(200000)</p><p>Instead of mixing all the data and randomly shuffle the data set, just like below.<br><img src="/images/DL/split.jpg" alt><br>Take 5000 examples from users into training set, and halving the remaining into dev and test set.</p><p>The advantage of this way of splitting up is that the target is well defined.<br>The disadvantage is that the training distribution is different from the dev and test set distributions. However, the way of splitting the data has a better performance in long term.</p><p><strong>Training-Dev Set</strong><br>Since the distributions among the training and the dev set are different now, it’s hard to know whether the difference between training error and the training error is caused by variance or from different distributions.<br>Therefore, take a small fraction of the original training set, called training-dev set. Don’t use training-dev set for training, but to check variance.<br>The difference between the training-dev set and the dev set is called <strong>data mismatch</strong>.<br><img src="/images/DL/dataMismatch.jpg" alt></p><p>Addressing data mismatch:</p><ul><li>Carry out manual error analysis to try to understand difference between training and dev/test sets.</li><li>Make training data more similar; or collect more data similar to dev/test sets</li></ul><h4 id="Transfer-learning"><a href="#Transfer-learning" class="headerlink" title="Transfer learning"></a>Transfer learning</h4><p><img src="/images/DL/transferEg.jpg" alt></p><p>When transfer learning makes sense:</p><ul><li>Task A and B have the same input x.</li><li>You have a lot more data for Task A than Task B.</li><li>Low level features from A could be helpful for learning B.</li></ul><p>Guideline:</p><ul><li>Delete last layer of neural network</li><li>Delete weights feeding into the last output layer of the neural network</li><li>Create a new set of randomly initialized weights for the last layers only</li><li>New data set (x,y)</li></ul><p><strong>Multi-task learning</strong><br>Example: detect pedestrians, cars, road signs and traffic lights at the same time. The output is a 4-dimension vector.<br><img src="/images/DL/multi-task.jpg" alt><br>Note that the second sum(j = 1 to 4) only over value of j with 0/1 label (not ? mark).</p><p>When multi-task learning makes sense</p><ul><li>Training on a set of tasks that could benefit from having shared lower-level features.</li><li>Usually: Amount of data you have for each task is quite similar.</li><li>Can train a big enough neural network to do well on all the tasks.</li></ul><h4 id="End-to-end-deep-learning"><a href="#End-to-end-deep-learning" class="headerlink" title="End-to-end deep learning"></a>End-to-end deep learning</h4><p>End-to-end deep learning is the simplification of a processing or learning systems into one neural network.<br><img src="/images/DL/end-to-end.jpg" alt></p><p>End-to-end deep learning cannot be used for every problem since it needs a lot of labeled data. It is used mainly in audio transcripts, image captures, image synthesis, machine translation, steering in self-driving cars, etc.</p><p><strong>Pros and cons of end-to-end deep learning</strong><br>Pros:<br>Let the data speak<br>Less hand-designing of components needed</p><p>Cons:<br>May need large amount of data<br>Excludes potentially useful hand-designed components</p><h2 id="Convolutional-Neural-Networks"><a href="#Convolutional-Neural-Networks" class="headerlink" title="Convolutional Neural Networks"></a>Convolutional Neural Networks</h2><h3 id="Week-One-3"><a href="#Week-One-3" class="headerlink" title="Week One"></a>Week One</h3><p><strong>Computer Vision Problems</strong></p><ul><li>Image Classification</li><li>Object Detection</li><li>Neural Style Transfer</li></ul><h4 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h4><p><img src="/images/DL/conv.jpg" alt></p><p><code>*</code> is the operator for convolution.</p><p><strong>Filter/Kernel</strong><br>The second operand is called <em>filter</em> in the course and often called <em>kernel</em> in the research paper.<br>There’re different types of filters:<br><img src="/images/DL/filter.jpg" alt><br>Filter usually has an size of odd number. <code>1*1, 3*3, 5*5...</code>(helps to highlight the centroid)</p><p>Vertical edge detection examples<br><img src="/images/DL/edges.jpg" alt></p><p><strong>Valid and Same Convolutions</strong><br>Suppose that the original image has a size of <em>n×n</em>, the filter has a size of <em>f×f</em>, then the result has a size of <em>(n-f+1)×(n-f+1)</em>. This is called <strong>Valid convolution</strong>.<br>The size will get smaller and smaller with the process of valid convolution.</p><p>To avoid such a problem, we can use <em>paddings</em> to enlarge the original image before convolution so that output size is the same as the input size.<br>If the filter’s size is f×f, then the padding <script type="math/tex">p=\frac{f-1}{2}</script>.</p><p>The main benefits of padding are the following:<br>· It allows  you to use a CONV layer without necessarily shrinking the height and width of the volumes. This is important for building deeper networks, since otherwise the height/width would shrink as you go to deeper layers. An important special case is the “same” convolution, in which the height/width is exactly preserved after one layer.<br>· It helps us keep more of the information at the border of an image. Without padding, very few values at the next layer would be affected by pixels as the edges of an image.</p><p><strong>Stride</strong><br>The simplest stride is 1, which means that the filter moves 1 step at a time. However, the stride can be not 1. For example, moves 2 steps at a time instead. That’s called <strong>strided convolution</strong>.</p><p>Given that:<br>Size of <script type="math/tex">n\times n</script> image, <script type="math/tex">f\times f</script> filter, padding <em>p</em>, stride <em>s</em>,<br>output size:</p><script type="math/tex; mode=display">\lfloor{\frac{n+2p-f}{s}+1}\rfloor\times\lfloor{\frac{n+2p-f}{s}+1}\rfloor</script><p><strong>technical</strong><br>In mathematics and DSP, the convolution involves another “flip” step. However, this step is omitted in CNN. The “real” technical note should be “cross-correlation” rather than convolution.<br>In convention, just use Convolution in CNN. </p><p><strong>Convolution over volumes</strong><br>The 1-channel filter cannot be applied to RGB images. But we can use filters with multiple <em>channels</em>(RGB images have 3 channels).</p><p>The number of the filter’s channel should match that of the image’s channel.<br>E.g.<br>A <script type="math/tex">6\times6\times3</script> image conv with a <script type="math/tex">3\times3\times3</script> filter, the result has a size of <script type="math/tex">4\times4\times1</script>. Note that this is <strong>only</strong> 1 channel! (The number of the result’s channel corresponds to the number of the filters).</p><script type="math/tex; mode=display">n\times n\times n_c\ *\ f\times f\times n_c\to(n-f+1)\times(n-f+1)\times n_c',n_c'=\#filters</script><h4 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h4><p><strong>notation</strong><br>If layer <em>l</em> is a convolution layer:</p><ul><li><script type="math/tex">f^{[l]}</script>= filter size</li><li><script type="math/tex">p^{[l]}</script>= padding</li><li><script type="math/tex">s^{[l]}</script>= stride</li><li><script type="math/tex">n_c^{[l]}</script>= number of filters</li></ul><p>Each filter is: <script type="math/tex">f^{[l]}\times f^{[l]}\times n_c^{[l-1]}</script><br>Activations: <script type="math/tex">a^{[l]}\to n_H^{[l]}\times n_w^{[l]}\times n_c^{[l]}</script>, <script type="math/tex">A^{[l]}\to m\times n_H^{[l]}\times n_w^{[l]}\times n_c^{[l]}</script><br>Weights: <script type="math/tex">f^{[l]}\times f^{[l]}\times n_c^{[l-1]}\times n_c^{[l]}</script>,(<script type="math/tex">n_c^{[l]}</script>: #filters in layer l.)<br>bias: <script type="math/tex">n_c^{[l]}-(1,1,1,n_c^{[l]})</script></p><ul><li><script type="math/tex; mode=display">n_H^{[l]}=\lfloor{\frac{n_H^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1}\rfloor</script></li></ul><p>Input: <script type="math/tex">n_H^{[l-1]}\times n_w^{[l-1]}\times n_c^{[l-1]}</script><br>Output: <script type="math/tex">n_H^{[l]}\times n_w^{[l]}\times n_c^{[l]}</script></p><p>E.g.<br><img src="/images/DL/cnnEg.jpg" alt></p><p><strong>Types of layers in a convolutional network</strong></p><ul><li>Convolution (conv)</li><li>Pooling (pool)</li><li>Fully Connected (FC)</li></ul><p><strong>Pooling layers</strong></p><ul><li>Max pooling: slides an (f,f) window over the input and stores the max value of the window in the output.</li><li>Average pooling: slides an (f,f) window over the input and stores the average value of the window in the output.</li></ul><p><img src="/images/DL/pools.jpg" alt></p><p>Hyperparameters:<br>f: filter size<br>s: stride<br>Max or average pooling<br>Note no parameters to learn.</p><p>Suppose that the input has a size of <script type="math/tex">n_H\times n_w\times n_c</script>, then after pooling, the output has a size of <script type="math/tex">\lfloor{\frac{n_H-f}{s}+1}\rfloor\times\lfloor{\frac{n_H-f}{s}+1}\rfloor\times n_c</script></p><p>A more complicated cnn:<br><img src="/images/DL/cnnEg1.jpg" alt></p><p>Backpropagation is discussed in programming assignment.</p><p><strong>Why convolutions</strong></p><ul><li><strong>Parameter sharing</strong>: A feature detector(such as a vertical edge detector) that’s useful in one part of the image is probably useful in another part of the image.</li><li><strong>Sparsity of connections</strong>: In each layer, each output value depends only on a small number of inputs.</li></ul><h3 id="Week-Two-3"><a href="#Week-Two-3" class="headerlink" title="Week Two"></a>Week Two</h3><h4 id="Classic-networks"><a href="#Classic-networks" class="headerlink" title="Classic networks"></a>Classic networks</h4><h5 id="LeNet-5"><a href="#LeNet-5" class="headerlink" title="LeNet - 5"></a>LeNet - 5</h5><p>Paper link: <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener">Gradient-Based Learning Applied to Document Recognition</a>(IEEE has another version of this paper.)<br><img src="/images/DL/LeNet.jpg" alt></p><p>Take the input, use a 5×5 filter with 1 stride, then use an average pooling with a 2×2 filter and s = 2. Again, use a 5×5 filter with 1 stride, then use an average pooling with a 2×2 filter and s = 2. After two fully connected layer, the output uses softmax to make classification.<br>conv → pool → conv → pool → fc → fc → output<br>With the decrease of nH and nW, the number of nC is increased.</p><h5 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h5><p>Paper link: <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" target="_blank" rel="noopener">ImageNet Classification with Deep Convolutional Neural Networks</a><br><img src="/images/DL/AlexNet.jpg" alt></p><p>Similar to LeNet, but much bigger. (60K -&gt; 60M)<br>It uses ReLU.</p><h5 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h5><p>Paper link: <a href="https://arxiv.org/pdf/1409.1556.pdf" target="_blank" rel="noopener">Very Deep Convolutional Networks for Large-Scale Image Recognition</a><br><img src="/images/DL/VGG.jpg" alt><br>CONV = 3×3 filter, s = 1, same(using padding to make the size same)<br>MAX-POOL = 2×2, s = 2<br>Only use these 2 filters.</p><h5 id="Residual-Networks"><a href="#Residual-Networks" class="headerlink" title="Residual Networks"></a>Residual Networks</h5><p>Paper link: <a href="https://arxiv.org/pdf/1512.03385.pdf" target="_blank" rel="noopener">Deep residual networks for image recognition</a></p><p>In the plain network, the training error won’t keep decreasing, it may increase at some threshold. In Residual network, the training error will keep decreasing.<br>The skip-connection makes it easy for the network to learn an identity mapping between the input and the output within the ResNet block.<br><img src="/images/DL/resError.jpg" alt></p><p>In ResNets, a “shortcut” or a “skip connection” allows the gradient to be directly backpropagated to earlier layers:<br><img src="/images/DL/shortcut.jpg" alt></p><h5 id="1×1-convolution"><a href="#1×1-convolution" class="headerlink" title="1×1 convolution"></a>1×1 convolution</h5><p>Paper link: <a href="https://arxiv.org/pdf/1312.4400.pdf" target="_blank" rel="noopener">Network in network</a></p><p>If the input has a volume of dimension <script type="math/tex">n_H\times n_W\times n_C</script>, then a single 1×1 convolutional filter has <script type="math/tex">1\times1\times n_C+1</script> parameters(including bias).<br>You can use a 1×1 convolutional layer to reduce <script type="math/tex">n_C</script> but not <script type="math/tex">n_H,n_W</script>.<br>You can use a pooling layer to reduce <script type="math/tex">n_H,n_W</script>, but not <script type="math/tex">n_C</script>. </p><h5 id="Inception-network"><a href="#Inception-network" class="headerlink" title="Inception network"></a>Inception network</h5><p>Paper link: <a href="https://arxiv.org/pdf/1409.4842.pdf" target="_blank" rel="noopener">Going deeper with convolutions</a><br>Don’t bother worrying about what filters to use. Use all kinds of filters and stack them together.<br>Module:<br><img src="/images/DL/inceptionModule.jpg" alt><br><img src="/images/DL/inceptionNetwork.jpg" alt></p><p>Typically, with deeper layers, <script type="math/tex">n_H</script> and <script type="math/tex">n_W</script> decrease, while <script type="math/tex">n_C</script> increases.</p><h4 id="Practical-advices-for-using-ConvNets"><a href="#Practical-advices-for-using-ConvNets" class="headerlink" title="Practical advices for using ConvNets"></a>Practical advices for using ConvNets</h4><p>Using Open-Source Implementations: GitHub</p><p>Reasons for using open-source implementations of ConvNet:<br>Parameters trained for one computer vision task are often useful as pretraining for other computer vision tasks.<br>It is a convenient way to get working an implementation of a complex ConvNet architecture.</p><h3 id="Week-Three-2"><a href="#Week-Three-2" class="headerlink" title="Week Three"></a>Week Three</h3><h4 id="Classification-localization-and-detection"><a href="#Classification-localization-and-detection" class="headerlink" title="Classification, localization and detection"></a>Classification, localization and detection</h4><p>Image classification: Given a image, make predictions of what classification it is.<br>Classification localization: In addition, put a bounding box to figure out where the object is.<br>Detection: Multiple objects appear in the image, detect all of them.</p><p>In classification localization, the output has some values <script type="math/tex">b_x, b_y</script> which show the position of the centroid of the object,(note that the upper left corner’s coordinates is (0,0) and the lower right corner’s is (1,1)) and <script type="math/tex">b_h, b_w</script> which show the height and width of the object.<br>If the output has 3 classes, then the format of the output looks like as follows:</p><script type="math/tex; mode=display">y=\begin{bmatrix}p_c\\b_x\\b_y\\b_h\\b_w\\c_1\\c_2\\c_3\end{bmatrix}</script><p>For example, if the image contains a car, then the output is</p><script type="math/tex; mode=display">y=\begin{bmatrix}1\\b_x\\b_y\\b_h\\b_w\\0\\1\\0\end{bmatrix}</script><p>and if the image doesn’t contain anything, the output is</p><script type="math/tex; mode=display">y=\begin{bmatrix}0\\?\\?\\?\\?\\?\\?\\?\end{bmatrix}</script><p>The loss function is</p><script type="math/tex; mode=display">\mathscr{L}(\hat y,y)=\begin{cases}(\hat y_1-y_1)^2+&(\hat y_2-y_2)^2+...+(\hat y_8-y_8)^2\text{if y = 1}\\(\hat y_1-y_1)^2 &\text{if y = 0}\end{cases}</script><p>Landmark detection<br>The output contains more information about the position of the landmarks <script type="math/tex">l_x,l_y</script>.</p><p>Sliding windows detection<br>Use a small sliding window with small stride scanning the image, detect the objects. Then use a slightly bigger sliding window, and then bigger.<br>However, it has high computation cost.</p><p>Turning FC layer into convolutional layers<br>Use a filter with the same size of the last layer, the number of filters is the same as the fully connected nodes.</p><p><img src="/images/DL/slidingWindows.jpg" alt></p><h4 id="YOLO"><a href="#YOLO" class="headerlink" title="YOLO"></a>YOLO</h4><p>Paper link: <a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">You Only Look Once: Unified, Real-Time Object Detection</a></p><h5 id="Bounding-boxes"><a href="#Bounding-boxes" class="headerlink" title="Bounding boxes:"></a>Bounding boxes:</h5><p>Divide the object into several grid cells(in general grids with a size of 19×19 are common), and only detect once if the object’s midpoint is in that grid.<br>Each grid’s upper left corner has a coordinate of (0,0) and lower right corner’s (1,1). Therefore,  the value of <script type="math/tex">b_x,b_y</script> should be between (0,1). And <script type="math/tex">b_h,b_w</script> can be greater than 1.</p><h5 id="IoU"><a href="#IoU" class="headerlink" title="IoU"></a>IoU</h5><p>Intersection over union</p><script type="math/tex; mode=display">IoU=\frac{size\ of\ intersection}{size\ of\ union}</script><p>If IoU≥0.5 we can estimate that the result is right.<br>More generally, IoU is a measure of the overlap between two bounding boxes.</p><h5 id="Non-max-suppression"><a href="#Non-max-suppression" class="headerlink" title="Non-max suppression"></a>Non-max suppression</h5><p>Algorithm:<br>Each output prediction is <script type="math/tex">\begin{bmatrix}p_c\\b_x\\b_y\\b_h\\b_w\end{bmatrix}</script> (just focus on one class at a time so there’s no <script type="math/tex">c_1,c_2</script>)<br>Discard all boxes with <script type="math/tex">p_c\le0.6</script><br>While there are any remaining boxes:<br>· Pick the box with the largest <script type="math/tex">p_c</script>. Output that as a prediction.<br>· Discard any remaining box with <script type="math/tex">IoU\ge0.5</script> with the box output in the previous step.</p><h5 id="Anchor-boxes"><a href="#Anchor-boxes" class="headerlink" title="Anchor boxes"></a>Anchor boxes</h5><p>In an image, some objects may be overlapping. To predict multiple objects in one grid cell, use some anchor boxes.</p><p>Previously:<br>Each object in training image is assigned to grid cell that contains that object’s midpoint.</p><p>With two anchor boxes:<br>Each object in training image is assigned to grid cell that contains object’s midpoint and anchor box for the grid cell with highest IoU.</p><p>The output vector has a size of <script type="math/tex">\#grid\times\#grid\times\#anchors\times(1+4+classes)</script><br>E.g.</p><script type="math/tex; mode=display">y=\begin{bmatrix}p_c\\b_x\\b_y\\b_h\\b_w\\c_1\\c_2\\c_3\\p_c\\b_x\\b_y\\b_h\\b_w\\c_1\\c_2\\c_3\end{bmatrix}</script><p>(Manually choose the shape of anchor boxes.)</p><h4 id="Region-proposals"><a href="#Region-proposals" class="headerlink" title="Region proposals"></a>Region proposals</h4><p>Paper link:<a href="https://arxiv.org/pdf/1311.2524" target="_blank" rel="noopener">Rich feature hierarchies for accurate object detection and semantic segmentation</a><br>Instead using sliding windows over and over again, use segmentation algorithm to predict which regions may contain objects.</p><p>R-CNN: Propose regions. Classify proposed regions one at a time. Output label + bounding box.<br>Fast R-CNN: Propose regions. Use convolution implementation of sliding windows to classify all the proposed regions.<br>Faster R-CNN: Use convolutional network to propose regions.</p><h3 id="Week-Four-1"><a href="#Week-Four-1" class="headerlink" title="Week Four"></a>Week Four</h3><h4 id="Face-Recognition"><a href="#Face-Recognition" class="headerlink" title="Face Recognition"></a>Face Recognition</h4><p>Face verification &amp; Face recognition<br>Verification:<br>· Input image, name/ID<br>· Output whether the input image is that of the claimed person.<br>This is a 1:1 matching problem.</p><p>Recognition:<br>· Has a database of K persons<br>· Get an input image<br>· Output ID if the image is any of the K persons(or “not recognized”)<br>This is a 1:K matching problem.<br>(High demand for single accuracy.)</p><p>Face verification requires comparing a new picture against one person’s face, whereas face recognition requires comparing a new picture against K person’s faces.</p><h5 id="One-shot-learning"><a href="#One-shot-learning" class="headerlink" title="One-shot learning"></a>One-shot learning</h5><p>Learning from one example to recognize the person again. The idea is learning a “similarity” function. (A bit similar to recommendation system.)<br>d(img1, img2) = degree of difference between images.<br>If <script type="math/tex">d(img1,img2)\le\tau</script>, the output is same; else the output is different.</p><h5 id="Siamese-network"><a href="#Siamese-network" class="headerlink" title="Siamese network"></a>Siamese network</h5><p>Parameters of NN define an encoding <script type="math/tex">f(x^{(i)})</script>. (Use a vector to represent the image x)<br>Goal: Learn parameters so that<br>if <script type="math/tex">x^{(i)},x^{(j)}</script> are the same person, <script type="math/tex">||f(x^{(i)})-f(x^{(j)})||^2</script> is small;<br>if <script type="math/tex">x^{(i)},x^{(j)}</script> are different person, <script type="math/tex">||f(x^{(i)})-f(x^{(j)})||^2</script> is large.</p><h5 id="Triplet-loss"><a href="#Triplet-loss" class="headerlink" title="Triplet loss"></a>Triplet loss</h5><p>Pick an anchor image(denoted as “A”), a positive image(denoted as “P”) and a negative image(denoted as “N”).<br>We can calculate the differences between A and P, A and N.</p><script type="math/tex; mode=display">d(A,P)=||f(A)-f(P)||^2,d(A,N)=||f(A)-f(N)||^2</script><p>We want that</p><script type="math/tex; mode=display">d(A,P)=||f(A)-f(P)||^2+\alpha\le d(A,N)=||f(A)-f(N)||^2</script><p>where α is called margin.</p><p>Loss function:</p><script type="math/tex; mode=display">\mathscr{L}(A,P,N)=\max(||f(A)-f(P)||^2-||f(A)-f(N)||^2+\alpha,0)</script><script type="math/tex; mode=display">J=\sum_{i=1}^m\mathscr{L}(A^{(i)},P^{(i)},N^{(i)})</script><p>About choosing the triplets A,P,N<br>During training, if A,P,N are chosen randomly, <script type="math/tex">d(A,P)+\alpha\le d(A,N)</script> is easily satisfied. Therefore, the gradient descent wouldn’t make much progress.<br>Thus, choose triplets that are “hard” to train on. That is, pick A,P,N such that <script type="math/tex">d(A,P)\approx d(A,N)</script></p><h4 id="Neural-Style-Transfer"><a href="#Neural-Style-Transfer" class="headerlink" title="Neural Style Transfer"></a>Neural Style Transfer</h4><h5 id="Neural-style-transfer-cost-function"><a href="#Neural-style-transfer-cost-function" class="headerlink" title="Neural style transfer cost function"></a>Neural style transfer cost function</h5><p>The input contains content image(denoted as C) and style image(denoted as S), and the output is the generated image(denoted as G).</p><script type="math/tex; mode=display">J(G)=\alpha J_{content}(C,G)+\beta J_{style}(S,G)</script><p>To find the generated image G:<br>1.Initiate G randomly (e.g. init with white noise)<br>2.Use gradient descent to minimize J(G). <script type="math/tex">G:=G-\frac{\partial}{\partial G}J(G)</script></p><p><strong>Content cost function</strong></p><ul><li>Say you use hidden layer <em>l</em> to compute content cost.</li><li>Use pre-trained ConvNet. (E.g., VGG network)</li><li>Let <script type="math/tex">a^{[l](C)}</script> and <script type="math/tex">a^{[l](G)}</script> be the activation of layer <em>l</em> on the images.</li><li>If <script type="math/tex">a^{[l](C)}</script> and <script type="math/tex">a^{[l](G)}</script> are similar, both images have similar content.</li></ul><script type="math/tex; mode=display">J_{content}(C,G)=\frac{1}{2}||a^{[l](C)}-a^{[l](G)}||^2</script><p><strong>Style cost function</strong><br>Say you are using layer <em>l</em>‘s activation to measure style.<br>Define style as <em>correlation</em> between activations across channels.</p><p>Let <script type="math/tex">a^{[l]}_{i,j,k}</script> = activation at (i,j,k). <script type="math/tex">G^{[l]}</script> is <script type="math/tex">n_C^{[l]}\times n_C^{[l]}</script></p><script type="math/tex; mode=display">G^{[l](S)}_{kk'}=\sum_{i=1}^{n_H^{[l]}}\sum_{j=1}^{n_W^{[l]}}a_{i,j,k}^{[l](S)}a_{i,j,k'}^{[l](S)}</script><script type="math/tex; mode=display">G^{[l](G)}_{kk'}=\sum_{i=1}^{n_H^{[l]}}\sum_{j=1}^{n_W^{[l]}}a_{i,j,k}^{[l](G)}a_{i,j,k'}^{[l](G)}</script><p>The style matrix is also called a “Gram matrix”. In linear algebra, the Gram matrix G of a set of vectors(<script type="math/tex">v_1,...,v_n</script>) is the matrix of dot products, whose entries are <script type="math/tex">G_{ij}=v_i^Tv_j=np.dot(v_i,v_j)</script>. In other words, <script type="math/tex">G_{ij}</script> compares how similar <script type="math/tex">v_i</script> is similar to <script type="math/tex">v_j</script>: If they are highly similar, you would expect them to have a large dot product, and thus for <script type="math/tex">G_{ij}</script> to be large.</p><script type="math/tex; mode=display">J^{[l]}_{style}(S,G)=\frac{1}{2n_H^{[l]}n_W^{[l]}n_C^{[l]}}||G^{[l](S)}-G^{[l](G)}||^2_F=\frac{1}{2n_H^{[l]}n_W^{[l]}n_C^{[l]}}\sum_k\sum_{k'}(G_{kk'}^{[l](S)}-G_{kk'}^{[l](G)})^2</script><script type="math/tex; mode=display">J_{style}(S,G)=\sum_{\lambda}\lambda^{[l]}J_{style}^{[l]}(S,G)</script><p>The style of an image can be represented using the Gram matrix of a hidden layer’s activations. However, we get even better results combining this representation  from multiple different layers. This is in contrast to the content representation, where usually using just a single hidden layer is sufficient.<br>Minimizing the style cost will cause the image G to follow the style of the image S.</p><h2 id="Sequence-Model"><a href="#Sequence-Model" class="headerlink" title="Sequence Model"></a>Sequence Model</h2><h3 id="Week-One-4"><a href="#Week-One-4" class="headerlink" title="Week One"></a>Week One</h3><h4 id="Recurrent-Neural-Networks"><a href="#Recurrent-Neural-Networks" class="headerlink" title="Recurrent Neural Networks"></a>Recurrent Neural Networks</h4><p><strong>Notation</strong><br>-<script type="math/tex">x^{<t>}</script>: denotes an object at the t’th timestep.<br>-<script type="math/tex">y^{<t>}</script>: index into the output position<br>-t: implies that these are temporal sequences<br>-<script type="math/tex">T_x</script>: the length of the input sequence<br>-<script type="math/tex">T_y</script>: the length of the output sequence<br>-<script type="math/tex">T_x^{(i)}</script>: the length of the i’th training example<br>-<script type="math/tex">T_y^{(i)}</script>: the output length of the i’th training example<br>-<script type="math/tex">x^{(i)<t>}</script>: the input at the t’th timestep of example i<br>-<script type="math/tex">y^{(i)<t>}</script>: the output at the t’th timestep of example i</p><p><strong>One-hot representation</strong><br>Using a large vector(a dictionary containing tens of thousands of words) to represent a word. Only one element is one(the corresponding position of the word in the dictionary) and the others are zero.</p><p>Why not a standard network?<br>Problems:<br>Inputs, outputs can be different lengths in different examples. (Different sentences have different lengths.)<br>Doesn’t share features learned across different positions of text. (A word may appear many times in a sentence. Need to make repetitions.)</p><p><strong>RNN cell</strong><br><img src="/images/DL/RNNcell.jpg" alt><br>Basic RNN cell. Takes as input <script type="math/tex">x^{<t>}</script>(current input) and <script type="math/tex">a^{<t-1>}</script>(previous hidden state containing information from the past), and outputs <script type="math/tex">a^{<t>}</script> which is given to the next RNN cell and also used to predict <script type="math/tex">y^{<t>}</script>.</p><h4 id="Forward-Propagation"><a href="#Forward-Propagation" class="headerlink" title="Forward Propagation"></a>Forward Propagation</h4><p><img src="/images/DL/RNNforward.jpg" alt></p><script type="math/tex; mode=display">a^{<0>}=\vec{0}</script><script type="math/tex; mode=display">a^{<1>}=g_1(W_{aa}a^{<0>}+W_{ax}x^{<1>}+b_a)</script><script type="math/tex; mode=display">\hat{y}^{<1>}=g_2(W_{ya}a^{<1>}+b_y)</script><script type="math/tex; mode=display">a^{<t>}=g(W_{aa}a^{<t-1>}+W_{ax}x^{<t>}+b_a)</script><script type="math/tex; mode=display">\hat{y}^{<t>}=g(W_{ya}a^{<t>}+b_y)</script><p>Here the weight W has two subscripts: the former corresponds to the result and the latter represents the operand that it multiply by. <script type="math/tex">a\leftarrow W_{ax}x</script></p><p>The activation function <script type="math/tex">g_1()</script> usually uses tanh, sometimes ReLU.<br>The <script type="math/tex">g_2()</script> function uses sigmoid to make binary classification.</p><p>The formulas can be simplified as follows:</p><script type="math/tex; mode=display">a^{<t>}=g(W_a[a^{<t-1>},x^{<t>}]+b_a)</script><script type="math/tex; mode=display">\hat{y}^{<1>}=g_2(W_ya^{<1>}+b_y)</script><p>Here, <script type="math/tex">W_a=\begin{bmatrix}W_{aa}&|&W_{ax}\end{bmatrix}</script>, and <script type="math/tex">[a^{<t-1>},x^{<t>}]=\begin{bmatrix}a^{<t-1>}\\x^{<t>}\end{bmatrix}</script></p><h4 id="Backward-Propagation"><a href="#Backward-Propagation" class="headerlink" title="Backward Propagation"></a>Backward Propagation</h4><script type="math/tex; mode=display">\mathscr{L}^{<t>}(\hat y ^{<t>},y^{<t>})=-y^{<t>}\log\hat y^{<t>}-(1-y^{<t>})\log(1-\hat y^{<t>})</script><script type="math/tex; mode=display">\mathscr{L}(\hat y,y)=\sum_{t=1}^{T_x}\mathscr{L}^{<t>}(\hat y^{<t>},y^{<t>})</script><h4 id="Different-types"><a href="#Different-types" class="headerlink" title="Different types"></a>Different types</h4><p><strong>One to one</strong><br><img src="/images/DL/one2one.jpg" alt><br>Usage: Simple neural network</p><p><strong>One to many</strong><br><img src="/images/DL/one2many.jpg" alt><br>Usage: Music generation, sequence generation</p><p><strong>Many to one</strong><br><img src="/images/DL/many2one.jpg" alt><br>Usage: Sentiment classification</p><p><strong>Many to many (I)</strong><br><img src="/images/DL/many2many.jpg" alt><br>Usage: Name entity recognition</p><p><strong>Many to many (II)</strong><br><img src="/images/DL/many2many1.jpg" alt><br>Usage: Machine translation</p><h4 id="Language-model"><a href="#Language-model" class="headerlink" title="Language model"></a>Language model</h4><p><img src="/images/DL/languageModel.jpg" alt></p><p><unk>: unknown words (words not shown in vocabulary)</unk></p><p><eos>: end of sentence</eos></p><p>Language model is used to calculate the probability using RNN. Each layer’s output is a probability given the previous activations.<br>E.g. given the sentence <em>Cats average 15 hours of sleep a day.</em>, <script type="math/tex">\hat y^{<1>}=P(cats)</script> (the probability of ‘cats’ appears in the beginning of the sentence); <script type="math/tex">\hat y^{<2>}=P(average|cat)</script> (conditional probability);…;<script type="math/tex">\hat y^{<9>}=P(<EOS>|...)</script></p><p><strong>Character-level language model</strong><br>Instead of using words, character-level generates sequences of characters. It’s more computational.</p><h4 id="Gated-Recurrent-Unit-GRU"><a href="#Gated-Recurrent-Unit-GRU" class="headerlink" title="Gated Recurrent Unit(GRU)"></a>Gated Recurrent Unit(GRU)</h4><p>The basic RNN unit:<br><img src="/images/DL/rnnUnit.jpg" alt></p><script type="math/tex; mode=display">a^{<t>}=g(W_a[a^{<t-1>},x^{<t>}]+b_a)</script><p>g() is tanh function.</p><p>GRU(simplified):<br><img src="/images/DL/GRU.jpg" alt><br>Instead of using <script type="math/tex">a^{<t>}</script>, use <script type="math/tex">c^{<t>}</script> instead(though in GRU <script type="math/tex">a^{<t>}=c^{<t>}</script>). Here <em>c</em> represents <em>memory cell</em>.</p><script type="math/tex; mode=display">\tilde c^{<t>}=\tanh(W_c[c^{<t-1>},x^{<t>}]+b_c)</script><script type="math/tex; mode=display">\Gamma_u=\sigma(W_u[c^{<t-1>},x^{<t>}]+b_u)</script><script type="math/tex; mode=display">\Gamma_c=\sigma(W_c[c^{<t-1>},x^{<t>}]+b_c)</script><script type="math/tex; mode=display">c^{<t>}=\Gamma_u*\tilde c^{<t>}+(1-\Gamma_u)*c^{<t-1>}</script><p><em>u</em>: update. <em>r</em>: relevance.<br>Gate u is a vector of dimension equal to the number of hidden units in the LSTM.<br>Gate r tells you how relevant is c<t-1> to computing the next candidate for c<t>.</t></t-1></p><h4 id="Long-Short-Term-Memory-LSTM-Unit"><a href="#Long-Short-Term-Memory-LSTM-Unit" class="headerlink" title="Long Short Term Memory(LSTM) Unit"></a>Long Short Term Memory(LSTM) Unit</h4><p>Difference between LSTM and GRU(LSTM comes earlier, and GRU can be regarded as a special case of LSTM).<br><img src="/images/DL/LSTM.jpg" alt></p><p><img src="/images/DL/LSTMpic.jpg" alt></p><p><strong>Forget Gate</strong><br>For the sake of this illustration, lets assume we are reading words in a piece of text, and want use an LSTM to keep track of grammatical structures, such as whether the subject is singular or plural. If the subject changes from a singular word to a plural word, we need to find a way to get rid of our previously stored memory value of the singular/plural state. In an LSTM, the forget gate lets us do this:</p><script type="math/tex; mode=display">\Gamma_f^{\langle t \rangle} = \sigma(W_f[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_f)\tag{1}</script><p>Here, <script type="math/tex">W_f</script> are weights that govern the forget gate’s behavior. We concatenate  <script type="math/tex">[a^{⟨t−1⟩},x^{⟨t⟩}]</script> and multiply by <script type="math/tex">W_f</script>. The equation above results in a vector  <script type="math/tex">\Gamma_f^{\langle t \rangle}</script> with values between 0 and 1. This forget gate vector will be multiplied element-wise by the previous cell state  <script type="math/tex">c^{\langle t-1 \rangle}</script>. So if one of the values of <script type="math/tex">\Gamma_f^{\langle t \rangle}</script> is 0 (or close to 0) then it means that the LSTM should remove that piece of information (e.g. the singular subject) in the corresponding component of <script type="math/tex">c^{\langle t-1 \rangle}</script>. If one of the values is 1, then it will keep the information.</p><p><strong>Update Gate</strong><br>Once we forget that the subject being discussed is singular, we need to find a way to update it to reflect that the new subject is now plural. Here is the formulate for the update gate:</p><script type="math/tex; mode=display">\Gamma_u^{\langle t \rangle} = \sigma(W_u[a^{\langle t-1 \rangle}, x^{\{t\}}] + b_u)\tag{2}</script><p>Similar to the forget gate, here <script type="math/tex">\Gamma_u^{\langle t \rangle}</script> is again a vector of values between 0 and 1. This will be multiplied element-wise with <code>$\tilde{c}^{\langle t \rangle}$</code>, in order to compute <script type="math/tex">c^{\langle t \rangle}</script>.</p><p><strong>Updating the cell</strong><br>To update the new subject we need to create a new vector of numbers that we can add to our previous cell state. The equation we use is:</p><script type="math/tex; mode=display">\tilde{c}^{\langle t \rangle} = \tanh(W_c[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_c)\tag{3}</script><p>Finally, the new cell state is:</p><script type="math/tex; mode=display">c^{\langle t \rangle} = \Gamma_f^{\langle t \rangle}* c^{\langle t-1 \rangle} + \Gamma_u^{\langle t \rangle} *\tilde{c}^{\langle t \rangle} \tag{4}</script><p><strong>Output gate</strong><br>To decide which outputs we will use, we will use the following two formulas:</p><script type="math/tex; mode=display">\Gamma_o^{\langle t \rangle}=  \sigma(W_o[a^{\langle t-1 \rangle}, x^{\langle t \rangle}] + b_o)\tag{5}</script><script type="math/tex; mode=display">a^{\langle t \rangle} = \Gamma_o^{\langle t \rangle}* \tanh(c^{\langle t \rangle})\tag{6}</script><p>Where in equation 5 you decide what to output using a sigmoid function and in equation 6 you multiply that by the tanh of the previous state.</p><h4 id="Bidirectional-RNN-BRNN"><a href="#Bidirectional-RNN-BRNN" class="headerlink" title="Bidirectional RNN(BRNN)"></a>Bidirectional RNN(BRNN)</h4><h3 id="Week-Two-Natural-Language-Processing-amp-Word-Embeddings"><a href="#Week-Two-Natural-Language-Processing-amp-Word-Embeddings" class="headerlink" title="Week Two - Natural Language Processing &amp; Word Embeddings"></a>Week Two - Natural Language Processing &amp; Word Embeddings</h3><p>Transfer learning and word embeddings<br>1.Learn word embeddings from large text corpus. (1-100B words)<br>(Or download pre-trained embedding online.)<br>2.Transfer embedding to new task with smaller training set. (say, 100k words)<br>3.Optional: Continue to finetune the word embeddings with new data.</p><p>Computation of Similarities:<br>Cosine similarity: <script type="math/tex">sim(u,v)=\frac{u^Tv}{||u||_2||v||_2}</script><br>Euclidean distance: <script type="math/tex">||u-v||^2</script></p><p>Embedding matrix<br>The embedding matrix is denoted as <em>E</em>.<br>The embedding for word  <em>j</em> can be calculated as <script type="math/tex">e_j=E\cdot o_j</script>.<br>Here, <em>e</em> means embedding and <em>o</em> means one-hot. And in practice, we just use specialized function to look up an embedding rather than use costly matrix multiplication. </p><p>Context/target pairs<br>Context:<br>· Last 4 words<br>· 4 words on left &amp; right<br>· Last 1 word<br>· Nearby 1 word</p><h4 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h4><p>Using skip-grams:</p><script type="math/tex; mode=display">o_c\to E\to e_c\to o\to \hat{y}</script><script type="math/tex; mode=display">Softmax:p(t|c)=\frac{e^{\theta_tT_{e_c}}}{\sum_{j=1}^{10000}e^{\theta^T_je_c}}</script><script type="math/tex; mode=display">\mathscr{L}(\hat y,y)=-\sum_{i=1}^{10000}y_i\log\hat{y_i}</script><p>Here, <script type="math/tex">e_c=E\cdot o_c</script> and <script type="math/tex">\theta_t</script> is the parameter associated with output t.</p><p>Problems:<br>The cost of computation <script type="math/tex">p(t|c)=\frac{e^{\theta_tT_{e_c}}}{\sum_{j=1}^{10000}e^{\theta^T_je_c}}</script> is too high.<br>Solution:<br>Using hierarchal softmax.</p><h4 id="Negative-sampling"><a href="#Negative-sampling" class="headerlink" title="Negative sampling"></a>Negative sampling</h4><p>Randomly choose k+1 examples, where only 1 example is positive and the remaining k are negative. (The value of k is dependent on the size of data sets. If the dataset is big, k = 2-5; if the dataset is small, k = 5-20).<br>Instead of using softmax, compute k times binary classification to reduce the computation.</p><h4 id="GloVe-word-vectors"><a href="#GloVe-word-vectors" class="headerlink" title="GloVe word vectors"></a>GloVe word vectors</h4><p>-<script type="math/tex">x_{ij}</script>: the number of times <em>i</em> appears in context of <em>j</em>. Thus, <script type="math/tex">x_{ij}=x_{ji}</script></p><script type="math/tex; mode=display">minimize\sum_{i=1}^{10000}\sum_{j=1}^{10000}f(X_{ij})(\theta_i^Te_j+b_i-b_j'-\log{X_{ij}})^2</script><h4 id="Applications-using-Word-Embeddings"><a href="#Applications-using-Word-Embeddings" class="headerlink" title="Applications using Word Embeddings"></a>Applications using Word Embeddings</h4><p>Sentiment Classification and Debiasing.</p><h3 id="Week-Three-3"><a href="#Week-Three-3" class="headerlink" title="Week Three"></a>Week Three</h3><p>Machine translation can be regarded as a conditional language model.<br><img src="/images/DL/conditionalLanguageModel.jpg" alt><br>The original language model compute the probability <script type="math/tex">P(y^{<1>},...,y^{<T_y>})</script>,while the machine translation computes the probability <script type="math/tex">P(y^{<1>},...,y^{<T_y>}|x^{<1>},...,x^{<T_x>})</script>. Therefore, it can be regarded as conditional language model.<br>The machine translation contains two parts: encoder and decoder.</p><p>Just find the most likely translation.</p><script type="math/tex; mode=display">\arg \max P(y^{<1>},...,y^{<T_y>}|x)</script><p>(not use greedy search)</p><h4 id="Beam-Search"><a href="#Beam-Search" class="headerlink" title="Beam Search"></a>Beam Search</h4><p>Pick a hyperparameter <em>B</em>. In each layer of RNN, pick B most possible output.<br>Since the probability can be computed as:</p><script type="math/tex; mode=display">P(y^{<1>},...,y^{<T_y>}|x)=P(y^{<1>}|x)\times P(y^{<2>}|x,y^{<1>})\times...\times P(y^{<T_y>}|x,y^{<1>},...,y^{<T_y-1>})</script><p>(Beam search with B=1 is greedy search.)</p><p><strong>Length normalization</strong></p><script type="math/tex; mode=display">\arg\max\prod_{t=1}^{T_y}P(y^{<t>}|x,y^{<1>},...,y^{<t-1>})</script><p>The range of possibilities is [0,1]. Therefore the original formula can be extremely small with many small values’ multiplication. To avoid such situations, use log in calculations:</p><script type="math/tex; mode=display">\arg\max\sum_{t=1}^{T_y}\log P(y^{<t>}|x,y^{<1>},...,y^{<t-1>})</script><p>Machine tends to make short translation to maximize the result, while a too short translation is not satisfying. Therefore, add another hyperparameter to counteract such problem:</p><script type="math/tex; mode=display">\frac{1}{T_y^\alpha}\sum_{t=1}^{T_y}\log P(y^{<t>}|x,y^{<1>},...,y^{<t-1>})</script><p>Unlike exact search algorithms like BFS or DFS, Beam Search runs faster but is not guaranteed to find exact maximum for arg max P(y|x).</p><p><strong>Error analysis</strong><br>There’re two main models in machine translation: RNN part and Beam Search part. If the training error is high, we want to figure out which part is not functioning well.<br>Use <script type="math/tex">(y^*}</script> to represent human’s translation and <script type="math/tex">(\hat y)</script> as machine’s.</p><p>Case 1: <script type="math/tex">P(y^*|x)>P(\hat y|x)</script><br>Beam search chose <script type="math/tex">\hat y</script>. But <script type="math/tex">y^*</script> attains higher P(y|x).<br>Conclusion: Beam search is at fault.</p><p>Case 2: <script type="math/tex">P(y^*|x)\le P(\hat y|x)</script><br>In fact, <script type="math/tex">y^*</script> is a better translation than <script type="math/tex">\hat y</script>. But RNN predicted <script type="math/tex">P(y^*|x)\le P(\hat y|x)</script><br>Conclusion: RNN model is at fault.</p><h4 id="Bleu-score"><a href="#Bleu-score" class="headerlink" title="Bleu score"></a>Bleu score</h4><script type="math/tex; mode=display">p_1=\frac{\sum_{unigram\in y}count_{clip}(unigram)}{\sum_{unigram\in\hat y}count(unigram)}</script><script type="math/tex; mode=display">p_n=\frac{\sum_{ngram\in y}count_{clip}(ngram)}{\sum_{ngram\in\hat y}count(ngram)}</script><p>Here, <script type="math/tex">p_n</script>= Bleu score on n-grams only.<br>Combined Bleu score:</p><script type="math/tex; mode=display">BP\cdot \exp(\frac{1}{4}\sum_{n=1}^4p_n)</script><p>BP is brevity penalty with </p><script type="math/tex; mode=display">BP=\begin{cases}1&\text{if MT_output_length>reference_output_length}\\\text{exp(1-MT_output_length/reference_output_length)} &\text{otherwise}\end{cases}</script><h4 id="Attention-model"><a href="#Attention-model" class="headerlink" title="Attention model"></a>Attention model</h4><p><img src="/images/DL/attentionModel.jpg" alt></p><script type="math/tex; mode=display">a^{<t,t'>}=\frac{\exp(e^{<t,t'>})}{\sum_{t'=1}^{T_x}\exp(e^{<t,t'>})}</script><p>Here, <script type="math/tex">a^{<t,t'>}</script>= amount of attention <script type="math/tex">y^{<t>}</script> should pay to <script type="math/tex">a^{<t'>}</script></p><script type="math/tex; mode=display">a^{<t'>}=(\overrightarrow a^{<t'>},\overleftarrow a^{<t'>})</script><script type="math/tex; mode=display">\sum_{t'}\alpha^{<1,t'>}=1</script><script type="math/tex; mode=display">c^{<1>}=\sum_{t'}\alpha^{<1,t'>}a^{<t'>}</script>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Neural-Networks-and-Deep-Learning&quot;&gt;&lt;a href=&quot;#Neural-Networks-and-Deep-Learning&quot; class=&quot;headerlink&quot; title=&quot;Neural Networks and Deep L
      
    
    </summary>
    
      <category term="Deep Learning" scheme="http://haelchan.me/categories/Deep-Learning/"/>
    
    
      <category term="learning note" scheme="http://haelchan.me/tags/learning-note/"/>
    
  </entry>
  
  <entry>
    <title>Python Learning</title>
    <link href="http://haelchan.me/2018/02/07/python-learning/"/>
    <id>http://haelchan.me/2018/02/07/python-learning/</id>
    <published>2018-02-07T15:02:37.000Z</published>
    <updated>2019-02-01T14:37:13.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Foreword"><a href="#Foreword" class="headerlink" title="Foreword"></a>Foreword</h2><h3 id="What"><a href="#What" class="headerlink" title="What"></a>What</h3><p>Learning Python by myself.</p><p>Here’s some environment configuration:<br>Version: Python 3.6.3<br>Platform: macOS 10.12.6<br>Interpreter: terminal<br>Text editor: Sublime Text 3<br>Notebook: Jupyter Notebook</p><h3 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h3><ul><li>Self interest</li><li>Courses Prerequisites: CS229, Deep Learning, etc</li><li>(Perhaps next semester I have the chance to teach freshman something about Python:D)</li></ul><h3 id="How"><a href="#How" class="headerlink" title="How"></a>How</h3><p>Material: </p><ul><li><a href="https://docs.python.org/3/tutorial/index.html" target="_blank" rel="noopener">The Python Tutorial</a>: Official documentation</li><li><a href="http://cs231n.github.io/python-numpy-tutorial/" target="_blank" rel="noopener">Python Numpty Tutorial</a>: Quick introduction to Python and Numpy provided by Stanford</li><li><a href="https://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000" target="_blank" rel="noopener">Python教程</a>: Chinese Python tutorial</li><li><a href="https://www.codestepbystep.com" target="_blank" rel="noopener">CodeStepByStep</a>: Code practice online</li><li><a href="https://docs.python.org/3/" target="_blank" rel="noopener">Python 3.6.4 documentation</a>(It has been updated to 3.7.0 now:D)</li><li><a href="http://www.nltk.org/book/" target="_blank" rel="noopener">Natural Language Processing with Python - Analyzing Text with the Natural Language Toolkit</a> (When learning NLTK, I also improve my Python skill as well)</li></ul><p>These materials should be enough. What I use is the official tutorial to get an detailed understanding of Python and Python’s most noteworthy features, and get a good idea of the language’s flavor and style. </p><h3 id="When"><a href="#When" class="headerlink" title="When"></a>When</h3><p>I plan to get the hang of Python in my winter vacation.<br>Try to get more familiar with Python with practical applications. (And keep updating this post!)</p><h2 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h2><h3 id="Interactive-Mode-amp-Executable-mode"><a href="#Interactive-Mode-amp-Executable-mode" class="headerlink" title="Interactive Mode &amp; Executable mode"></a>Interactive Mode &amp; Executable mode</h3><p>When commands are read from a tty, the interpreter is said to be in <em>interactive mode</em>. To start interactive mode, type <code>Python3</code> in terminal(the default Python version of macOS is Python2, so type <code>Python</code> would start Python2). To stop interactive mode, type <code>exit()</code> to quit.</p><p>If code is saved as file with <code>.py</code>, then type <code>Python3 filename.py</code> in terminal to compile and run the file.</p><p>On BSD’ish Unix systems, Python scripts can be made directly executable, like shell scripts, by putting the line <code>#!/usr/bin/env python3</code>.<br>The script can be given an executable mode, or permission, using the <code>chmod +x script.py</code> command.<br>On Windows systems, there is no notion of an “executable mode”. The Python installer automatically associates <code>.py</code> files with <code>python.exe</code> so that a double-click on a Python file will run it as a script. The extension can also be <code>pyw</code>, in that case, the console window that normally appears is suppressed.</p><h3 id="Comment-amp-Prompt"><a href="#Comment-amp-Prompt" class="headerlink" title="Comment &amp; Prompt"></a>Comment &amp; Prompt</h3><p><strong>Comment</strong><br>Comments in Python start with the hash character, #, and extend to the end of the physical line.<br>The <code>#</code> sign will only comment out a single line, if it’s necessary to add multi-line comments, just begin with <code>#</code> each line. (For multi-line comments, include the whole block in a set of triple quotation marks is okay in .py file, but not interactive mode.)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This is a comment.</span></span><br><span class="line"><span class="comment"># Here's another</span></span><br></pre></td></tr></table></figure><p><strong>Prompt</strong><br>When commands are read from a tty, the interpreter is said to be in interactive mode. In this mode it prompts for the next command with the <em>primary prompt</em>, usually three greater-than signs (<code>&gt;&gt;&gt;</code>); for continuation lines it prompts with the <em>secondary prompt</em>, by default three dots (<code>...</code>).<br>Generally the prompts are primary prompt. Secondary prompts are used in control flow like <code>if</code>, <code>while</code>, etc.<br><em>Primary prompt</em> (after <code>python3</code> command in my terminal): </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ python3</span><br><span class="line">Python <span class="number">3.6</span><span class="number">.3</span> (default, Oct <span class="number">28</span> <span class="number">2017</span>, <span class="number">21</span>:<span class="number">24</span>:<span class="number">10</span>) </span><br><span class="line">[GCC <span class="number">4.2</span><span class="number">.1</span> Compatible Apple LLVM <span class="number">8.1</span><span class="number">.0</span> (clang<span class="number">-802.0</span><span class="number">.42</span>)] on darwin</span><br><span class="line">Type <span class="string">"help"</span>, <span class="string">"copyright"</span>, <span class="string">"credits"</span> <span class="keyword">or</span> <span class="string">"license"</span> <span class="keyword">for</span> more information.</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p><em>Secondary prompt</em> (example from tutorial):</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>the_world_is_flat = <span class="keyword">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">if</span> the_world_is_flat:</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">"Be careful not to fall off!"</span>)</span><br><span class="line">...</span><br><span class="line">Be careful <span class="keyword">not</span> to fall off!</span><br></pre></td></tr></table></figure><h3 id="Input-and-Output"><a href="#Input-and-Output" class="headerlink" title="Input and Output"></a>Input and Output</h3><p>Well, this part is not introduced in tutorial, but when dealing with online practice(like CodeStepByStep), it’s necessary to know how to input and output. So I’d include some information here.</p><p><strong>Input:</strong><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">It&apos;s advisable to add some prompts when asking for input, so we can add string parameters when calling `input` function. ```x = input(&quot;prompts&quot;)</span><br></pre></td></tr></table></figure></p><p>The default type of x is string. If we input an integer and what to use x as an integer, use type conversion with <code>int()</code>. <code>x = int(input(&quot;prompts&quot;))</code></p><p><strong>Output:</strong><br>Just like MATLAB, we can output a variable’s value by typing the variable’s name, or use the <code>print()</code> function.<br>When concatenating strings in <code>print()</code>, we can use both <code>,</code> and <code>+</code>. When using <code>,</code>, we don’t need to convert int/float to string, and every <code>,</code> is treated as a blank space; while we need to convert int/float to string using <code>str()</code> when using <code>+</code>, but <code>+</code> won’t add extra blank space.<br><a href="https://docs.python.org/3/library/stdtypes.html#printf-style-bytes-formatting" target="_blank" rel="noopener">printf-style</a></p><h4 id="format"><a href="#format" class="headerlink" title="format"></a>format</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">'We are the &#123;&#125; who say "&#123;&#125;!"'</span>.format(<span class="string">'knights'</span>, <span class="string">'Ni'</span>))</span><br><span class="line">We are the knights who say <span class="string">"Ni!"</span></span><br></pre></td></tr></table></figure><p>The brackets and characters within them (called format fields) are replaced with the objects passed into the <code>str.format()</code> method. </p><p><strong>Positional arguments</strong><br>A number in the brackets can be used to refer to the position of the object passed into the <code>str.format()</code> method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">'&#123;0&#125; and &#123;1&#125;'</span>.format(<span class="string">'spam'</span>, <span class="string">'eggs'</span>))</span><br><span class="line">spam <span class="keyword">and</span> eggs</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">'&#123;1&#125; and &#123;0&#125;'</span>.format(<span class="string">'spam'</span>, <span class="string">'eggs'</span>))</span><br><span class="line">eggs <span class="keyword">and</span> spam</span><br></pre></td></tr></table></figure><p><strong>Keyword arguments</strong><br>If keyword arguments are used in the <code>str.format()</code> method, their values are referred to by using the name of the argument.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">'This &#123;food&#125; is &#123;adjective&#125;.'</span>.format(</span><br><span class="line"><span class="meta">... </span>      food=<span class="string">'spam'</span>, adjective=<span class="string">'absolutely horrible'</span>))</span><br><span class="line">This spam <span class="keyword">is</span> absolutely horrible.</span><br></pre></td></tr></table></figure><p>Positional and keyword arguments can be arbitrarily combined.</p><p><strong>Old string formatting</strong><br>The <code>%</code> operator can also be used for string formatting. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> math</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">'The value of PI is approximately %5.3f.'</span> % math.pi)</span><br><span class="line">The value of PI <span class="keyword">is</span> approximately <span class="number">3.142</span>.</span><br></pre></td></tr></table></figure><p><a href="https://docs.python.org/3/library/stdtypes.html#old-string-formatting" target="_blank" rel="noopener">printf-style String Formatting</a></p><h3 id="Coding-Style"><a href="#Coding-Style" class="headerlink" title="Coding Style"></a>Coding Style</h3><p><a href="https://www.python.org/dev/peps/pep-0008/" target="_blank" rel="noopener">PEP 8 - Style Guide for Python Code</a></p><ul><li>Use 4-space indentation, and no tabs.</li><li>Wrap lines so that they don’t exceed 79 characters.</li><li>Use blank lines to separate functions and classes, and larger blocks of code inside functions.</li><li>When possible, put comments on a line of their own.</li><li>Use docstrings.</li><li>Use spaces around operators and after commas, but not directly inside bracketing constructs: <code>a = f(1, 2) + g(3, 4)</code></li><li>Name your classes and functions consistently; the convention is to <code>CamelCase</code> for classes and <code>lower_case_with_underscores</code> for functions and methods. Always use <code>self</code> as the name for the first method argument.</li><li>Don’t use fancy encodings if your code is meant to be used in international environments. Python’s default, UTF-8, or even plain ASCII work best in any case.</li><li>Likewise, don’t use non-ASCII characters in identifiers if there is only the slightest chance people speaking a different language will read or maintain the code.</li></ul><h3 id="Number"><a href="#Number" class="headerlink" title="Number"></a>Number</h3><p>Python interpreter can act as a simple calculator. So just type math expressions will get the calculation result.</p><p><strong>Data type</strong><br><code>int</code><br><code>float</code></p><p><code>Decimal</code><br><code>Fraction</code></p><p>Complex numbers: use <code>j</code> or <code>J</code> suffix to indicate the imaginary part(e.g. <code>3+5j</code>).</p><p><strong>Operation</strong><br><code>+</code>: Addition. <code>2 + 2 = 4</code><br><code>-</code>: Subtract. <code>3 - 1 = 2</code><br><code>*</code>: Multiply. <code>2 * 2 = 4</code><br><code>**</code>: Power. <code>2 ** 7 = 128</code><br><code>/</code>: Division. <strong><em>Always returns a float.</em></strong> <code>10 / 3 = 3.3333333333333335</code><br><code>//</code>: Floor division. Discard any fractional result and get an integer result.</p><p><code>_</code>: Last printed expression(easier to continue calculations).</p><h3 id="Conditions"><a href="#Conditions" class="headerlink" title="Conditions"></a>Conditions</h3><p><strong>Comparison</strong><br><code>&lt;</code> less than, <code>&gt;</code> greater than, <code>==</code> equal to, <code>&lt;=</code> less than or equal to, <code>&gt;=</code> greater than, <code>!=</code> not equal to<br><code>in</code> and <code>not in</code> check whether a value occurs (does not occur) in a sequence.<br><code>is</code> and <code>is not</code> compare whether two objects are really the same object; this only matters for mutable objects like lists.</p><p>Boolean<br><code>and</code>, <code>or</code>, <code>not</code><br><code>True</code>, <code>False</code>.<br>Any non-zero integer value is true; zero is false. The condition may also be any sequence(string, list, etc): anything with a non-zero length is true, empty sequences are false.<br>short-circuit.</p><h3 id="Strings"><a href="#Strings" class="headerlink" title="Strings"></a>Strings</h3><p>Strings in Python can be enclosed in both single quotes and double quotes. </p><p><strong>Escape character</strong><br>Just like C, <code>\</code> can be used to escape quotes in Python as well. <code>\t</code>,<code>\n</code>,etc.</p><p>Use <em>raw strings</em> by adding an <code>r</code> before the first quote can prevent <code>\</code> from being treated as special characters. Often used in regular expressions.(See notes about regular expressions <a href="http://haelchan.me/2018/05/23/regular-expression/">here</a>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(<span class="string">r'C:\some\name'</span>)</span><br><span class="line">C:\some\name</span><br></pre></td></tr></table></figure><p><strong>Concatenating</strong><br>Strings can be concatenated with <code>+</code> operator, and repeated with <code>*</code>.<br>For <em>string literals</em>(i.e. the ones enclosed between quotes) next to each other are automatically concatenated.<br>Remember to use <code>str()</code> when concatenating strings and other data types(e.g. int).<br><code>s.join()</code> can combine the words of the text into a string using <code>s</code> as the glue.</p><p><strong>Index</strong><br>Indices of strings can be non-negative(just like C array) and negative(start counting from the right).</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> +---+---+---+---+---+---+</span><br><span class="line"> | P | y | t | h | o | n |</span><br><span class="line"> +---+---+---+---+---+---+</span><br><span class="line"> 0   1   2   3   4   5   6</span><br><span class="line">-6  -5  -4  -3  -2  -1</span><br></pre></td></tr></table></figure><p><strong>Slicing</strong><br>Like MATLAB, Python supports slicing(<code>word[0:2]</code>), which allows to obtain substring.<br>Note the start is always included, and the end always excluded.<br>Default: an omitted first index defaults to zero, an omitted second index defaults to the size of the string being sliced.</p><p>Attempting to use an index that is too large will result in an error. However, out of range slice indexes are handled gracefully when used for slicing.</p><p><strong>Substring</strong><br>We test if a string contains a particular substring using the <code>in</code> operator.<br>We can also find the position of a substring with a string, using <code>find()</code>.<br><code>s.find(t)</code>: index of first instance of string <code>t</code> inside <code>s</code> (<code>-1</code> if not found)<br><code>s.rfind(t)</code>: index of last instance of string <code>t</code> inside <code>s</code> (<code>-1</code> if not found)<br><code>s.index(t)</code>: like <code>s.find(t)</code> except it raises <code>ValueError</code> if not found<br><code>s.rindex(t)</code>: like <code>s.rfind(t)</code> except it raises <code>ValueError</code> if not found</p><p><strong>Immutable</strong><br>Python strings are immutable, just as in Java. Thus, if it’s necessary to edit a string, just create a new one.</p><p><strong>Multi-lines</strong><br>This is introduced in <a href="http://www.nltk.org/book/ch03.html" target="_blank" rel="noopener">NLTK’book Chapter 3</a><br>Sometimes strings go over several lines. Python provides us with various ways of entering them. In the next example, a sequence of two strings is joined into a single string. We need to use backslash or parentheses so that the interpreter knows that the statement is not complete after the first line.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ex1 = <span class="string">"This is example using "</span>\</span><br><span class="line">      <span class="string">"backslash"</span></span><br><span class="line">ex2 = (<span class="string">"This is example using"</span></span><br><span class="line">       <span class="string">"parentheses."</span>)</span><br><span class="line"><span class="comment"># the above methods won't add newline '\n' to the string</span></span><br><span class="line"></span><br><span class="line">ex3 = <span class="string">'''hello</span></span><br><span class="line"><span class="string">         world'''</span></span><br><span class="line"><span class="comment"># Using triple-quoted string helps to add newline to the string</span></span><br></pre></td></tr></table></figure><p><strong>Others</strong><br>The built-in function <code>len()</code> returns the length of a string.<br><a href="https://docs.python.org/3.6/library/stdtypes.html?highlight=upper#string-methods" target="_blank" rel="noopener">String Methods</a></p><p><code>s.startswith(t)</code>: test if <code>s</code> starts with <code>t</code><br><code>s.endswith(t)</code>: test if <code>s</code> ends with <code>t</code><br><code>t in s</code>: test if t is a substring of s<br><code>s.islower()</code>: test if <code>s</code> contains cased characters and all are lowercase<br><code>s.isupper()</code>: test if <code>s</code> contains cased characters and all are uppercase<br><code>s.isalpha()</code>: test if <code>s</code> is non-empty and all characters in <code>s</code> are alphabetic<br><code>s.isalnum()</code>: test if <code>s</code> is non-empty and all characters in <code>s</code> are alphanumeric<br><code>s.isdigit()</code>: test if <code>s</code> is non-empty and all characters in <code>s</code> are digits<br><code>s.istitle()</code>: test if <code>s</code> contains cased characters and is titlecased (i.e. all words in <code>s</code> have initial capitals)</p><p><code>title()</code>: convert the first character in each word to uppercase and remaining characters to lowercase in string and returns new string<br><code>upper()</code>: convert all characters to uppercase<br><code>lower()</code>: convert all characters to lowercase</p><p><code>rstrip()</code>: returns a copy of the string with trailing characters removed.<br><code>lstrip()</code>: returns a copy of the string with leading characters removed.<br><code>strip()</code>: returns a copy of the string with the leading and trailing characters removed.</p><p><code>replace(t, u)</code>: replace instances of <code>t</code> with <code>u</code>.</p><h3 id="Control-Flow"><a href="#Control-Flow" class="headerlink" title="Control Flow"></a>Control Flow</h3><p><strong>if Statement</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> ... :</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">elif</span> ... :</span><br><span class="line">    ...</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure><p>There can be zero or more <code>elif</code> parts, and the <code>else</code> part is optional.<br>Note there’s no <code>switch</code> <code>case</code> in Python.</p><p><strong>for Statement</strong><br>Python’s <code>for</code> statement iterates over the items of any sequence(a list or a string), in the order that they appear in the sequence.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Measure some strings:</span></span><br><span class="line"><span class="meta">... </span>words = [<span class="string">'cat'</span>, <span class="string">'window'</span>, <span class="string">'defenestrate'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> w <span class="keyword">in</span> words:</span><br><span class="line"><span class="meta">... </span>    print(w, len(w))</span><br><span class="line"><span class="meta">... </span></span><br><span class="line">cat <span class="number">3</span></span><br><span class="line">window <span class="number">6</span></span><br><span class="line">defenestrate <span class="number">12</span></span><br></pre></td></tr></table></figure><p><strong>The range() Function</strong><br><code>for i in range(5)</code><br><code>range()</code> function may contain 1, 2 or 3 parameters.<br><code>range(term)</code>: Generate <em>term</em> values from 0 to <em>(term - 1)</em>. Note that term should be positive integers or it will return an empty list.<br><code>range(begin, end)</code>: Generate values from <em>begin</em> to <em>(end - 1)</em>.<br><code>range(begin, end, step)</code>: Specify a different increment(step), which can even be negative.</p><p>In many ways the object returned by <code>range()</code> behaves as if it is a list, but in fact it isn’t. It is an object which returns the successive items of the desired sequence when you iterate over it, but it doesn’t really make the list, thus saving space.<br>We say such an object is <strong><em>iterable</em></strong>, that is, suitable as a target for functions and constructs that expect something from which they can obtain successive items until the supply is exhausted.</p><p><strong>break and continue Statements, and else Clauses on Loops</strong></p><p>The <code>break</code> statement breaks out of the innermost enclosing <code>for</code> or <code>while</code> loop.<br>The <code>continue</code> statement continues with the next iteration of the loop.<br>These two statements are borrowed from C.</p><p>Loop statements may have an <code>else</code> clause; it is executed when the loop terminates through exhaustion of the list (with <code>for</code>) or when the condition becomes false (with <code>while</code>), but not when the loop is terminated by a <code>break</code> statement.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> n <span class="keyword">in</span> range(<span class="number">2</span>, <span class="number">10</span>):</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">2</span>, n):</span><br><span class="line"><span class="meta">... </span>        <span class="keyword">if</span> n % x == <span class="number">0</span>:</span><br><span class="line"><span class="meta">... </span>            print(n, <span class="string">'equals'</span>, x, <span class="string">'*'</span>, n//x)</span><br><span class="line"><span class="meta">... </span>            <span class="keyword">break</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">else</span>:</span><br><span class="line"><span class="meta">... </span>        <span class="comment"># loop fell through without finding a factor</span></span><br><span class="line"><span class="meta">... </span>        print(n, <span class="string">'is a prime number'</span>)</span><br><span class="line">...</span><br><span class="line"><span class="number">2</span> <span class="keyword">is</span> a prime number</span><br><span class="line"><span class="number">3</span> <span class="keyword">is</span> a prime number</span><br><span class="line"><span class="number">4</span> equals <span class="number">2</span> * <span class="number">2</span></span><br><span class="line"><span class="number">5</span> <span class="keyword">is</span> a prime number</span><br><span class="line"><span class="number">6</span> equals <span class="number">2</span> * <span class="number">3</span></span><br><span class="line"><span class="number">7</span> <span class="keyword">is</span> a prime number</span><br><span class="line"><span class="number">8</span> equals <span class="number">2</span> * <span class="number">4</span></span><br><span class="line"><span class="number">9</span> equals <span class="number">3</span> * <span class="number">3</span></span><br></pre></td></tr></table></figure><p><strong>pass Statements</strong><br>The <code>pass</code> statement does nothing. It can be used when a statement is required syntactically but the program requires no action.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line"><span class="meta">... </span>    <span class="keyword">pass</span>  <span class="comment"># Busy-wait for keyboard interrupt (⌃+C)</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>This is commonly used for creating minimal classes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="class"><span class="keyword">class</span> <span class="title">MyEmptyClass</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">pass</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><p><code>pass</code> can be used as a place-holder for a function or conditional body when you are working on new code, allowing you to keep thinking at a more abstract level.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">initlog</span><span class="params">(*args)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">pass</span>   <span class="comment"># Remember to implement this!</span></span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="Functions"><a href="#Functions" class="headerlink" title="Functions"></a>Functions</h3><p><a href="https://docs.python.org/3/glossary.html#term-function" target="_blank" rel="noopener">Detail</a><br>The keyword <code>def</code> introduces a function <em>definition</em>. It must be followed by the function name and the parenthesized list of formal parameters. The statements that form the body of the function start at the next line, and must be indented.</p><p><strong>docstring</strong><br>The first statement of the function body can optionally be a string literal(in three single quotes <code>&#39;&#39;&#39; &#39;&#39;&#39;</code>); this string literal is the function’s documentation string, or docstring. This line should begin with a capital letter and end with a period.<br>Docstrings can include a doctest block, illustrating the use of the function and the expected output. These can be tested automatically using Python’s <code>docutils</code> module. Docstrings should document the type of each parameter to the function, and the return type. At a minimum, that can be done in plain text.</p><p><strong>call by value</strong><br>The actual parameters (arguments) to a function call are introduced in the local symbol table of the called function when it is called; thus, arguments are passed using <em>call by value</em> (where the value is always an object reference, not the value of the object). When a function calls another function, a new local symbol table is created for that call.<br>Actually, <em>call by object reference</em> would be a better description, since if a mutable object is passed, the caller will see any changes the callee makes to it (items inserted into a list).</p><p><strong>return</strong><br>The <code>return</code> statement returns with a value from a function. <code>return</code> without an expression argument returns <code>None</code>. Falling off the end of a function also returns <code>None</code>.</p><p><strong>default argument values</strong><br>When defining functions, it’s useful to specify a default value for one or more arguments. This creates a function that can be called with fewer arguments than it is defined to allow.<br>Remember that the default value is evaluated only once. </p><p><strong>keyword argument &amp; positional argument</strong><br>From <a href="https://docs.python.org/3/glossary.html#term-argument" target="_blank" rel="noopener">Glossary</a><br>A value passed to a <code>function</code>(or <code>method</code>) when calling the function. There are two kinds of argument:<br>· <em>keyword argument</em>. an argument preceded by an identifier(e.g. <code>name=</code>) in a function call or passed as a value in a dictionary preceded by <code>**</code>.<br>· <em>positional argument</em>. an argument that is not a keyword argument. Positional arguments can appear at the beginning of an argument list and/or be passed as elements of an <code>iterable</code> preceded by <code>*</code>.</p><p>When a final formal parameter of the form <code>**name</code> is present, it receives a dictionary containing all keyword arguments except for those corresponding to a formal parameter. This may be combined with a formal parameter of the form <code>*name</code> (described in the next subsection) which receives a tuple containing the positional arguments beyond the formal parameter list. (<code>*name</code> must occur before <code>**name</code>.)</p><h2 id="Data-Structures"><a href="#Data-Structures" class="headerlink" title="Data Structures"></a>Data Structures</h2><p>Sequence Types - list, tuple, range<br>Set Types - set, frozenset<br>Mapping Types - dict</p><h3 id="Lists"><a href="#Lists" class="headerlink" title="Lists"></a>Lists</h3><p>List can be written as a list of comma-separated values (items) between square brackets. (similar to Java’s ArrayList)<br>Lists might contain items of different types, but usually the items all have the same type. <code>squares = [1, 4, 9, 16, 25]</code><br>It’s a good choice to name the list variable with plurals.<br>Index and slicing are similar to those of String.</p><p><strong>Mutable</strong><br>Lists are a mutable type, i.e. it is possible to change their content. Just use the index to change the content(like array in C).</p><p><strong>Concatenation</strong><br><code>+</code> operator: <code>squares + [36, 49]</code><br><code>append</code> method: <code>squares.append(64)</code></p><p><strong>Insert</strong><br>Use <code>insert()</code> to add elements in the specified location.<br>Use <code>append()</code> to add elements to the end of the list.</p><p><strong>Delete</strong><br>Use <code>del</code> if the index of element is known. <code>del a[0]</code></p><p><strong>Methods of list objects</strong><br><code>list.append(x)</code><br>Add an item to the end of the list. Equivalent to <code>a[len(a):] = [x]</code>.</p><p><code>list.extend(iterable)</code><br>Extend the list by appending all the items from the iterable. Equivalent to <code>a[len(a):] = iterable</code>.</p><p><code>list.insert(i,x)</code><br>Insert an item at a given position. The first argument is the index of the element <em>before</em> which to insert, so <code>a.insert(0, x)</code> inserts at the front of the list, and <code>a.insert(len(a), x)</code> is equivalent to <code>a.append(x)</code></p><p><code>list.remove(x)</code><br>Remove the first item from the list whose value is x. It is an error if there is no such item.</p><p><code>list.pop([i])</code><br>Remove the item at the given position in the list, and return it. If no index is specified, <code>a.pop()</code> removes and returns the last item in the list.</p><p><code>list.index(x[, start[, end]])</code><br>Return zero-based index in the list of the first item whose value is x. Raises a <code>ValueError</code> if there is no such item.<br>The optional arguments <em>start</em> and <em>end</em> are interpreted as in the slice notation and are used to limit the search to a particular subsequence of the list. The returned index is computed relative to the beginning of the full sequence rather than the <em>start</em> argument.</p><p><code>list.count(x)</code><br>Return the number of times <em>x</em> appears in the list.</p><p><code>list.sort()</code><br>Sort the items of the list in place.</p><p><code>list.reverse()</code><br>Reverse the elements of the list in place.</p><p><code>list.copy()</code><br>Return a shallow copy of the list. Equivalent to <code>a[:]</code>.</p><p><strong>List Comprehensions</strong><br>A <a href="https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions" target="_blank" rel="noopener">list comprehension</a> consists of brackets containing an expression followed by a <code>for</code> clause, then zero or more <code>for</code> of <code>if</code> clauses. The result will be a new list resulting from evaluating the expression in the context of the <code>for</code> and <code>if</code> clauses which follow it.<br>E.g:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>squares = []</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line"><span class="meta">... </span>    squares.append(x**<span class="number">2</span>)</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>squares</span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">9</span>, <span class="number">16</span>, <span class="number">25</span>, <span class="number">36</span>, <span class="number">49</span>, <span class="number">64</span>, <span class="number">81</span>]</span><br></pre></td></tr></table></figure><p>It is equivalent to <code>squares = [x**2 for x in range(10)]</code></p><p>Various ways to iterate over sequences:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> s                           <span class="comment"># iterate over the items of s</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> sorted(s)                   <span class="comment"># iterate over the items of s in order</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> set(s)                      <span class="comment"># iterate over unique elements of s</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> reversed(s)                 <span class="comment"># iterate over elements of s in reverse</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> set(s).difference(t)        <span class="comment"># iterate over elements of s not in t</span></span><br></pre></td></tr></table></figure><p>Given a sequence <code>s</code>, <code>enumerate(s)</code> returns pairs consisting of an index and the item at that index.</p><p><strong>sort</strong><br>Python list have a built-in <code>list.sort()</code> method that modifies the list in-place. There is also a <code>sorted()</code> built-in function that builds a new sorted list from an iterable.<br><code>sorted()</code> function returns a new sorted list. <code>list.sort()</code> modifies the list in-place (and returns <code>None</code> to avoid confusion). Usually it’s less convenient than <code>sorted()</code> - but if you don’t need the original list, it’s slightly more efficient.</p><p>Both <code>list.sort()</code>  and <code>sorted()</code> have a <em>key</em> parameter to specify a function to be called on each list element prior to making comparisons. A common pattern is to sort complex objects using some of the object’s indices as keys. The same technique works for objects with name attributes.<br>The key-function patterns are very common, and Python provides convenience functions to make accessor functions easier and faster. The <code>operator</code> module has <code>itemgetter()</code>, <code>attrgetter()</code> and a <code>methodcaller()</code> function.</p><p>Both <code>list.sort()</code> and <code>sorted()</code> accept a <em>reverse</em> parameter with a boolean value. This is used to flag descending sorts.</p><p>Sorts are guaranteed to be stable.<br><a href="https://docs.python.org/3/howto/sorting.html#sortinghowto" target="_blank" rel="noopener">Sorting HOWTO</a></p><h3 id="Tuples"><a href="#Tuples" class="headerlink" title="Tuples"></a>Tuples</h3><p>A tuple consists of a number of values separated by commas, and typically enclosed using parentheses.<br>Tuples are constructed using the comma operator. Parentheses are a more general feature of Python syntax, designed for grouping. A tuple containing a single element is defined by adding a trailing comma. The empty tuple is a special case, and is designed using empty parentheses<code>()</code>.</p><p><strong>Comparison with list</strong></p><div class="table-container"><table><thead><tr><th>Type</th><th>tuple</th><th>list</th></tr></thead><tbody><tr><td>Immutable</td><td>immutable</td><td>mutable</td></tr><tr><td>Element</td><td>heterogeneous</td><td>homogeneous</td></tr><tr><td>Access</td><td>unpacking/indexing</td><td>iterating</td></tr></tbody></table></div><p><strong>Packing and Unpacking</strong><br>Packing<br><code>t = 12345, 54321, &#39;hello!&#39;</code> is an example of <em>tuple packing</em></p><p>Unpacking<br><code>x, y, z = t</code> is called <em>sequence unpacking</em> and works for any sequence on the right-hand side. Sequence unpacking requires that there are as many variables on the left side of the equals sign as there are elements in the sequence. Note that multiple assignment is really just a combination of tuple packing and sequence unpacking.</p><p><code>zip()</code> takes the items of two or more sequences and “zips” them together into a single list of tuples.<br>Introduction in <a href="https://docs.python.org/3.3/library/functions.html#zip" target="_blank" rel="noopener">documentation</a>: </p><blockquote><p>Make an iterator that aggregates elements from each of the iterables.<br>Returns an iterator of tuples, where the i-th tuple contains the i-th element from each of the argument sequences or iterables. The iterator stops when the shortest input iterable is exhausted.</p></blockquote><p>Therefore, if two (or more) items have diffrent lengths, the length of the tuple is only the shortest. The trailing will be discarded. (If those values are important, use <code>itertools.zip_longest()</code> instead.)</p><h3 id="Sets"><a href="#Sets" class="headerlink" title="Sets"></a>Sets</h3><p>A <em>set</em> is an unordered collection with no duplicate elements.<br>Curly braces <code>{}</code> or the <code>set()</code> function can be used to create sets.<br><code>set()</code> can be used to remove duplicated items in the list.</p><h3 id="Dictionaries"><a href="#Dictionaries" class="headerlink" title="Dictionaries"></a>Dictionaries</h3><p>Dictionaries are index by <em>keys</em>, which can be any immutable type; strings and numbers can always be keys. Tuples can be used as keys if they contain only strings, numbers, or tuples; if a tuple contains any mutable object either directly or indirectly, it cannot be used as a key. Since lists are mutable, lists can’t be used as keys as well.</p><p>The dictionary methods <code>keys()</code>, <code>values()</code> and <code>items()</code> allow us to access the keys, values, and key-value pairs. (The type is <code>dict_keys</code>, etc. Sometimes we have to convert to list before further processing).</p><p>Python’s Dictionary Methods: A summary of commonly-used methods and idioms involving dictionaries.<br><code>d = {}</code>: create an empty dictionary and assign it to <code>d</code><br><code>d[key] = value</code>: assign a value to a given dictionary key<br><code>d.keys()</code>: the list of keys of the dictionary<br><code>list(d)</code>: the list of keys of the dictionary<br><code>sorted(d)</code>: the keys of the dictionary, sorted<br><code>key in d</code>: test whether a particular key is in the dictionary<br><code>for key in d</code>: iterate over the keys of the dictionary<br><code>d.values()</code>: the list of values in the dictionary<br><code>dict([(k1,v1), (k2,v2), ...])</code>: create a dictionary from a list of key-value pairs<br><code>d1.update(d2)</code>: add all items from d2 to <code>d1</code><br><code>defaultdict(int)</code>: a dictionary whose default value is zero</p><p><strong>Default dictionary</strong><br>If we try to access a key that is not in a dictionary, we get an error. However, it’s often useful if a dictionary can automatically create an entry for this new key and give it a default value, such as zero or the empty list. For this reason, a special kind of dictionary called a <code>defaultdict</code> is available. In order to use it, we have to supply a parameter which can be used to create the default value, e.g. <code>int</code>, <code>float</code>, <code>str</code>, <code>list</code>, <code>dict</code>, <code>tuple</code>.<br>If the parameter is <code>None</code>, it’s just just like the original <code>dict</code>. And We can specify any default value we like using lambda expression.</p><h3 id="Some-advanced-data-structures"><a href="#Some-advanced-data-structures" class="headerlink" title="Some advanced data structures"></a>Some advanced data structures</h3><h4 id="Deque"><a href="#Deque" class="headerlink" title="Deque"></a>Deque</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">deque([iterable[, maxlen]])</span><br></pre></td></tr></table></figure><p><a href="https://docs.python.org/3/library/collections.html#deque-objects" target="_blank" rel="noopener">Deques</a> are a generalization of stacks and queues. Deques support thread-safe, memory efficient appends and pops from either side of the deque with approximately the sam O(1) performance in either direction.<br>The constructor returns a new deque object initialized left-to-right with data from <em>iterable</em>.<br>If <em>iterable</em> is not specified, the new deque is empty.<br>If <em>maxlen</em> is not specified or is <code>None</code>, deques may grow to an arbitrary length. Otherwise, the deque is bounded to the specified maximum length. Once a bounded length deque is full, when new items are added, a corresponding number of items are discarded from the opposite end. (Can be used for keeping last N items)</p><p>Methods: <code>append()</code>, <code>appendleft()</code>, <code>pop()</code>, <code>popleft()</code>, etc.</p><h4 id="Heapq"><a href="#Heapq" class="headerlink" title="Heapq"></a>Heapq</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br></pre></td></tr></table></figure><p>The module provides an implementation of the heap queue algorithm. The implementation uses arrays for which <code>heap[k] &lt;= heap[2*k+1]</code> and <code>heap[k] &lt;= heap[2*k+2]</code> for all k, counting elements from zero.<br>Python implementation uses zero-based indexing. This makes the relationship between the index for a node and the indices for its children slightly less obvious. And the pop method returns the smallest item.</p><p>Functions:<br><code>heapq.heappush(heap, item)</code>, <code>heapq.heappop(heap)</code>, <code>heapq.heapify(x)</code>, <code>heapq.nlargest(n, iterable, key=None)</code>, <code>heapq.nsmallest(n, iterable, key=None)</code>.<br>The latter two functions perform best for smaller values of n. For larger values, it is more efficient to use the <code>sorted()</code> function (with slicing). Also, when <code>n==1</code>, it is more efficient to use the built-in <code>min()</code> and <code>max()</code> functions.</p><h2 id="Modules"><a href="#Modules" class="headerlink" title="Modules"></a>Modules</h2><p>A <em>module</em> is a file containing Python definitions and statements. The file name is the module name with the suffix <code>.py</code> appended. Definitions from a module can be <em>imported</em> into other modules or into the <em>main</em> module (the collection of variables that you have access to in a script executed at the top level and in calculator mode). Within a module, the module’s name (as a string) is available as the value of the global variable <code>__name__</code>.</p><h2 id="Files"><a href="#Files" class="headerlink" title="Files"></a>Files</h2><h3 id="open"><a href="#open" class="headerlink" title="open"></a>open</h3><p><code>open(filename, mode)</code><br><code>open()</code> returns a file object. The first argument is a string containing the filename. The second argument is another string containing a few characters describing the way in which the file will be used. <em>mode</em> can be <code>&#39;r&#39;</code> when the file will only be read, <code>&#39;w&#39;</code> for only writing (an existing file with the same name will be erased), and <code>&#39;a&#39;</code> opens the file for appending; any data written to the file is automatically added to the end. <code>&#39;r+&#39;</code> opens the file for both reading and writing. The mode argument is optional; <code>&#39;r&#39;</code> will be assumed if it’s omitted.</p><p>It is good practice to use the <code>with</code> keyword when dealing with file objects. (similar to <code>with tf.Session() as sess:</code> in TensorFlow). The advantage is that the file is properly closed after its suite finishes, even if an exception is raised at some point. Using with is also much shorter than writing equivalent try-finally blocks:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> open(<span class="string">'workfile'</span>) <span class="keyword">as</span> f:</span><br><span class="line"><span class="meta">... </span>    read_data = f.read()</span><br></pre></td></tr></table></figure><p>If not using <code>with</code> keyword, just call <code>f.close()</code> to close the file and immediately free up any system resources used by it. </p><h3 id="read-amp-write"><a href="#read-amp-write" class="headerlink" title="read &amp; write"></a>read &amp; write</h3><p><code>f.read(size)</code><br>Reads some quantity of data and returns it as a string (in text mode) or bytes object(in binary mode). <em>size</em> is an optional numeric argument. When <em>size</em> is omitted or negative, the entire contents of the file will be read and returned. Otherwise, at most <em>size</em> bytes are read and returned. If the end of the file has been reached, <code>f.read()</code> will return an empty string(<code>&#39;&#39;</code>).</p><p><code>f.readline()</code><br>Reads a single line from the file; a newline character(<code>\n</code>) is left at the end of the string, and is only omitted on the last line of the file if the file doesn’t end in a newline. This makes the return value unambiguous: if <code>f.readline()</code> returns an empty string, the end of the file has been reached, while a blank line is represented by <code>&#39;\n&#39;</code>, a string containing only a single newline.</p><p>Looping over the file object:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line"><span class="meta">... </span>    print(line, end=<span class="string">''</span>)</span><br></pre></td></tr></table></figure><p>Read all the lines of a file in a list: <code>list(f)</code> or <code>f.readlines()</code>.</p><p><code>f.write(string)</code> writes the contents of string to the file, returning the number of characters written. Other types of objects need to be converted - either to a string (in text mode) or a bytes object (in binary mode) - before writing them.</p><h3 id="File-System"><a href="#File-System" class="headerlink" title="File System"></a>File System</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.listdir()</span><br></pre></td></tr></table></figure><p>It will return a list of the file name in the current directory.</p><h2 id="Exceptions"><a href="#Exceptions" class="headerlink" title="Exceptions"></a>Exceptions</h2><p>Errors detected during execution are called <em>exceptions</em>.<br>Exceptions come in different types, and the type is printed as part of the message. The string printed as the exception type is the name of the built-in exception that occurred.(true for all built-in exceptions, but need not be true for user-defined exceptions) </p><p>All built-in exceptions are listed <a href="https://docs.python.org/3/library/exceptions.html#bltin-exceptions" target="_blank" rel="noopener">here</a></p><h3 id="Handling-Exceptions"><a href="#Handling-Exceptions" class="headerlink" title="Handling Exceptions"></a>Handling Exceptions</h3><p>The <code>try</code> statement works as follows.</p><ul><li>First, the <em>try clause</em> is executed.</li><li>If no exceptions occurs, the <em>except clause</em> is skipped and execution of the <code>try</code> statement is finished.</li><li>If an exception occurs during execution of the try clause, the rest of the clause is skipped. Then if its type matches the exception named after the <code>except</code> keyword, the except clause is executed, and then execution continues after the <code>try</code> statement.</li><li>If an exception occurs which does not match the exception named in the except clause, it is passed on to outer <code>try</code> statements; if no handlers is found, it is an <em>unhandled exception</em> and execution stops with a message.</li></ul><p>A <code>try</code> statement may have more than one except clause, to specify handlers for different exceptions. At most one handler will be executed. Handlers only handle exceptions that occur in the corresponding try clause, not in other handlers of the same try statement. An except clause may name multiple exceptions as a parenthesized tuple.</p><h2 id="Objects"><a href="#Objects" class="headerlink" title="Objects"></a>Objects</h2><p>Assignment always copies the value of an expression, but a value is not always what you might expect it to be. In particular, the “value” of a structured object such as a list is actually just a <em>reference</em> to the object.<br>Python provides two ways to check that a pair of items are the same. <code>==</code> and <code>is</code>. The <code>is</code> operator tests for object identity.<br>We can use <code>id()</code> function to find out the numerical identifier for any object.</p><h2 id="Some-tips"><a href="#Some-tips" class="headerlink" title="Some tips"></a>Some tips</h2><h3 id="The-Zen-of-Python"><a href="#The-Zen-of-Python" class="headerlink" title="The Zen of Python"></a>The Zen of Python</h3><p><code>import this</code></p><blockquote><p>The Zen of Python, by Tim Peters</p><p>Beautiful is better than ugly.<br>Explicit is better than implicit.<br>Simple is better than complex.<br>Complex is better than complicated.<br>Flat is better than nested.<br>Sparse is better than dense.<br>Readability counts.<br>Special cases aren’t special enough to break the rules.<br>Although practicality beats purity.<br>Errors should never pass silently.<br>Unless explicitly silenced.<br>In the face of ambiguity, refuse the temptation to guess.<br>There should be one— and preferably only one —obvious way to do it.<br>Although that way may not be obvious at first unless you’re Dutch.<br>Now is better than never.<br>Although never is often better than <em>right</em> now.<br>If the implementation is hard to explain, it’s a bad idea.<br>If the implementation is easy to explain, it may be a good idea.<br>Namespaces are one honking great idea — let’s do more of those!</p></blockquote><hr><p>The keyword argument <code>end</code> can be used to avoid the newline after the output, or end the output with a different string.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a, b = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">while</span> b &lt; <span class="number">1000</span>:</span><br><span class="line"><span class="meta">... </span>    print(b, end=<span class="string">','</span>)</span><br><span class="line"><span class="meta">... </span>    a, b = b, a+b</span><br><span class="line">...</span><br><span class="line"><span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">8</span>,<span class="number">13</span>,<span class="number">21</span>,<span class="number">34</span>,<span class="number">55</span>,<span class="number">89</span>,<span class="number">144</span>,<span class="number">233</span>,<span class="number">377</span>,<span class="number">610</span>,<span class="number">987</span>,</span><br></pre></td></tr></table></figure><hr><p><code>in</code> keyword tests whether or not a sequence contains a certain value.</p><hr><p>The square brackets in the method signature denote that the parameter is optional, not that you should type square brackets at that position. This is frequent in the Python Library Reference.</p><hr><h3 id="Looping-Techniques"><a href="#Looping-Techniques" class="headerlink" title="Looping Techniques"></a>Looping Techniques</h3><p>When looping through dictionaries, the key and corresponding value can be retrieved at the same time using the <code>items()</code> method.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>knights = &#123;<span class="string">'gallahad'</span>: <span class="string">'the pure'</span>, <span class="string">'robin'</span>: <span class="string">'the brave'</span>&#125;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> k, v <span class="keyword">in</span> knights.items():</span><br><span class="line"><span class="meta">... </span>    print(k, v)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>When looping through a sequence, the position index and corresponding value can be retrieved at the same time using the <code>enumerate()</code> function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i, v <span class="keyword">in</span> enumerate([<span class="string">'tic'</span>, <span class="string">'tac'</span>, <span class="string">'toe'</span>]):</span><br><span class="line"><span class="meta">... </span>    print(i, v)</span><br><span class="line">...</span><br><span class="line"><span class="number">0</span> tic</span><br><span class="line"><span class="number">1</span> tac</span><br><span class="line"><span class="number">2</span> toe</span><br></pre></td></tr></table></figure><p>To loop over two or more sequence at the same time, the entries can be paired with the <code>zip()</code> function.</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>questions = [<span class="string">'name'</span>, <span class="string">'quest'</span>, <span class="string">'favorite color'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>answers = [<span class="string">'lancelot'</span>, <span class="string">'the holy grail'</span>, <span class="string">'blue'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> q, a <span class="keyword">in</span> zip(questions, answers):</span><br><span class="line"><span class="meta">... </span>    print(<span class="string">'What is your &#123;0&#125;?  It is &#123;1&#125;.'</span>.format(q, a))</span><br><span class="line">...</span><br><span class="line">What is your name?  It is lancelot.</span><br><span class="line">What is your quest?  It is the holy grail.</span><br><span class="line">What is your favorite color?  It is blue.</span><br></pre></td></tr></table></figure><p>To loop over a sequence in reverse, first specify the sequence in a forward direction and then call the <code>reversed()</code> function.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> i <span class="keyword">in</span> reversed(range(<span class="number">1</span>, <span class="number">10</span>, <span class="number">2</span>)):</span><br><span class="line"><span class="meta">... </span>    print(i)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>To loop over a sequence in sorted order, use the <code>sorted()</code> function which returns a new sorted list while leaving the source unaltered.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>basket = [<span class="string">'apple'</span>, <span class="string">'orange'</span>, <span class="string">'apple'</span>, <span class="string">'pear'</span>, <span class="string">'orange'</span>, <span class="string">'banana'</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">for</span> f <span class="keyword">in</span> sorted(set(basket)):</span><br><span class="line"><span class="meta">... </span>    print(f)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h3 id="Eval"><a href="#Eval" class="headerlink" title="Eval"></a>Eval</h3><p><a href="https://docs.python.org/3/library/functions.html#eval" target="_blank" rel="noopener">Eval: Built-in Function</a><br>The return value is the result of the evaluated expression. Syntax errors are reported as exceptions.<br>This function can also be used to execute arbitrary code objects (such as those created by <code>compile()</code>). In this case pass a code object instead of a string.</p><h3 id="Conversion-between-String-and-List"><a href="#Conversion-between-String-and-List" class="headerlink" title="Conversion between String and List"></a>Conversion between String and List</h3><p>The following lines make the most common conversions between string and list.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s = <span class="string">'some words or sentences'</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># String to list</span></span><br><span class="line">word = s.split()</span><br><span class="line"></span><br><span class="line"><span class="comment"># List to string</span></span><br><span class="line">restore = <span class="string">' '</span>.join(word)</span><br></pre></td></tr></table></figure><p>For more information, refer to documentation of <a href="https://docs.python.org/3/library/stdtypes.html?highlight=split#str.split" target="_blank" rel="noopener">split()</a> and <a href="https://docs.python.org/3/library/stdtypes.html?highlight=join#str.join" target="_blank" rel="noopener">join()</a>. The following section <em>split with multiple delimiters</em> is also helpful.</p><h3 id="Split-with-multiple-delimiters"><a href="#Split-with-multiple-delimiters" class="headerlink" title="Split with multiple delimiters"></a>Split with multiple delimiters</h3><p>See my post <a href="http://haelchan.me/2018/05/23/regular-expression/">Regular Expression</a>.</p><h3 id="Execution-time-measurement"><a href="#Execution-time-measurement" class="headerlink" title="Execution time measurement"></a>Execution time measurement</h3><p><strong>Measure execution time of small code snippets</strong><br><a href="https://docs.python.org/3.7/library/timeit.html?highlight=timer" target="_blank" rel="noopener">timeit</a><br><code>timeit</code> provides a simple way to time small bits of Python code.<br>Command-Line Interface:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ python3 -m timeit &apos;&quot;-&quot;.join(str(n) for n in range(100))&apos;</span><br><span class="line">10000 loops, best of 5: 30.2 usec per loop</span><br><span class="line">$ python3 -m timeit &apos;&quot;-&quot;.join([str(n) for n in range(100)])&apos;</span><br><span class="line">10000 loops, best of 5: 27.5 usec per loop</span><br><span class="line">$ python3 -m timeit &apos;&quot;-&quot;.join(map(str, range(100)))&apos;</span><br><span class="line">10000 loops, best of 5: 23.2 usec per loop</span><br></pre></td></tr></table></figure><p>Python Interface:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> timeit</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>timeit.timeit(<span class="string">'"-".join(str(n) for n in range(100))'</span>, number=<span class="number">10000</span>)</span><br><span class="line"><span class="number">0.3018611848820001</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>timeit.timeit(<span class="string">'"-".join([str(n) for n in range(100)])'</span>, number=<span class="number">10000</span>)</span><br><span class="line"><span class="number">0.2727368790656328</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>timeit.timeit(<span class="string">'"-".join(map(str, range(100)))'</span>, number=<span class="number">10000</span>)</span><br><span class="line"><span class="number">0.23702679807320237</span></span><br></pre></td></tr></table></figure><p>(In NLTK, it is introduced to use <code>timeit.Timer</code>, but that’s not necessary. <code>timeit.timeit()</code> will automatically create a Timer instance.)<br><code>timeit.timeit(stmt=&#39;pass&#39;, setup=&#39;pass&#39;, timer=&lt;default timer&gt;, number=1000000, globals=None)</code></p><p>The <code>global</code> parameter is new in Python 3.5. Passing <code>globals()</code> to the <em>global</em> parameter will cause the code to be executed within the current global namespcae. This can be more convenient than individually specifying imports.</p><p><strong>General</strong><br><a href="https://docs.python.org/3.7/library/time.html?highlight=timer#time.process_time" target="_blank" rel="noopener">https://docs.python.org/3.7/library/time.html?highlight=timer#time.process_time</a><br>Use <code>time.process_time()</code> method (which is new in Python 3.3) to record the beginning and end time. </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line">start = time.process_time()</span><br><span class="line"><span class="comment"># code execution</span></span><br><span class="line">end = time.process_time()</span><br><span class="line">print(end - start)</span><br></pre></td></tr></table></figure><p>And for accuracy, Python 3.7 introduces <code>process_time_ns()</code> which return time as nanoseconds.</p><h3 id="Python-2-to-3"><a href="#Python-2-to-3" class="headerlink" title="Python 2 to 3"></a>Python 2 to 3</h3><p>2to3 is a Python program that reads Python 2.x source code and applies a series of fixers to transform it into valid Python 3.x code. 2to3 will usually be installed with the Python interpreter as a script.<br>2to3’s basic arguments are a list of files or directories to transform.<br><code>2to3 example.py</code><br>A diff against the original source file is printed. 2to3 can also write the needed modifications right back to the source file. Writing the changes back is enabled with the <em>-w</em> flag:<br><code>2to3 -w example.py</code><br><a href="https://docs.python.org/3.1/library/2to3.html" target="_blank" rel="noopener">2to3 document</a></p><h3 id="Get-all-indicies-of-items-with-multiple-occurrence-in-a-list"><a href="#Get-all-indicies-of-items-with-multiple-occurrence-in-a-list" class="headerlink" title="Get all indicies of items with multiple occurrence in a list"></a>Get all indicies of items with multiple occurrence in a list</h3><p><code>index()</code> will return index in the list of the first item. But what if we need the all indices of an item with multiple occurrence? It seems that Python doesn’t provide a handy method. A common solution is:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">indices = [i <span class="keyword">for</span> i, x <span class="keyword">in</span> enumerate(my_list) <span class="keyword">if</span> x == <span class="string">"whatever"</span>]</span><br></pre></td></tr></table></figure><h3 id="Convert-all-strings-in-a-list-to-int"><a href="#Convert-all-strings-in-a-list-to-int" class="headerlink" title="Convert all strings in a list to int"></a>Convert all strings in a list to int</h3><p>Use map function</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">results = list(map(int, results))</span><br></pre></td></tr></table></figure><p>or list comprehension</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">results = [int(i) <span class="keyword">for</span> i <span class="keyword">in</span> results]</span><br></pre></td></tr></table></figure><h3 id="Round-off-in-Python"><a href="#Round-off-in-Python" class="headerlink" title="Round off in Python"></a>Round off in Python</h3><p>There are some methods in Python to round off float number:</p><p><a href="https://docs.python.org/3.7/library/functions.html?highlight=round#round" target="_blank" rel="noopener">round(<em>number</em>[, <em>ndigits</em>])</a> return <em>number</em> rounded to <em>ndigits</em> precision after the decimal point. If <em>ndigits</em> is omitted or is <code>None</code>, it returns the nearest integer to its input.  </p><p>For the built-in types supporting round(), values are rounded to the closest multiple of 10 to the power minus <em>ndigits</em>; if two multiples are equally close, rounding is done toward the even choice (<a href="http://en.wikipedia.org/wiki/Rounding#Round_half_to_even" target="_blank" rel="noopener">bankers’ rounding</a>).</p><blockquote><p>Note: The behavior of <code>round()</code> for floats can be surprising: for example, <code>round(2.675, 2)</code>gives <code>2.67</code> instead of the expected <code>2.68</code>. This is not a bug: it’s a result of the fact that most decimal fractions can’t be represented exactly as a float. See <a href="https://docs.python.org/3.7/tutorial/floatingpoint.html#tut-fp-issues" target="_blank" rel="noopener">Floating Point Arithmetic: Issues and Limitations</a> for more information.</p></blockquote><p><code>print(&quot;%.1f&quot; % number)</code> is used when printing the number out. It seems that it also observes bankers’ rounding.</p><p>To make it more accurate, we can use <a href="https://docs.python.org/3/library/decimal.html" target="_blank" rel="noopener"><code>decimal()</code></a> module. (Referred from <a href="https://stackoverflow.com/questions/9301690/python-float-round-error-117-285-round-to-117-28-not-117-29/9301754#9301754" target="_blank" rel="noopener">Stack Overflow</a>)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> decimal</span><br><span class="line">a = decimal.Decimal(<span class="string">'117.285'</span>)</span><br><span class="line">rounded = a.quantize(decimal.Decimal(<span class="string">'.01'</span>), rounding=decimal.ROUND_HALF_UP)</span><br></pre></td></tr></table></figure><h3 id="List-of-Lists"><a href="#List-of-Lists" class="headerlink" title="List of Lists"></a>List of Lists</h3><p>To flatten the list:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">flat_list = [item <span class="keyword">for</span> sublist <span class="keyword">in</span> l <span class="keyword">for</span> item <span class="keyword">in</span> sublist]</span><br><span class="line"><span class="comment"># this is the most efficient way</span></span><br><span class="line"><span class="comment"># reference: https://stackoverflow.com/questions/952914/how-to-make-a-flat-list-out-of-list-of-lists</span></span><br></pre></td></tr></table></figure><p>To compute the cartesian product:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># https://docs.python.org/3/library/itertools.html#itertools.product</span></span><br><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line">result = list(itertools.product(*somelists))</span><br></pre></td></tr></table></figure><h2 id="Something-to-be-detailed"><a href="#Something-to-be-detailed" class="headerlink" title="Something to be detailed"></a>Something to be detailed</h2><p>Here stores something that is introduced in tutorial briefly, waiting to be discussed in detail in the future.</p><p><strong>Arbitrary Argument Lists and Unpacking Argument Lists</strong><br>Introduced in Section 4.7, </p><p><strong>Method</strong><br>A method is a function that ‘belongs’ to an object and is named obj.methodname, where obj is some object (this may be an expression), and methodname is the name of a method that is defined by the object’s type. Different types define different methods. Methods of different types may have the same name without causing ambiguity.</p><p><strong>Lambda Expressions</strong><br>Small anonymous functions can be created with the <code>lambda</code> keyword. This function returns the sum of its two arguments: <code>lambda a, b: a+b</code>. Lambda functions can be used wherever function objects are required. They are syntactically restricted to a single expression. Semantically, they are just syntactic sugar for a normal function definition. Like nested function definitions, lambda functions can reference variables from the containing scope:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">make_incrementor</span><span class="params">(n)</span>:</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> <span class="keyword">lambda</span> x: x + n</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = make_incrementor(<span class="number">42</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f(<span class="number">0</span>)</span><br><span class="line"><span class="number">42</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f(<span class="number">1</span>)</span><br><span class="line"><span class="number">43</span></span><br></pre></td></tr></table></figure><p><strong>The del statement</strong><br>Remove an item from a list given its index instead of its value. The <code>del</code> statement can also be used to remove slices from a list or clear the entire list.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">-1</span>, <span class="number">1</span>, <span class="number">66.25</span>, <span class="number">333</span>, <span class="number">333</span>, <span class="number">1234.5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">del</span> a[<span class="number">0</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="number">1</span>, <span class="number">66.25</span>, <span class="number">333</span>, <span class="number">333</span>, <span class="number">1234.5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">del</span> a[<span class="number">2</span>:<span class="number">4</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[<span class="number">1</span>, <span class="number">66.25</span>, <span class="number">1234.5</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">del</span> a[:]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">[]</span><br></pre></td></tr></table></figure><p><code>del</code> can also be used to delete entire variables.<br><code>del</code> can delete a key:value pair in dict, the parameter is just dict’s key.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Foreword&quot;&gt;&lt;a href=&quot;#Foreword&quot; class=&quot;headerlink&quot; title=&quot;Foreword&quot;&gt;&lt;/a&gt;Foreword&lt;/h2&gt;&lt;h3 id=&quot;What&quot;&gt;&lt;a href=&quot;#What&quot; class=&quot;headerlink&quot; 
      
    
    </summary>
    
    
      <category term="tips" scheme="http://haelchan.me/tags/tips/"/>
    
      <category term="learning note" scheme="http://haelchan.me/tags/learning-note/"/>
    
      <category term="Python" scheme="http://haelchan.me/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>再见2017 你好2018</title>
    <link href="http://haelchan.me/2017/12/31/hello-2018/"/>
    <id>http://haelchan.me/2017/12/31/hello-2018/</id>
    <published>2018-01-01T02:43:55.000Z</published>
    <updated>2018-10-18T14:15:55.000Z</updated>
    
    <content type="html"><![CDATA[<p>窗外的跨年狂欢夜开始全场倒计时，室内的手机屏幕上的显示时间却调皮地率先变成了00:00。<br>新的一天。新的一月。以及 新的一年。<br>回首2017，有很多人生的第一次呢。第一次知乎回答收到赞感谢收藏，第一次写专栏文章，搭建了自己的博客并发表了第一篇文章，第一次加入实验室，第一次阅读英文论文，第一次完成Coursera的课程……其实也都蛮微不足道的，不过万事开头难啦。既然踏出了勇敢的第一步，那就坚持大步走下去吧。纵使路途荆棘遍布，亦会有鲜花相随。<br>为了申请美国的研究生，下个学期将会无比繁忙。TOEFL、GRE、补CS的相关课程、重修数学课……此外还要考虑实习、科研、交流等事项，可以说是很辛苦呢。不过这都是带有明确的目的指向，而不似大一大二的漫无目的。希望自己能在2018好好努力，争取收到各种好消息叭。<br>年底时莫名冒出了“佛系”的说法，意指无欲无求，一切随缘。感觉自己在生活方面很早就进入了该种状态吧。虽然在玉泉属于最年轻的一批人，不过莫名地感觉过上了玉泉的老人生活。不波不澜，似直流信号，偶尔的脉冲信号荡起一丝起伏。<br>2017年也发生了许多大事。中国的互联网是健忘的，不过，历史不能遗忘。此不言，仅铭记。<br>你好2018，请多指教~<br><img src="/images/sth/IMG_0138.JPG" alt></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;窗外的跨年狂欢夜开始全场倒计时，室内的手机屏幕上的显示时间却调皮地率先变成了00:00。&lt;br&gt;新的一天。新的一月。以及 新的一年。&lt;br&gt;回首2017，有很多人生的第一次呢。第一次知乎回答收到赞感谢收藏，第一次写专栏文章，搭建了自己的博客并发表了第一篇文章，第一次加入实验
      
    
    </summary>
    
      <category term="Essay" scheme="http://haelchan.me/categories/Essay/"/>
    
    
  </entry>
  
  <entry>
    <title>Bitcoin and cryptocurrencies</title>
    <link href="http://haelchan.me/2017/12/27/bitcoin-and-cryptocurrencies/"/>
    <id>http://haelchan.me/2017/12/27/bitcoin-and-cryptocurrencies/</id>
    <published>2017-12-28T07:53:31.000Z</published>
    <updated>2018-10-18T15:08:01.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Week-One-Introduction-to-Crypto-and-Cryptocurrencies"><a href="#Week-One-Introduction-to-Crypto-and-Cryptocurrencies" class="headerlink" title="Week One - Introduction to Crypto and Cryptocurrencies"></a>Week One - Introduction to Crypto and Cryptocurrencies</h2><h3 id="Hash-function"><a href="#Hash-function" class="headerlink" title="Hash function"></a>Hash function</h3><p>Takes any string as input<br>fixed-size output (e.g. 256bits just as BitCoin)<br>efficiently computable</p><h4 id="Security-properties"><a href="#Security-properties" class="headerlink" title="Security properties:"></a>Security properties:</h4><p><strong>Collision-free</strong><br>Nobody can find x and y such that x!=y and H(x)=H(y).</p><p>Note: Collisions do exist. The possible outputs are finite(string of 256 bits in size), while the possible inputs can be a string of any size.</p><p>To find a collision of any hash function: Trying <script type="math/tex">2^{130}</script> randomly chosen inputs and chances are 99.8% that two of them will collide. Well, it takes too long to matter.<br>For some possible H’s, there’s a faster way to find collisions; for others, we haven’t known yet.<br>No H has been <strong>proven</strong> collision-free.</p><p><strong>Hiding</strong><br>(r|x) means that take all the bits of r and put after them all the bits of x.<br>If r is chosen from a probability distribution that has high min-entropy, then given H(r|x), it is infeasible to find x.<br>High min-entropy means that the distribution is “very spread out”, so that no particular value is chosen with more than negligible probability.</p><p><strong>Puzzle-friendly</strong><br>For every possible output value y, if k is chosen from a distribution with high min-entropy, then it is infeasible to find x such that H(k|x)=y.</p><h4 id="Applications"><a href="#Applications" class="headerlink" title="Applications"></a>Applications</h4><p><strong>Hash as message digest.</strong><br>If we know H(x)=H(y), it’s safe to assume that x=y.<br>To recognize a file that we saw before, just remember its hash.<br>It’s useful because the hash is small, while a file may be very big.</p><p><strong>Commitment</strong><br>Want to “seal a value in an envelope”, and “open the envelope” later.<br>Commit to a value, reveal it later.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(com, key) := commit(msg)</span><br><span class="line">match := verify(com, key, msg)</span><br></pre></td></tr></table></figure></p><p>To seal msg in envelope:<br>  (com, key) := commit(msg) — then publish com<br>To open envelope:<br>  publish key, msg<br>  anyone can use verify() to check validity</p><p><strong>Commitment API:</strong><br>commit(msg):=(H(key|msg),key) , where key is a random 256-bit value<br>verify(com, key, msg):=(H(key|msg)==com)</p><p>Security properties:<br>Hiding: Given com, infeasible to find msg.<br>Binding: Infeasible to find <code>msg != msg&#39;</code> such that <code>verify(commit(msg), msg&#39;) == true</code>.</p><p><strong>Search Puzzle</strong><br>Given a “puzzle ID” <em>id</em> (from high min-entropy distribution) and a target set Y,<br>try to find a “solution” x such that H(id|x)∈Y.<br>Puzzle-friendly property implies that no solving strategy is much better than trying random values of x.</p><h4 id="SHA256"><a href="#SHA256" class="headerlink" title="SHA256"></a>SHA256</h4><p><img src="/images/btc/sha256.jpg" alt><br>SHA-256 takes the message that you’re hashing, and it breaks the message up into blocks that are 512 bits in size(add some padding at the end 100…00).<br>IV: 256 bit value(look up in a standards document).<br>c: the compression function. Take 768 bits(256+512) and run through this function and out comes 256 bits.</p><h3 id="Hash-Pointer"><a href="#Hash-Pointer" class="headerlink" title="Hash Pointer"></a>Hash Pointer</h3><p>Hash pointer is pointer to where some info is stored, and (cryptographic) hash of the info.<br>If we have a hash pointer, we can ask to get the info back, and verify that it hasn’t changed.</p><p>Data structure of blockchain:<br><img src="/images/btc/hashll.jpg" alt><br>This is blockchain, and it’s similar to linked list.<br>If some adversary wants to change a block’s data(e.g., the left one), then the content of that block is changed. And the middle block’s hash pointer is not consistent to the left block any more. So the adversary has to change middle block’s header, and next block’s header and so on.</p><p>Merkle tree<br>Advantages of Merkle trees:<br>Tree holds many items, but just need to remember the root hash.<br>Can verify membership in O(log n) time/space.</p><h3 id="Digital-Signature"><a href="#Digital-Signature" class="headerlink" title="Digital Signature"></a>Digital Signature</h3><p>What we want from signatures is two things:</p><ol><li>Only you can make your signature, but anyone who sees your signature can verify that it’s valid.</li><li>Signature is tied to a particular document, because signature is not just a signature, it signifies your agreement or endorsement of a particular document.</li></ol><p>API for digital signatures<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(sk,pk):=generateKeys(keySize)</span><br><span class="line">sig:=sign(sk, message)</span><br><span class="line">isValid:=verify(pk, message, sig)</span><br></pre></td></tr></table></figure></p><p><strong>Requirements for signatures</strong><br>·Valid signatures verify<br>verify(pk, message, sign(sk, message))==true<br>·Can’t forge signatures<br>An adversary who knows your public key and gets to see signatures on some other messages, can’t forge your signature on some message that he wants to forge it on.</p><p><strong>Practical stuff</strong><br>algorithms are randomized<br>  need good source of randomness<br>limit on message size<br>  fix: use Hash(message) rather than message<br>fun trick: sign a hash pointer<br>  signature “covers” the whole structure</p><h3 id="Simple-Cryptocurrencies"><a href="#Simple-Cryptocurrencies" class="headerlink" title="Simple Cryptocurrencies"></a>Simple Cryptocurrencies</h3><h4 id="GoofyCoin"><a href="#GoofyCoin" class="headerlink" title="GoofyCoin"></a>GoofyCoin</h4><p>Goofy can create new coins.<br><img src="/images/btc/goofy.jpg" alt></p><p>A coin’s owner can spend it.<br><img src="/images/btc/goofy2.jpg" alt></p><p>The recipient can pass on the coin again.<br><img src="/images/btc/goofy3.jpg" alt></p><p>Problem: <strong>Double-spending attack</strong><br>the main design challenge in digital currency<br><img src="/images/btc/doublespend.jpg" alt></p><h4 id="ScroogeCoin"><a href="#ScroogeCoin" class="headerlink" title="ScroogeCoin"></a>ScroogeCoin</h4><p><img src="/images/btc/scrooge.jpg" alt></p><p>CreateCoin transaction creates new coins.<br><img src="/images/btc/scrooge1.jpg" alt></p><p>PayCoin transaction consumes (and destroys) some coins, and create new coins of the same total value.<br><img src="/images/btc/scrooge2.jpg" alt><br>Valid if:<br>-consumed coins valid<br>-not already consumed<br>-total value out = total value in<br>-signed by owners of all consumed coins</p><p>Note: Coins are immutable, that is, coins can’t be transferred, subdivided, or combined.</p><h2 id="Week-Two-How-Bitcoin-Achieves-Decentralization"><a href="#Week-Two-How-Bitcoin-Achieves-Decentralization" class="headerlink" title="Week Two - How Bitcoin Achieves Decentralization"></a>Week Two - How Bitcoin Achieves Decentralization</h2><p>Why consensus protocols?<br>Traditional motivation: reliability in distributed systems<br>Distributed key-value store enables various applications: DNS, public key directory, stock trades …</p><h2 id="Distributed-consensus"><a href="#Distributed-consensus" class="headerlink" title="Distributed consensus"></a>Distributed consensus</h2><p>The protocol terminates and all correct nodes decide on the same value<br>This value must have been proposed by some correct node</p><p>Why consensus is hard?<br>Nodes may crash<br>Nodes may be malicious<br>Network is imperfect (Not all pairs of nodes connected; faults in network; latency)</p><p>Consensus algorithm (simplified)</p><ol><li>New transactions are broadcast to all nodes</li><li>Each node collects new transactions into a block</li><li>In each round a random node gets to broadcast its block</li><li>Other nodes accept the block only if all transactions in it are valiid (unspent, valid signatures)</li><li>Nodes express their acceptance of the block by including its hash in the next block they create</li></ol><p>What can a malicious node do? - Double Spending<br><img src="/images/btc/malicious.jpg" alt><br>Honest nodes will extend the longest valid branch. In the above image, the green block and the red block are identical. So chances are that the next node will extend the red block, and so on, which makes the double-spending attack.</p><p>However, from Bob’s view, it looks like this:<br><img src="/images/btc/bob.jpg" alt><br>Double-spend attack only occurs with 1 confirmations. If Bob is patient enough and wait for some other confirmations, he’s not likely to suffer double-spend.<br>Double-spend probability decreases exponentially with number of confirmations.<br>(Most common heuristic: 6 confirmations)</p><h3 id="Incentives-and-Proof-of-Work"><a href="#Incentives-and-Proof-of-Work" class="headerlink" title="Incentives and Proof of Work"></a>Incentives and Proof of Work</h3><p><strong>Incentive 1: block reward</strong><br>Creator of block gets to</p><ul><li>include special coin-creation transaction in the block</li><li>choose recipient address of this transaction</li></ul><p>Note block creator gets to “collect” the reward only if the block ends up on long-term consensus branch.</p><p><strong>Incentive 2: transaction fees</strong><br>Creator of transaction can choose to make output value less than input value<br>Remainder is a transaction fee and goes to block creator<br>Purely voluntary, like a tip.</p><p><img src="/images/btc/pow.jpg" alt></p><p><strong>PoW property</strong><br>1: difficult to compute<br>Only some nods bother to compute - miners</p><p>2: parameterizable cost<br>Nodes automatically re-calculate the target every two weeks.<br>Goal: average time between blocks = 10 minutes</p><script type="math/tex; mode=display">Prob(Alice\ wins\ next\ block)=fraction\ of\ global\ hash\ power\ she\ controls</script><p>3: trivial to verify<br>Nonce must be published as part of block<br>Other miners simply verify that <script type="math/tex">H(nonce||prev_hash||tx||...||tx)<target</script></p><p><strong>Key security assumption</strong><br>Attacks infeasible if majority of miners <em>weighted by hash power</em> follow the protocol.</p><p>for individual miner:</p><script type="math/tex; mode=display">mean\ time\ to\ find\ block=\frac{10\ minutes}{fraction\ of\ hash\ power}</script><p>**What can a “51% attacker” do?<br>Steal coins from existing address?  ×</p><p>Suppress some transactions?<br>· From the block chain              √<br>· From the P2P network              ×</p><p>Change the block reward?            ×</p><p>Destroy confidence in Bitcoin?      √√</p><h2 id="Week-Three"><a href="#Week-Three" class="headerlink" title="Week Three"></a>Week Three</h2><h3 id="Bitcoin-transactions"><a href="#Bitcoin-transactions" class="headerlink" title="Bitcoin transactions"></a>Bitcoin transactions</h3><p>Comparison between an account-based ledger and a transaction-based ledger:<br><strong>An account-based ledger</strong><br><img src="/images/btc/ledger1.jpg" alt><br>If we want to check whether a transaction is valid, we might need to scan backwards until genesis. That’s quite inconvenient and inefficiency.</p><p><strong>A transaction-based ledger</strong><br><img src="/images/btc/ledger2.jpg" alt><br>The verification just needs finite scan with hash pointers.</p><p>Then, we can easily realize functions like merging value and joint payments.<br><strong>Merging value</strong><br><img src="/images/btc/merging.jpg" alt><br>(The slides shown on Coursera has some mistakes, so I have to screenshot on the course)</p><p><strong>Joint payments</strong><br><img src="/images/btc/joint.jpg" alt></p><p>Here’s what a Bitcoin transaction look like:<br><img src="/images/btc/transaction.jpg" alt></p><h3 id="Bitcoin-Scripts"><a href="#Bitcoin-Scripts" class="headerlink" title="Bitcoin Scripts"></a>Bitcoin Scripts</h3><p>Note that in Bitcoin transaction, instead of assigning a public key, Bitcoin uses script.</p><p><strong>Design goals</strong></p><ul><li>Build for Bitcoin (inspired by Forth)</li><li>Simple, compact</li><li>Support for cryptography</li><li>Stack-based</li><li>Limits on time/memory</li><li>No looping</li></ul><p><strong>Instructions</strong><br>256 opcodes total (15 disabled, 75 reserved)</p><ul><li>Arithmetic</li><li>If/then </li><li>Logic/data handling</li><li>Crypto: Hashes, signature verification, multi-signature verification</li></ul><p>Instruction:</p><p><code>&lt;sig&gt;</code> : Input script. Push data onto the stack.<br><code>&lt;pubKey&gt;</code> : Push data onto the stack.</p><p><code>OP_DUP</code> : Duplicate instruction. Take the value on the top of the stack, pop it off, and then write two copies back to the stack.</p><p><code>OP_HASH160</code> : Take the top value on the stack and compute a cryptographic hash of it.</p><p><code>pubKeyHash</code> : The hash of public key that was actually used by the recipient when trying to claim the coins.<br><code>pubKeyHash?</code> : Specified by the sender of the coins. The public key that the sender specified, had to be used to generate the signature to redeem these coins.</p><p><code>OP_EQUALVERIFY</code> : Check if the two values at the top of the stack equal. It the two values aren’t equal, an error’s gonna be thrown and the script will stop executing. It they are, the instruction will consume those two data items that are at the top of the stack.</p><p><code>OP_CHECKSIG</code> : Verify the entire transaction was successfully signed. Pop those remaining two items off of the stack, check the signature is valid.</p><p><code>OP_CHECKMULTISIG</code> :<br>Built-in support for joint signatures<br>Specify <em>n</em> public key<br>Specify t (threshold)<br>Verification requires <em>t</em> signatures<br>(BUG: Extra data value popped from the stack and ignored)</p><h3 id="Bitcoin-Blocks"><a href="#Bitcoin-Blocks" class="headerlink" title="Bitcoin Blocks"></a>Bitcoin Blocks</h3><p><img src="/images/btc/blockStructure.jpg" alt></p><p><img src="/images/btc/block.jpg" alt></p><p>There’s a special transaction, which is the coinbase transaction:<br><img src="/images/btc/coinbase.jpg" alt><br>Since this transaction creates new coins, it’s prev_out has a null hash pointer.<br>Miners can put anything in coinbase.<br>The value is slightly more than the set value, which contains transaction fees.</p><h3 id="Bitcoin-Network"><a href="#Bitcoin-Network" class="headerlink" title="Bitcoin Network"></a>Bitcoin Network</h3><p>P2P Network</p><ul><li>Ad-hoc protocol (runs on TCP port 8333)</li><li>Ad-hoc network with random topology</li><li>All nodes are equal</li><li>New nodes can join at any time</li><li>Forget non-responding nodes after 3 hr</li></ul><h4 id="Propagation"><a href="#Propagation" class="headerlink" title="Propagation"></a>Propagation</h4><p><strong>Transaction propagation</strong><br>· Transaction valid with current blockchain<br>· (default) script matches a whitelist - avoid unusual scripts<br>· Haven’t seen before - Avoid infinite loops<br>· Doesn’t conflict with others I’ve relayed - avoid double-spends</p><p><strong>Block propagation</strong><br>Relay a new block when you hear it if:<br>· Block meets the hash target<br>· Block has all valid transactions - Run <em>all</em> scripts, even if you wouldn’t relay<br>· Block builds on current longest chain - Avoid forks</p><h4 id="Nodes"><a href="#Nodes" class="headerlink" title="Nodes"></a>Nodes</h4><p><strong>Fully-validating nodes</strong><br>· Permanently connected<br>· Store entire block chain (Storage cost: 20 GB)<br>· Hear and forward every node/transaction</p><p><strong>Thin/SPV clients(not fully-validating)</strong><br>Ideas: don’t store everything<br>· Store block headers only (1000x cost saving)<br>· Request transactions as needed - to verify incoming payment<br>· Trust fully-validating nodes </p><h3 id="Limitations-amp-Improvements"><a href="#Limitations-amp-Improvements" class="headerlink" title="Limitations &amp; Improvements"></a>Limitations &amp; Improvements</h3><p>Hard-coded limits in Bitcoin<br>· 10 min. average creation time per block<br>· 1 M bytes in a block<br>· 20,000 signature operations per block<br>·· 100 M <em>satoshi</em>s per bitcoin<br>·· 21 M total bitcoins maximum<br>·· 50,25,12,5… bitcoin mining reward<br>·· : These affect economic balance of power too much to change now</p><p>Throughput limits in Bitcoin<br>· 1 M bytes/block (10 min)<br>· &gt;250 bytes/transaction<br>· 7 transactions/sec</p><p>Cryptographic limits in Bitcoin<br>· Only 1 signature algorithm (ECDSA/P256)<br>· Hard-coded hash functions</p><h4 id="Forking"><a href="#Forking" class="headerlink" title="Forking"></a>Forking</h4><p><strong>Hard-forking</strong><br>If old nodes didn’t update software, they will reject all the new transactions/blocks.<br>Old nodes will never catch up.</p><p><strong>Soft forks</strong><br>Observation: we can add new features which only <em>limit</em> the set of valid transactions</p><p>Need majority of nodes to enforce new rules</p><p>Old nodes will approve.</p><p>Risks exist that old nodes might mine now-invalid blocks, since there’re new limits on blocks.</p><p><strong>Soft fork possibilities</strong><br>· New signature schemas<br>· Extra per-block metadata - Shove in the coinbase parameter; Commit to UTXO tree in each block</p><p><strong>Hard forks</strong><br>· New op codes<br>· Changes to size limits<br>· Changes to mining rate<br>· Many small bug fixes (like bug in MULTISIG)</p><p>Currently seem very unlikely to happen.</p><h2 id="Week-Five"><a href="#Week-Five" class="headerlink" title="Week Five"></a>Week Five</h2><h3 id="Mining-Hardware"><a href="#Mining-Hardware" class="headerlink" title="Mining Hardware"></a>Mining Hardware</h3><h4 id="CPU"><a href="#CPU" class="headerlink" title="CPU"></a>CPU</h4><p>Throughput on a high-end PC = 10-20 MHz ≈ 2^24<br>139461 years on average to find a block today</p><h4 id="GPU"><a href="#GPU" class="headerlink" title="GPU"></a>GPU</h4><p>GPUs designed for high-performance graphics - high parallelism &amp; high throughput<br>First used for Bitcoin ca. October 2010<br>Implemented in OpenCL - Later: hacks for specific cards</p><p><strong>Advantages</strong><br>· easily available, easy to set up<br>· parallel ALUs<br>· bit-specific instructions<br>· can drive many from 1 CPU<br>· can overclock</p><p><strong>Disadvantages</strong><br>· poor utilization of hardware<br>· poor cooling<br>· large power draw<br>· few boards to hold multiple GPUs</p><p>Throughput on a good card = 20-200 MHz ≈ 2^27<br>173 years on average to find a block w/100 cards</p><h4 id="FPGA"><a href="#FPGA" class="headerlink" title="FPGA"></a>FPGA</h4><p>First used for Bitcoin ca. June 2011<br>Implemented in Verilog</p><p><strong>Advantages</strong><br>· higher performance than GPUs - excellent performance on bitwise operations<br>· better cooling<br>· extensive customization, optimization</p><p><strong>Disadvantages</strong><br>· higher power draw than GPUs designed for - frequent malfunctions, errors<br>· poor optimization of 32-bit adds<br>· fewer hobbyists with sufficient expertise<br>· more expensive than GPUs<br>· marginal performance/cost advantage over GPUs</p><p>Throughput on a good card = 100-1000 MHz ≈ 2^30<br>25 years on average to find a block w/100 boards</p><h4 id="Bitcoin-ASIC"><a href="#Bitcoin-ASIC" class="headerlink" title="Bitcoin ASIC"></a>Bitcoin ASIC</h4><p>· special purpose - approaching known limits on feature sizes, less than 10x performance improvement expected<br>· designed to be run constantly for life<br>· require significant expertise, long lead-times<br>· perhaps the fastest chip development ever</p><h2 id="Week-Six"><a href="#Week-Six" class="headerlink" title="Week Six"></a>Week Six</h2><h3 id="Anonymity"><a href="#Anonymity" class="headerlink" title="Anonymity"></a>Anonymity</h3><p>Anonymity in computer science:</p><script type="math/tex; mode=display">Anonymity=pseudonymity+unlinkability</script><p><strong>Pseudonymity</strong>: Bitcoin addresses are public key hashes rather than real identities<br><strong>Unlinkability</strong>: different interactions of the same user with the system should not be linkable to each other</p><p>Unlinkability in Bitcoin:<br>Hard to link different addresses of the same user<br>Hard to link different transactions of the same user<br>Hard to link sender of a payment to its recipient</p><p><strong>Blind signature</strong><br>two-party protocol to create digital signature without signer knowing the input</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Week-One-Introduction-to-Crypto-and-Cryptocurrencies&quot;&gt;&lt;a href=&quot;#Week-One-Introduction-to-Crypto-and-Cryptocurrencies&quot; class=&quot;headerl
      
    
    </summary>
    
      <category term="Blockchain" scheme="http://haelchan.me/categories/Blockchain/"/>
    
    
      <category term="learning note" scheme="http://haelchan.me/tags/learning-note/"/>
    
  </entry>
  
</feed>
